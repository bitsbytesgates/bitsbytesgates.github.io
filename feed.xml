<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://bitsbytesgates.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://bitsbytesgates.com/" rel="alternate" type="text/html" /><updated>2023-02-17T14:23:29+00:00</updated><id>https://bitsbytesgates.com/feed.xml</id><title type="html">Bits, Bytes, and Gates</title><subtitle>There&apos;s oh so much fun to be had. At the leading edge,  at the bleeding edge, at the confluence of bits, bytes, and gates.</subtitle><entry><title type="html">New Year, New Space</title><link href="https://bitsbytesgates.com/intro/2023/02/16/NewYearNewSpace.html" rel="alternate" type="text/html" title="New Year, New Space" /><published>2023-02-16T00:00:00+00:00</published><updated>2023-02-16T00:00:00+00:00</updated><id>https://bitsbytesgates.com/intro/2023/02/16/NewYearNewSpace</id><content type="html" xml:base="https://bitsbytesgates.com/intro/2023/02/16/NewYearNewSpace.html"><![CDATA[<p>We’ve more than gotten started on the new year. In fact, DVCon – reliably
and predictably held during the last few days of February and initial 
few of March is right around the corner. And, here I am just getting the
first post of the year out.</p>

<p>Getting a late start on blog posts this year isn’t for lack of interesting
ideas and projects to, though. Here are a few things you can look forward 
to in the coming Blog year.</p>

<h1 id="new-space">New Space</h1>
<p>First, about the new space… For many years, I hosted the bitsbytesgates 
blog on the blogspot.com platform.
It was (relatively) easy to type up posts, and simple was good. That said,
the more technical content I put out (code snippets and such), the more I 
started to hit the usability edges of the blogspot environment. So, like
so many of my peers, I decided it was time to take the plunge and move 
to a new platform, and that this was the year to do it.</p>

<p>Going forward, you can find the bitsbytesgates blog at <a href="https://bitsbytesgates.com">https://bitsbytesgates.com</a>.</p>

<h1 id="python-functional-verification">Python Functional Verification</h1>
<p>Using Python for functional verification continues to be an interest of
mine. Declarative descriptions have also been an interest of mine for some time. 
This year, I want to look more deeply at how some of those declarative
approaches to capturing aspects of verification environments can be deployed
in Python to make Python-based functional verification even more productive.</p>

<h1 id="constrained-random-generation-and-functional-coverage">Constrained-Random Generation and Functional Coverage</h1>
<p>If you’ve been following the blog for a while, you’re aware of some of the
Python-based projects that bring constrained-random generation and 
functional coverage capture and manipulation into Python. Using these
capabilities in Python continues to be key, but I’m looking at making 
some of these capabilities available to projects not implemented in 
Python.</p>

<h1 id="portable-test-and-stimulus-pss">Portable Test and Stimulus (PSS)</h1>
<p>I’ve been involved in the Accellera standards committee for 
Portable Test and Stimulus (PSS) since its inception, but haven’t written
much about it here … until this year. I’m very optimistic about the 
opportunity PSS has to substantially improve the way that we approach
system-level tests – and, especially, the creation of bare-metal software
test content. Look for more about PSS, starting with a ground-up tour, in
the coming year.</p>

<h1 id="conclusion">Conclusion</h1>
<p>With that, welcome to the new blog space, and to a new year of posts. And,
if you happen to be attending DVCon 2023 in San Jose, I hope we get a
chance to interact live and in person!</p>]]></content><author><name></name></author><category term="Intro" /><summary type="html"><![CDATA[We’ve more than gotten started on the new year. In fact, DVCon – reliably and predictably held during the last few days of February and initial few of March is right around the corner. And, here I am just getting the first post of the year out.]]></summary></entry><entry><title type="html">Simplifying Custom Template-Generated Content</title><link href="https://bitsbytesgates.com/2022/08/21/simplifying-custom-template-generated.html" rel="alternate" type="text/html" title="Simplifying Custom Template-Generated Content" /><published>2022-08-21T17:18:00+00:00</published><updated>2022-08-21T17:18:00+00:00</updated><id>https://bitsbytesgates.com/2022/08/21/simplifying-custom-template-generated</id><content type="html" xml:base="https://bitsbytesgates.com/2022/08/21/simplifying-custom-template-generated.html"><![CDATA[<p style="text-align: center;">&nbsp;</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjW6ygOoIw0lvUu7rGQcU-QVVwfR0SFtT6zCJtRzOilSOqHR8OwMnyDjG3C32-arWwoRRP0mKzGieAzgnyEPWk5fFsnzWNezMqZgDFNOUQy9MCRdodg9cWa3ghlBgKJgxJmXLOAXXCkMJI7tdm-fvXLQZIVvYFCXSXc7yHXz5NjDUt3SRbvhit25z52PQ/s540/splash.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="300" data-original-width="540" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjW6ygOoIw0lvUu7rGQcU-QVVwfR0SFtT6zCJtRzOilSOqHR8OwMnyDjG3C32-arWwoRRP0mKzGieAzgnyEPWk5fFsnzWNezMqZgDFNOUQy9MCRdodg9cWa3ghlBgKJgxJmXLOAXXCkMJI7tdm-fvXLQZIVvYFCXSXc7yHXz5NjDUt3SRbvhit25z52PQ/s16000/splash.png" /></a></div><br /><p></p><p>As a verification engineer, it's quite common to work with data and code that follow a regular pattern. Having an efficient way to create this repetitive code is a significant productivity boost. While there certainly are places in the code where 'your critical generation or checking algorithm' goes, much of the structure of an agent, a test environment, etc remain the same. The same goes for other parts of the flow, such as project meta-data, test lists, etc. There are two things that keep us from just making copies of a set of 'golden' files to create the basis for a new UVM agent, project, etc: some or all of the files need to have some data substituted or changed. For example, we want to substitute the name of the new UVM agent we're creating into most of the new SystemVerilog source code.</p><p>Custom code generators have been developed for some of these tasks. These often focus on providing a domain-specific way to capture input data, such as the structure of a UVM testbench or the layout of registers in a design. But there are many more opportunities to generate template-driven code that cannot justify the investment to create a focused solution.</p><p>A few years ago, I created the <a href="https://github.com/fvutils/vte/">Verification Template Engine (VTE)</a> to serve my needs for generating template-driven content. I developed VTE with three user-experience requirements in mind:</p><p></p><ul style="text-align: left;"><li>Creating a new template should be very easy, but have access to powerful generation features</li><li>Managing the available templates should be simple for a user.&nbsp;</li><li>The core tools should be generic, and make few or no assumptions about what is being generated</li></ul><div>VTE focuses on organizing and discovering template content, but leverages the <a href="https://palletsprojects.com/p/jinja/">Jinja2 template engine</a> to do the heavy lifting of template expansion. In some sense, you can think of VTE as providing a user interface to the Jinaj2 library.</div><div><br /></div><div>I've been using VTE since developing it, but am just getting back to create proper documentation, which you can find here: <a href="https://fvutils.github.io/vte/">https://fvutils.github.io/vte/</a>. As part of that work, I created a quickstart guide which is both in the documentation, and forms the remainder of this post.&nbsp;</div><div><br /></div><p></p><div><div><b>Installing VTE</b></div><div>The easiest way to install VTE is from PyPi.</div><div><br /></div><div><span style="font-family: courier;">% python3 -m pip install --user vte</span></div><div>Test that you can run VTE by running the command (vte) and/or invoking the module:</div><div><br /></div><div><span style="font-family: courier;">% vte --help</span></div><div><span style="font-family: courier;">% python3 -m vte --help</span></div><div><br /></div><div><b>Creating a Template</b></div><div>VTE discovers templates by searching directories on the VTE_TEMPLATE_PATH environment variable. VTE uses a marker file named .vte to identify the root of a template. All files and directories in and below a template directory are considered to be part of the template. The template identifier is composed from the directory names between the directory listed in VTE_TEMPLATE_PATH and the directory containing the .vte marker file.</div><div><br /></div><div>Let’s look at an example to illustrate the rules.</div><div><br /></div><div><span style="font-family: courier;">templates</span></div><div><span style="font-family: courier;">&nbsp; uvm</span></div><div><span style="font-family: courier;">&nbsp; &nbsp; agent</span></div><div><span style="font-family: courier;">&nbsp; &nbsp; &nbsp; .vte</span></div><div><span style="font-family: courier;">&nbsp; &nbsp; component</span></div><div><span style="font-family: courier;">&nbsp; &nbsp; &nbsp; .vte</span></div><div><span style="font-family: courier;">&nbsp; doc</span></div><div><span style="font-family: courier;">&nbsp; &nbsp; blog_post</span></div><div><span style="font-family: courier;">&nbsp; &nbsp; &nbsp; .vte</span></div><div><span style="font-family: courier;">&nbsp; &nbsp; readme</span></div><div><span style="font-family: courier;">&nbsp; &nbsp; &nbsp; .vte</span></div><div><br /></div><div>Let’s assume we add the templates directory to VTE_TEMPLATE_PATH. VTE will find four templates:</div><div><br /></div><div><span style="font-family: courier;">uvm.agent</span></div><div><span style="font-family: courier;">uvm.component</span></div><div><span style="font-family: courier;">doc.blog_post</span></div><div><span style="font-family: courier;">doc.readme</span></div><div><br /></div><div>All files in and below the directory containing the .vte marker will be rendered when the template is used.</div><div><br /></div><div><b>Creating the Template Structure</b></div><div>Let’s create a very simple template structure. Create the following directory structure:</div><div><br /></div><div><span style="font-family: courier;">templates</span></div><div><span style="font-family: courier;">&nbsp; doc</span></div><div><span style="font-family: courier;">&nbsp; &nbsp; readme</span></div><div><br /></div><div>Change directory to templates/doc/readme and run the quickstart command:</div><div><br /></div><div><span style="font-family: courier;">% vte quickstart</span></div><div><span style="font-family: courier;">Verification Template Engine Quickstart</span></div><div><span style="font-family: courier;">Template directory: templates/doc/readme</span></div><div><span style="font-family: courier;">Template Description []? Create a simple README</span></div><div><br /></div><div>This command will prompt for a description to use for the template. Enter a description and press ENTER. This will create the .vte marker file.</div><div><br /></div><div>View the .vte file. You’ll see that the initial version is quite simple. For now, this is all we need.</div><div><br /></div><div><span style="font-family: courier;">template:</span></div><div><span style="font-family: courier;">&nbsp; description: Create a simple README</span></div><div><span style="font-family: courier;">&nbsp; parameters: []</span></div><div><span style="font-family: courier;">#&nbsp; &nbsp;- name: param_name</span></div><div><span style="font-family: courier;">#&nbsp; &nbsp; &nbsp;description: param_desc</span></div><div><span style="font-family: courier;">#&nbsp; &nbsp; &nbsp;default: param_default</span></div><div><br /></div><div><b>Creating the Template File</b></div><div>Now, let’s create the template file that will be processed when we render the template. Our readme template only has one file: README.md.</div><div><br /></div><div>Create a file named README.md containing the following content in the templates/doc/readme directory:</div><div><br /></div><div><span style="font-family: courier;"># README for {{name}}</span></div><div><span style="font-family: courier;">TODO: put in some content of interest</span></div><div><br /></div><div>VTE supports defining and using multiple parameters, but defines one built-in parameter that must be supplied for all templates: name. Our template file references name using Jinja2 syntax for variable references.</div><div><br /></div><div>We have now created a simple template for creating README.md files.</div><div><br /></div><div><b>Rendering a Template</b></div><div>In order to render templates, VTE must first be able to discover them. Add the templates directory to the VTE_TEMPLATE_PATH environment variable.</div><div><br /></div><div><span style="font-family: courier;">% export VTE_TEMPLATE_PATH=&lt;path&gt;/templates # Bourne shell</span></div><div><span style="font-family: courier;">% setenv VTE_TEMPLATE_PATH &lt;path&gt;/templates # csh/tsh</span></div><div>Let’s test this out by running the vte list command:</div><div><br /></div><div><span style="font-family: courier;">% vte list</span></div><div><span style="font-family: courier;">doc.readme - Create a simple README</span></div><div><br /></div><div>If you see the doc.readme line above, VTE has successfully discovered the template.</div><div><br /></div><div>Now, let’s actually generate something. Let’s create a new directory parallel to the templates directory in which to try this out</div><div><br /></div><div><span style="font-family: courier;">% mkdir scratch</span></div><div><span style="font-family: courier;">% cd scratch</span></div><div><br /></div><div>Finally, let’s run the generate command:</div><div><br /></div><div><span style="font-family: courier;">% vte generate doc.readme my_project</span></div><div><span style="font-family: courier;">Note: processing template README.md</span></div><div><br /></div><div>VTE prints a line for each template file is processes. The output above confirms that is processed the template README.md file.</div><div><br /></div><div>Let’s have a look at the result. View the README.md file in the scratch directory.</div><div><br /></div><div><span style="font-family: courier;"># README for my_project</span></div><div><span style="font-family: courier;">TODO: put in some content of interest</span></div><div><br /></div><div>Node that the {{name}} reference was replaced by the name (my_project) that we specified.</div><div><br /></div><div>You have now created your first VTE template!</div><div><br /></div></div><p><b>Conclusion</b></p><p>As the tutorial above illustrates, creating a new template for use with VTE is no more effort than making a few name substitutions. If you use the template more than once, you will already have received a positive return on the effort invested. While templates can be simple, you have the full power of the <a href="https://palletsprojects.com/p/jinja/">Jinja2</a> template engine when you need to do something more complex. I encourage you to check out the <a href="https://fvutils.github.io/vte/">VTE documentation</a> and look for opportunities where using template-driven content generation can make your life easier and make you more productive.</p><p><br /></p><div style="text-align: center;">Copyright 2022 Matthew Ballance</div><div><p style="font-variant-east-asian: normal; font-variant-numeric: normal; line-height: 16px; margin-bottom: 0in;"><span style="color: #666666;"><span face="Trebuchet MS, Trebuchet, Verdana, sans-serif"><span style="font-size: 9pt;"><i><span style="background: rgb(255, 255, 255);">The views and opinions expressed above are solely those of the author and do not represent those of my employer or any other party.</span></i></span></span></span></p></div>]]></content><author><name>Matthew Ballance</name></author><category term="Jinja2" /><category term="Code Generation" /><category term="Templates" /><category term="Python" /><category term="Functional Verification" /><summary type="html"><![CDATA[&nbsp;As a verification engineer, it's quite common to work with data and code that follow a regular pattern. Having an efficient way to create this repetitive code is a significant productivity boost. While there certainly are places in the code where 'your critical generation or checking algorithm' goes, much of the structure of an agent, a test environment, etc remain the same. The same goes for other parts of the flow, such as project meta-data, test lists, etc. There are two things that keep us from just making copies of a set of 'golden' files to create the basis for a new UVM agent, project, etc: some or all of the files need to have some data substituted or changed. For example, we want to substitute the name of the new UVM agent we're creating into most of the new SystemVerilog source code.Custom code generators have been developed for some of these tasks. These often focus on providing a domain-specific way to capture input data, such as the structure of a UVM testbench or the layout of registers in a design. But there are many more opportunities to generate template-driven code that cannot justify the investment to create a focused solution.A few years ago, I created the Verification Template Engine (VTE) to serve my needs for generating template-driven content. I developed VTE with three user-experience requirements in mind:Creating a new template should be very easy, but have access to powerful generation featuresManaging the available templates should be simple for a user.&nbsp;The core tools should be generic, and make few or no assumptions about what is being generatedVTE focuses on organizing and discovering template content, but leverages the Jinja2 template engine to do the heavy lifting of template expansion. In some sense, you can think of VTE as providing a user interface to the Jinaj2 library.I've been using VTE since developing it, but am just getting back to create proper documentation, which you can find here: https://fvutils.github.io/vte/. As part of that work, I created a quickstart guide which is both in the documentation, and forms the remainder of this post.&nbsp;Installing VTEThe easiest way to install VTE is from PyPi.% python3 -m pip install --user vteTest that you can run VTE by running the command (vte) and/or invoking the module:% vte --help% python3 -m vte --helpCreating a TemplateVTE discovers templates by searching directories on the VTE_TEMPLATE_PATH environment variable. VTE uses a marker file named .vte to identify the root of a template. All files and directories in and below a template directory are considered to be part of the template. The template identifier is composed from the directory names between the directory listed in VTE_TEMPLATE_PATH and the directory containing the .vte marker file.Let’s look at an example to illustrate the rules.templates&nbsp; uvm&nbsp; &nbsp; agent&nbsp; &nbsp; &nbsp; .vte&nbsp; &nbsp; component&nbsp; &nbsp; &nbsp; .vte&nbsp; doc&nbsp; &nbsp; blog_post&nbsp; &nbsp; &nbsp; .vte&nbsp; &nbsp; readme&nbsp; &nbsp; &nbsp; .vteLet’s assume we add the templates directory to VTE_TEMPLATE_PATH. VTE will find four templates:uvm.agentuvm.componentdoc.blog_postdoc.readmeAll files in and below the directory containing the .vte marker will be rendered when the template is used.Creating the Template StructureLet’s create a very simple template structure. Create the following directory structure:templates&nbsp; doc&nbsp; &nbsp; readmeChange directory to templates/doc/readme and run the quickstart command:% vte quickstartVerification Template Engine QuickstartTemplate directory: templates/doc/readmeTemplate Description []? Create a simple READMEThis command will prompt for a description to use for the template. Enter a description and press ENTER. This will create the .vte marker file.View the .vte file. You’ll see that the initial version is quite simple. For now, this is all we need.template:&nbsp; description: Create a simple README&nbsp; parameters: []#&nbsp; &nbsp;- name: param_name#&nbsp; &nbsp; &nbsp;description: param_desc#&nbsp; &nbsp; &nbsp;default: param_defaultCreating the Template FileNow, let’s create the template file that will be processed when we render the template. Our readme template only has one file: README.md.Create a file named README.md containing the following content in the templates/doc/readme directory:# README for {{name}}TODO: put in some content of interestVTE supports defining and using multiple parameters, but defines one built-in parameter that must be supplied for all templates: name. Our template file references name using Jinja2 syntax for variable references.We have now created a simple template for creating README.md files.Rendering a TemplateIn order to render templates, VTE must first be able to discover them. Add the templates directory to the VTE_TEMPLATE_PATH environment variable.% export VTE_TEMPLATE_PATH=&lt;path&gt;/templates # Bourne shell% setenv VTE_TEMPLATE_PATH &lt;path&gt;/templates # csh/tshLet’s test this out by running the vte list command:% vte listdoc.readme - Create a simple READMEIf you see the doc.readme line above, VTE has successfully discovered the template.Now, let’s actually generate something. Let’s create a new directory parallel to the templates directory in which to try this out% mkdir scratch% cd scratchFinally, let’s run the generate command:% vte generate doc.readme my_projectNote: processing template README.mdVTE prints a line for each template file is processes. The output above confirms that is processed the template README.md file.Let’s have a look at the result. View the README.md file in the scratch directory.# README for my_projectTODO: put in some content of interestNode that the {{name}} reference was replaced by the name (my_project) that we specified.You have now created your first VTE template!ConclusionAs the tutorial above illustrates, creating a new template for use with VTE is no more effort than making a few name substitutions. If you use the template more than once, you will already have received a positive return on the effort invested. While templates can be simple, you have the full power of the Jinja2 template engine when you need to do something more complex. I encourage you to check out the VTE documentation and look for opportunities where using template-driven content generation can make your life easier and make you more productive.Copyright 2022 Matthew BallanceThe views and opinions expressed above are solely those of the author and do not represent those of my employer or any other party.]]></summary></entry><entry><title type="html">PyUCIS: Manipulating Coverage Data</title><link href="https://bitsbytesgates.com/2022/07/17/pyucis-manipulating-coverage-data.html" rel="alternate" type="text/html" title="PyUCIS: Manipulating Coverage Data" /><published>2022-07-17T20:37:00+00:00</published><updated>2022-07-17T20:37:00+00:00</updated><id>https://bitsbytesgates.com/2022/07/17/pyucis-manipulating-coverage-data</id><content type="html" xml:base="https://bitsbytesgates.com/2022/07/17/pyucis-manipulating-coverage-data.html"><![CDATA[<p style="text-align: center;">&nbsp;<a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhp1-_7rtBzwcTNYBtGBUsqaxugtgc8RIz3D1KHRhbTpqghM-oTZjXq_6-ngByPlwmSHZgnu6f5e8ptq6wLa5EZFkOuOHWmgYY0JLua-wLrlTw38FcWT_hMZuFzwOEnxYv1oEimFAAqnr4bcDEo2meEFEZQvH7YdDXMGUixfLzHC6KGfQ_iuXOgn48ogg/s540/splash.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="300" data-original-width="540" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhp1-_7rtBzwcTNYBtGBUsqaxugtgc8RIz3D1KHRhbTpqghM-oTZjXq_6-ngByPlwmSHZgnu6f5e8ptq6wLa5EZFkOuOHWmgYY0JLua-wLrlTw38FcWT_hMZuFzwOEnxYv1oEimFAAqnr4bcDEo2meEFEZQvH7YdDXMGUixfLzHC6KGfQ_iuXOgn48ogg/s16000/splash.png" /></a><br /><br /></p><p>In a prior post, we looked at how to inspect coverage as a text report and export coverage data using the PyVSC API, and view coverage graphically using the PyUCIS-Viewer. Recent enhancements have enabled the PyUCIS library to provide even more ways to manipulate coverage data. Over the next couple of posts, we’ll look at those enhancements.&nbsp;</p><p><b>New ‘ucis’ Command</b></p><p>PyUCIS is a library for working with the Accellera UCIS data model. It started as a library for other applications and libraries, such as PyVSC and the PyUCIS Viewer, to use to read and write data using the UCIS data model. Recent enhancements have added standalone functionality which can meaningfully be accessed from the command line.&nbsp;</p><p>You can find documentation for the ucis command and sub-commands in the <a href="https://pyucis.readthedocs.io/en/latest/commands.html">PyUCIS documentation</a>. Fundamentally, there are four key operations:</p><p></p><ul style="text-align: left;"><li>Convert coverage data from one format to another</li><li>Merge coverage data from multiple databases into a single database</li><li>Produce coverage reports in various formats</li><li>Obtain information about available coverage data and report formats</li></ul><p></p><p>These commands are just a starting point. They will be enhanced over time, and more commands may be added as well. If you have suggestions for new commands and/or new capabilities for existing commands, feel free to add an enhancement request on the <a href="https://github.com/fvutils/pyucis/issues">PyUCIS GitHub page</a>.</p><p><b>Plug-in Framework</b></p><p>PyUCIS has added a plug-in framework with support for database formats and report formats. The goal is to make commands operating on coverage data extensible extensible from the beginning, as well as to enable the set of supported coverage-data formats and report formats to be easily extended without changing PyUCIS.&nbsp; I’ll devote a future post to the plug-in framework. For now, the ucis command supports listing the available coverage-data and report plug-ins. For example:</p><p><span style="font-family: courier;">% ucis list-db-formats</span></p><p><span style="font-family: courier;">libucis - Reads coverage data via an implementation of the UCIS C API</span></p><p><span style="font-family: courier;">xml&nbsp; &nbsp; &nbsp;- Supports reading and writing UCIS XML interchange</span></p><p><span style="font-family: courier;">yaml&nbsp; &nbsp; - Reads coverage data from a YAML file</span></p><p><br /></p><p><b>New Input Format</b></p><p>One often-requested PyUCIS feature is the ability to merge coverage data from several input coverage databases into a single resulting coverage database. One of the first challenges I faced in implementing this functionality was how to write tests. The UCIS API is written with applications in mind. I’ve found it to be a pretty-verbose API when it comes to writing tests. Consequently, tests written directly in terms of the API aren’t particularly easy to follow from a code perspective.</p><p>I decided to define a YAML format to make it simpler to capture coverage data in an easy-to -read way. Initially, this was just for testing. However, it may also be a useful interchange format that is less verbose and complex (also, quite possibly, more simplistic) that the XML interchange format defined by the UCIS standard.</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdaeXpmEYuvyXmWxluRALy-WLMci6JOtkxwHv8pizdR3AZyMLLDcKkbEzSkaj6fT7b9U4yxc06CLKMX1xy6Cc_VM2Ntgk0A7ri3epneWGF8BKVgzafT4-ks3BwSeZVIqKeUZFdxnVAc9AIiIC9gS75isWG5JSdGHqlobAPQSLcodmUBOHPA0lTrC6n9Q/s250/yaml_coverage_spec.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="237" data-original-width="250" height="237" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdaeXpmEYuvyXmWxluRALy-WLMci6JOtkxwHv8pizdR3AZyMLLDcKkbEzSkaj6fT7b9U4yxc06CLKMX1xy6Cc_VM2Ntgk0A7ri3epneWGF8BKVgzafT4-ks3BwSeZVIqKeUZFdxnVAc9AIiIC9gS75isWG5JSdGHqlobAPQSLcodmUBOHPA0lTrC6n9Q/s1600/yaml_coverage_spec.png" width="250" /></a></div><div><br /></div><div>A simple coverage specification is shown above. This coverage data describes a covergroup type (my_cvg) with a single instance (i1). A single coverpoint (cp1) has two bins (b1, b2) of which one has a single hit and one has no hits. While this coverage specification was created to make setting of test coverage data simpler for a human, I believe it may also be useful as a simple coverage-interchange format. If you find it useful, please let the community know via the <a href="https://github.com/fvutils/pyucis/discussions">Discussion forum</a> on the PyUCIS GitHub page.</div><div><br /></div><div>You can find more details on the <a href="https://pyucis.readthedocs.io/en/latest/reference/yaml_coverage.html">YAML Coverage Data Format reference documentation</a> page.&nbsp;</div><p><b>Merging Coverage Data</b></p><p>One consistently-requested feature for PyUCIS is the ability to merge multiple databases into a single unified coverage database. PyUCIS now supports basic merge functionality. Currently, PyUCIS performs a union merge where all unique coverage features found in all the input databases are propagated to the output database. I anticipate that more merge algorithms will need to be added over time, but hopefully this is a good start.</p><p><br /></p><p>Let’s take a look at a very simple case. Let’s say we have two coverage-data sets shown below:</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg6vvu8qYu9EklhJd0IkaC4_Px0ZcCjPQZbFy1-TtgNviArsSrtHc-PN7rSRYJE2XpUo4uTHQ7YaBTY3D8_VcOjL3EuOsvVRKlwTvAQA6H9LRVLyL0bAtZdBSi3Wv51MQR4PzCk2WQj-EckBSwdhhcVylHWirWl36g7amzFFmC9jah_wc1YLYMtZmL9uA/s504/merge_input_data.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="245" data-original-width="504" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg6vvu8qYu9EklhJd0IkaC4_Px0ZcCjPQZbFy1-TtgNviArsSrtHc-PN7rSRYJE2XpUo4uTHQ7YaBTY3D8_VcOjL3EuOsvVRKlwTvAQA6H9LRVLyL0bAtZdBSi3Wv51MQR4PzCk2WQj-EckBSwdhhcVylHWirWl36g7amzFFmC9jah_wc1YLYMtZmL9uA/s16000/merge_input_data.png" /></a></div><br /><p>The structure of these two coverage databases is the same (same covergroup type, instance, and coverpoint). Each coverage database has 50% coverage. Let’s merge these two databases and report the coverage.</p><p><span style="font-family: courier;">% ucis merge -if yaml -o merge.xml coverage_1.ycdb coverage_2.ycdb</span></p><p>We specify the two input databases, as well as their format (yaml). We specify the output database as merge.xml.</p><p>The resulting coverage report on the merged database will report 100% coverage, as expected:</p><p><span style="font-family: courier;">% ucis report merge.xml</span></p><p><span style="font-family: courier;">TYPE i1 : 100.000000%</span></p><p><span style="font-family: courier;">&nbsp; &nbsp; CVP cp1 : 100.000000%</span></p><p><b>Reporting Coverage Data</b></p><p>Reporting is a key activity when working with coverage data. We’ve looked at the ability to browse coverage data graphically using the PyUCIS-Viewer, but getting a textual report is every bit as important. In addition to presenting information concisely, textual reports can be processed programmatically to extract key pieces of data.&nbsp;</p><p>We can list the currently-available report plugins using the ucis command:</p><p><span style="font-family: courier;">% ucis list-rpt-formats</span></p><p><span style="font-family: courier;">json - Produces a machine-readable JSON coverage report</span></p><p><span style="font-family: courier;">txt&nbsp; - Produces a human-readable textual coverage report</span></p><p><br /></p><p>The default report is textual. Let’s create a textual report on the YAML coverage-data above:</p><p><span style="font-family: courier;">% ucis report -if yaml coverage.ycdb&nbsp;</span></p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifRBJaI9kmG0at2fuySwAQRHqOrow3uiP17zEjyQmj0dU-N0z3ZJNIuxebRKzImTChJgFqOt1GvsbhS967pQ1gR42WDsn5QPTlcLx3WsuTlhF8oTs6bpQZAUlFdUrda7VrP9CJJ2hBxGzeReERNEC9fnSc_b2P0A79M4AQx-kIl2MgdAEEywcZmgVFBw/s244/txt_coverage_report.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="85" data-original-width="244" height="85" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifRBJaI9kmG0at2fuySwAQRHqOrow3uiP17zEjyQmj0dU-N0z3ZJNIuxebRKzImTChJgFqOt1GvsbhS967pQ1gR42WDsn5QPTlcLx3WsuTlhF8oTs6bpQZAUlFdUrda7VrP9CJJ2hBxGzeReERNEC9fnSc_b2P0A79M4AQx-kIl2MgdAEEywcZmgVFBw/s1600/txt_coverage_report.png" width="244" /></a></div><br /><p>Note that we need to specify the format of the input data (yaml). The result is a simple human-readable report of the coverage data in the database.</p><p>What if we wanted to post-process the data using a script? We certainly could extract what we need by parsing the output above, but working with data in a machine-readable format is often much simpler. Let’s report our data in JSON format:</p><p><span style="font-family: courier;">% ucis report -if yaml -of json coverage.ycdb</span>&nbsp;</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi1LqtUHblxEwKju_-m9tESy3trKWjJi_-y-NkYK73HhoU-Etm9xdqIwqzkS7E_ASMnNmu86N6dYm2GfSb8-WeHZdspBEWi3oM4CLmnnLr_Fjok8prBX5rg0r3ip4QQ7Z8H0meu74yU3PbLxcuPHw41KAx42h2pHHpeClfVe4O6TJyMrEgWX4eiDrNxtg/s862/json_coverage_report.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="862" data-original-width="409" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi1LqtUHblxEwKju_-m9tESy3trKWjJi_-y-NkYK73HhoU-Etm9xdqIwqzkS7E_ASMnNmu86N6dYm2GfSb8-WeHZdspBEWi3oM4CLmnnLr_Fjok8prBX5rg0r3ip4QQ7Z8H0meu74yU3PbLxcuPHw41KAx42h2pHHpeClfVe4O6TJyMrEgWX4eiDrNxtg/s16000/json_coverage_report.png" /></a></div><div>Obviously, the data is less compact and more verbose. But, reading this into a Python script for further post-processing is incredibly simple! If you’re interested in the JSON report format, have a look at the schema documentation &lt;https://pyucis.readthedocs.io/en/latest/reference/coverage_report_json.html&gt;.</div><div><br /></div><div>So, for now, PyUCIS supports two textual report formats, and would benefit from more report formats. For example, a plain HTML report and a fancy interactive web-based report. If someone in the community has the skills and is interested, the project would definitely be interested!</div><p><b>Next Steps</b></p><p>PyUCIS continues to evolve, adding a more more hopefully-useful features at a time. Stay tuned for a future post on the plug-in interface, and the addition of more coverage-database and report formats.&nbsp;</p><p><br /></p><div style="text-align: center;">Copyright 2022 Matthew Ballance</div><div><p style="font-variant-east-asian: normal; font-variant-numeric: normal; line-height: 16px; margin-bottom: 0in;"><span style="color: #666666;"><span face="Trebuchet MS, Trebuchet, Verdana, sans-serif"><span style="font-size: 9pt;"><i><span style="background: rgb(255, 255, 255);">The views and opinions expressed above are solely those of the author and do not represent those of my employer or any other party.</span></i></span></span></span></p></div>]]></content><author><name>Matthew Ballance</name></author><category term="functional coverage" /><category term="Python" /><category term="Altera" /><category term="UCIS" /><category term="Accellera" /><summary type="html"><![CDATA[&nbsp;In a prior post, we looked at how to inspect coverage as a text report and export coverage data using the PyVSC API, and view coverage graphically using the PyUCIS-Viewer. Recent enhancements have enabled the PyUCIS library to provide even more ways to manipulate coverage data. Over the next couple of posts, we’ll look at those enhancements.&nbsp;New ‘ucis’ CommandPyUCIS is a library for working with the Accellera UCIS data model. It started as a library for other applications and libraries, such as PyVSC and the PyUCIS Viewer, to use to read and write data using the UCIS data model. Recent enhancements have added standalone functionality which can meaningfully be accessed from the command line.&nbsp;You can find documentation for the ucis command and sub-commands in the PyUCIS documentation. Fundamentally, there are four key operations:Convert coverage data from one format to anotherMerge coverage data from multiple databases into a single databaseProduce coverage reports in various formatsObtain information about available coverage data and report formatsThese commands are just a starting point. They will be enhanced over time, and more commands may be added as well. If you have suggestions for new commands and/or new capabilities for existing commands, feel free to add an enhancement request on the PyUCIS GitHub page.Plug-in FrameworkPyUCIS has added a plug-in framework with support for database formats and report formats. The goal is to make commands operating on coverage data extensible extensible from the beginning, as well as to enable the set of supported coverage-data formats and report formats to be easily extended without changing PyUCIS.&nbsp; I’ll devote a future post to the plug-in framework. For now, the ucis command supports listing the available coverage-data and report plug-ins. For example:% ucis list-db-formatslibucis - Reads coverage data via an implementation of the UCIS C APIxml&nbsp; &nbsp; &nbsp;- Supports reading and writing UCIS XML interchangeyaml&nbsp; &nbsp; - Reads coverage data from a YAML fileNew Input FormatOne often-requested PyUCIS feature is the ability to merge coverage data from several input coverage databases into a single resulting coverage database. One of the first challenges I faced in implementing this functionality was how to write tests. The UCIS API is written with applications in mind. I’ve found it to be a pretty-verbose API when it comes to writing tests. Consequently, tests written directly in terms of the API aren’t particularly easy to follow from a code perspective.I decided to define a YAML format to make it simpler to capture coverage data in an easy-to -read way. Initially, this was just for testing. However, it may also be a useful interchange format that is less verbose and complex (also, quite possibly, more simplistic) that the XML interchange format defined by the UCIS standard.A simple coverage specification is shown above. This coverage data describes a covergroup type (my_cvg) with a single instance (i1). A single coverpoint (cp1) has two bins (b1, b2) of which one has a single hit and one has no hits. While this coverage specification was created to make setting of test coverage data simpler for a human, I believe it may also be useful as a simple coverage-interchange format. If you find it useful, please let the community know via the Discussion forum on the PyUCIS GitHub page.You can find more details on the YAML Coverage Data Format reference documentation page.&nbsp;Merging Coverage DataOne consistently-requested feature for PyUCIS is the ability to merge multiple databases into a single unified coverage database. PyUCIS now supports basic merge functionality. Currently, PyUCIS performs a union merge where all unique coverage features found in all the input databases are propagated to the output database. I anticipate that more merge algorithms will need to be added over time, but hopefully this is a good start.Let’s take a look at a very simple case. Let’s say we have two coverage-data sets shown below:The structure of these two coverage databases is the same (same covergroup type, instance, and coverpoint). Each coverage database has 50% coverage. Let’s merge these two databases and report the coverage.% ucis merge -if yaml -o merge.xml coverage_1.ycdb coverage_2.ycdbWe specify the two input databases, as well as their format (yaml). We specify the output database as merge.xml.The resulting coverage report on the merged database will report 100% coverage, as expected:% ucis report merge.xmlTYPE i1 : 100.000000%&nbsp; &nbsp; CVP cp1 : 100.000000%Reporting Coverage DataReporting is a key activity when working with coverage data. We’ve looked at the ability to browse coverage data graphically using the PyUCIS-Viewer, but getting a textual report is every bit as important. In addition to presenting information concisely, textual reports can be processed programmatically to extract key pieces of data.&nbsp;We can list the currently-available report plugins using the ucis command:% ucis list-rpt-formatsjson - Produces a machine-readable JSON coverage reporttxt&nbsp; - Produces a human-readable textual coverage reportThe default report is textual. Let’s create a textual report on the YAML coverage-data above:% ucis report -if yaml coverage.ycdb&nbsp;Note that we need to specify the format of the input data (yaml). The result is a simple human-readable report of the coverage data in the database.What if we wanted to post-process the data using a script? We certainly could extract what we need by parsing the output above, but working with data in a machine-readable format is often much simpler. Let’s report our data in JSON format:% ucis report -if yaml -of json coverage.ycdb&nbsp;Obviously, the data is less compact and more verbose. But, reading this into a Python script for further post-processing is incredibly simple! If you’re interested in the JSON report format, have a look at the schema documentation &lt;https://pyucis.readthedocs.io/en/latest/reference/coverage_report_json.html&gt;.So, for now, PyUCIS supports two textual report formats, and would benefit from more report formats. For example, a plain HTML report and a fancy interactive web-based report. If someone in the community has the skills and is interested, the project would definitely be interested!Next StepsPyUCIS continues to evolve, adding a more more hopefully-useful features at a time. Stay tuned for a future post on the plug-in interface, and the addition of more coverage-database and report formats.&nbsp;Copyright 2022 Matthew BallanceThe views and opinions expressed above are solely those of the author and do not represent those of my employer or any other party.]]></summary></entry><entry><title type="html">Tools and Techniques to Improve YAML-File Usability</title><link href="https://bitsbytesgates.com/2022/06/27/tools-and-techniques-to-improve-yaml.html" rel="alternate" type="text/html" title="Tools and Techniques to Improve YAML-File Usability" /><published>2022-06-27T00:31:00+00:00</published><updated>2022-06-27T00:31:00+00:00</updated><id>https://bitsbytesgates.com/2022/06/27/tools-and-techniques-to-improve-yaml</id><content type="html" xml:base="https://bitsbytesgates.com/2022/06/27/tools-and-techniques-to-improve-yaml.html"><![CDATA[<p style="text-align: center;">&nbsp;</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeUm4wAbSaE2GiFPDoa1M_6U6OqGwT5jusXQtor0CaVBSy_OYi1JCUko102SmcL0MBhnNOpBpQznLycMXV4T3OyDLnYaicBSEAyTMzPIyjswUegXQPOutmCFliEKg3Njs3gollZawm6YiJq1Q2fb2APy3gJlnwSgON-q__3hMqnytgYdhA_1YWKHuGZg/s540/splash.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="300" data-original-width="540" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeUm4wAbSaE2GiFPDoa1M_6U6OqGwT5jusXQtor0CaVBSy_OYi1JCUko102SmcL0MBhnNOpBpQznLycMXV4T3OyDLnYaicBSEAyTMzPIyjswUegXQPOutmCFliEKg3Njs3gollZawm6YiJq1Q2fb2APy3gJlnwSgON-q__3hMqnytgYdhA_1YWKHuGZg/s16000/splash.png" /></a></div><br /><p></p><p>This blog post is a bit of a departure from many that I’ve created for this blog. Most of my blog posts are about things I’ve created. This post is about a collection of tools that I use in developing the things I create.&nbsp;&nbsp;</p><p>I’ve recently come back to working on some new features in PyUCIS, the Python library for accessing functional coverage data. PyUCIS provides an implementation of the Accellera UCIS, and several back-end implementations. Good tests are critical when developing new functionality and, in the case of PyUCIS, tests rely on having coverage data to manipulate. As it so happens, while the UCIS API is good for providing tools access to coverage data, it’s not a great interface for humans (and, specifically, for test writers). What test writers need is a very concise and easy-to-read mechanism to capture the coverage data on which the library should operate. How should we capture this data? A couple decades ago, I might have toyed with developing a small language grammar to capture exactly the data I needed. Today, using a mark-up language like YAML or JSON to capture such data is my go-to approach.</p><p><b>YAML - A Data Format for Everything and Nothing</b></p><p>There are many reasons for the popularity of YAML for capturing application-configuration information, such as what we need to capture coverage data. YAML’s structure of a nested series of mappings and lists lends itself to easily capturing all manner of data. Furthermore, support for reading and writing YAML is available the vast majority of programming languages.&nbsp;</p><p>However, the ease with which we can define new data formats, and create simple processors to accept data captured in these formats can be deceptive. It’s tempting to think that, because YAML defines a standard set of structures for capturing data, users will find it easy and intuitive to capture data in our specific format. It’s tempting to think that our format might be so simple that only a little documentation with a few examples may be more than sufficient. The truth, however, is that making our application-specific data format usable requires us to do many of the same things that we would have to do if we defined a custom language. Our YAML-based format must be fully-documented, our data processors must be robust in accepting valid content, rejecting invalid content, and not silently ignore unrecognized input. I’ve had the painful experience of coming back to a project (yep, one that I created) after a few months away and having to dig into the YAML-processing code to remember the data format.&nbsp;</p><p>The apparent ease with which we can access data from our application code is also a bit deceptive. Most YAML-reading libraries provide access to the data through a hierarchy of maps and list that mirrors the structure of the data. Depending on how we might want to subsequently process the data, we might first copy it to a set of custom data object, or we might access it by directly querying the maps and lists. In both cases,&nbsp;&nbsp;</p><p>The really thing about YAML, though, is that many tools exist precisely to help make a custom YAML-based format easy to use and reliable to implement. For the most part, I will focus on tools available in the Python ecosystem. However, many of these tools are equally-useful in when implementing applications in other languages. YAML-processing libraries exist in other language ecosystems as well.</p><p><b>PyUCIS Coverage Example</b></p><p>Let’s look at the following tools in the context of the YAML data format that PyUCIS uses to capture coverage data for testing. Here’s a small example:</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgfhgdr7Od7ZdsjwfNr912DRS5sh6z49-lKhnUCNzdbptH-ovF_YCyBbzDmzNrlQPGib_OvrMjpVwLqhe61HSwt39QSQ_p_TQXxkTcGpELesLCoUAjjXb2p-KA3Wwt3R2DBcIdDggHTN7Vjfd_O4rl_NJq2ES_TMpPT3lIiXy-QoDg4-AEbcUqwG3POzw/s393/coverage_yaml.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="393" data-original-width="245" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgfhgdr7Od7ZdsjwfNr912DRS5sh6z49-lKhnUCNzdbptH-ovF_YCyBbzDmzNrlQPGib_OvrMjpVwLqhe61HSwt39QSQ_p_TQXxkTcGpELesLCoUAjjXb2p-KA3Wwt3R2DBcIdDggHTN7Vjfd_O4rl_NJq2ES_TMpPT3lIiXy-QoDg4-AEbcUqwG3POzw/s16000/coverage_yaml.png" /></a></div><div class="separator" style="clear: both; text-align: center;"><br /></div><div class="separator" style="clear: both; text-align: left;">The root of data in the document is named ‘coverage’. Currently, ‘coverage’ consists of a series of covergroup types under the ‘covergroups’ section. Each covergroup type has a name and a list of instances. A covergroup instance holds coverpoints, which have bins in which hit counts are stored. The format is intended to make it very simple to capture coverage data for use in testing coverage reporting and merging tools. It’s also not a bad format to bring in coverage data from other tools.</div><div class="separator" style="clear: both; text-align: left;"><br /></div><div class="separator" style="clear: both; text-align: left;"><b>PyYAML</b></div><div class="separator" style="clear: both; text-align: left;"><div class="separator" style="clear: both;">It’s incredibly simple to read data from a YAML-formatted file. I’ve tended to use the PyYAML Python library, but there are many other choices. With PyYAML, reading in file like the example above is incredibly simple:</div><div class="separator" style="clear: both;"><br /></div><div class="separator" style="clear: both;"><span style="font-family: courier;">import yaml</span></div><div class="separator" style="clear: both;"><span style="font-family: courier;"><br /></span></div><div class="separator" style="clear: both;"><span style="font-family: courier;">with open(“coverage.yaml”, “r”) as fp:</span></div><div class="separator" style="clear: both;"><span style="font-family: courier;"><span style="white-space: pre;">	</span>yaml_data = yaml.load(fp, Loader=yaml.FullLoader)</span></div><div class="separator" style="clear: both;"><br /></div><div class="separator" style="clear: both;">The result is a hierarchy of Python dictionaries and lists containing the data from the file, which we can walk by indexing. For example:</div><div class="separator" style="clear: both;"><br /></div><div class="separator" style="clear: both;"><span style="font-family: courier;">for cg in yaml_data[“coverage”][“covergroups”]:</span></div><div class="separator" style="clear: both;"><span style="font-family: courier;">&nbsp; print(“Covergroup type: %s” % cg[“name”])</span></div><div style="font-weight: bold;"><br /></div></div><b>JSON Schema</b><div><b><br /></b><div><div>One thing we will always want to ensure is that a coverage file conforms to the required syntax. One way to do this is to hand-code a validator that walks through the data structure from the parser and confirms that required elements are present and unexpected elements are not. Another is to create a schema for the document and use a validation library.&nbsp;</div><div>We will create a schema for the coverage file format. Creating a schema is the most efficient way to enable validation of our file format. In addition, once we have a schema, there are many other ways that we can use it.</div><div>Despite the fact that we are using YAML for our data, we will capture the schema using json-schema.</div><div style="font-weight: bold;"><br /></div><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6PcfTk_RTY780zR_cGW4akORUYkp9K9EeglDvrt627RUf1Zbb065vcMHve20Nf3AztmLwPWto_x1AJbk33EX2oGC4L-zoxyrxjQ3XSmRNG018INzG66fly9nF1yRuyjXV68g1KxA7QkBoZDUs0t6vPQa615j0-n-b0XgeKN0Bo_UkTSpYgAt5Lw1I8w/s759/schema_ex.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="650" data-original-width="759" height="549" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6PcfTk_RTY780zR_cGW4akORUYkp9K9EeglDvrt627RUf1Zbb065vcMHve20Nf3AztmLwPWto_x1AJbk33EX2oGC4L-zoxyrxjQ3XSmRNG018INzG66fly9nF1yRuyjXV68g1KxA7QkBoZDUs0t6vPQa615j0-n-b0XgeKN0Bo_UkTSpYgAt5Lw1I8w/w640-h549/schema_ex.png" width="640" /></a></div><br /><p>The example above is the first part of the schema for our coverage data. It’s a bit verbose, but notice a few things:</p><p></p><ul style="text-align: left;"><li>The root of our document is an object (a dictionary with keys and values) with a single root element coverage</li><li>A coverage section is an array of covergroupType. Note that the schema refers to this separate declaration, which allows it to be referenced and reused in multiple locations.</li><li>covergroupType&nbsp; specifies that it is an object that has three possible sub-entries (name, weight, instances)</li><li>Of these possible sub-entries, only ‘name’ is required</li></ul><p></p><p>This merely scratches the surface of what is possible to describe with json-schema. There’s a bit of a learning curve, but my experience has been that it’s pretty straightforward once you learn a few fundamentals.</p><p>Once we have a schema, we can validate the data-structure returned from the YAML parser against the schema using the jsonschema Python library.</p><p><span style="font-family: courier;">import yaml</span></p><p><span style="font-family: courier;">import json</span></p><p><span style="font-family: courier;">import jsonschema</span></p><p><span style="font-family: courier;"><br /></span></p><p><span style="font-family: courier;">with open(“coverage.yaml”, “r”) as fp:</span></p><p><span style="font-family: courier;"><span style="white-space: pre;">	</span>yaml_data = yaml.load(fp, Loader=yaml.FullLoader)</span></p><p><span style="font-family: courier;">with open(“coverage_schema.json”, “r”) as fp:</span></p><p><span style="font-family: courier;">&nbsp; &nbsp; &nbsp;schema = json.load(fp)</span></p><p><span style="font-family: courier;">jsonschema.validate(instance=yaml_data, schema=schema)</span></p><p><br /></p><p>Validating a document prior to attempting to process the data structure from the YAML parser allows us to simplify our processing code because we can assuming that the structure of the data is correct.</p><p><b>Python-JsonSchema-Objects</b></p><p>The simplest way to obtain data is to operate directly on the data structure returned by the parser.&nbsp;</p><p>While&nbsp; this is simple and straightforward, there is at least one significant pitfall: it’s almost never a good idea to use string literals. Consider what happens if we change the name of one of our optional keywords just a bit.&nbsp;</p><p><span style="font-family: courier;">weight=1</span></p><p><span style="font-family: courier;">if “weight” in cg.keys():</span></p><p><span style="font-family: courier;">&nbsp; weight = cg[“weight”]</span></p><p>If we neglect to update all the locations in our code that use this string literal, some of our data will simply be silently ignored. Clearly, there are some incremental steps we can take – for example, defining a constant for each string literal, making it easier to update.&nbsp;</p><p>Another approach is to work with classes that are generated from our schema. This approach makes it much more likely that we’ll find data misuse issues earlier, and has the added benefit of giving us actual classes to work with. I recently discovered the python-jsonschema-objects project, and used it on PyUCIS for the first time. Thus far, I’m extremely impressed and plan to use it more broadly.</p><p>The short version of how it works is as follows. python-jsonschema-objects works off of a JSON-schema document. Each section of the schema (eg covergroupType) should be given a title from which the class name will be derived. Call python-schema-objects to build a Python namespace containing class declarations. Your code can then create classes and populate them – either directly or from parsed data.</p><p>It looks like this:</p><p><span style="font-family: courier;">import python_jsonscehma_objects as pjs</span></p><p><span style="font-family: courier;"><br /></span></p><p><span style="font-family: courier;">builder = pjs.ObjectBuilder(schema)</span></p><p><span style="font-family: courier;">ns = builder.build_classes()</span></p><p><span style="font-family: courier;">cov = ns.CoverageData().from_json(json.dumps(yaml_data))</span></p><p><span style="font-family: courier;"><br /></span></p><p><span style="font-family: courier;">if cov.covergroups is not None:</span></p><p><span style="font-family: courier;">&nbsp; for cg in cov.covergroups:</span></p><p><span style="font-family: courier;">&nbsp; &nbsp; print(“cg: %s” % cg.name)</span></p><p><br /></p><p></p><p>The ‘ns’ object above contains the classes derived from the definitions in the schema. We can create an instance of a CoverageData class that contains our schema-compliant data just by loading the JSON representation of that YAML data. From there on, we can directly access our data as class fields.</p><p><b>VSCode YAML Editor</b></p><p>Thus far, we’ve primarily looked at tools that help the developer. The final two tools are focused on improving the user experience. Both leverage our document schema.</p><p>Visual Studio Code (VSCode) is a free integrated development environment (IDE) produced by Microsoft. In open source terms, it’s free as in beer. My understanding is that there are compatible truly open source versions as well. As with many IDEs, there is an extensive ecosystem of plug-ins available&nbsp; to assist in developing different types of code. One of those plug-ins supports YAML development.</p><p>So, what does having a schema allow an intelligent editor to do for us? Well, for one thing, it can check the validity of a YAML file as we type it and allow us to fix errors as we go.&nbsp;</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhmCCgG5L9mMcINvPYrvpocZhzGm-RKNB9pw1alxT6ady2z5MXX3dKUiJ5eJoIizFGYo1LidRk8zvopvVjuA0c0ieimBq4KAmwUHlwcEIV6ngLGPmo9zYFEi-Fn1wpuDeGo708acq1JLH0hjqHMqv0OZV7EK2-Jj5B8hXyJeTh9GravZTTLZw0Q6JFRrQ/s602/vscode_autocomplete.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="232" data-original-width="602" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhmCCgG5L9mMcINvPYrvpocZhzGm-RKNB9pw1alxT6ady2z5MXX3dKUiJ5eJoIizFGYo1LidRk8zvopvVjuA0c0ieimBq4KAmwUHlwcEIV6ngLGPmo9zYFEi-Fn1wpuDeGo708acq1JLH0hjqHMqv0OZV7EK2-Jj5B8hXyJeTh9GravZTTLZw0Q6JFRrQ/s16000/vscode_autocomplete.png" /></a></div><br /><p>It can suggest what content is valid based on where we are in the document. For example, the schema states that we can have coverpoints and crosses elements inside an instances section. The editor knows this, and prompts us with what it knows is valid.</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxqBocp04oJKSm7KgrkmvArNER7hJZKhT-t0ptwZhRlXUEQY7YsdYrqB7iIYvsXaLVTVNmf-5TsgI-QkhG8WCDTwHfqueqlEw3mto07zL8mnHqQWBlNsLh9eDEhkT7Jfot4nhjP0j3dZCLI7B2K5nlAbQQenJY-y8RBWHZ2xwzMLEj9jglgOu1-bEu3A/s407/vscode_hover.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="197" data-original-width="407" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxqBocp04oJKSm7KgrkmvArNER7hJZKhT-t0ptwZhRlXUEQY7YsdYrqB7iIYvsXaLVTVNmf-5TsgI-QkhG8WCDTwHfqueqlEw3mto07zL8mnHqQWBlNsLh9eDEhkT7Jfot4nhjP0j3dZCLI7B2K5nlAbQQenJY-y8RBWHZ2xwzMLEj9jglgOu1-bEu3A/s16000/vscode_hover.png" /></a></div><div class="separator" style="clear: both; text-align: center;"><br /></div><div class="separator" style="clear: both; text-align: left;"><br /></div><div class="separator" style="clear: both; text-align: left;">It can also shows us information about the document section we’re hovering over. Features like these can significantly improve ease of use, making it easier for your users to get started.&nbsp;</div><div><br /></div><b>Sphinx Json Schema</b></div><div><div>Over time, I’ve really come to love Sphinx-Doc for documenting projects. I really like the way it enables combining human-created content with content extracted from the implementation code. I think it finds a great middle ground between tools that fully-generate documentation from code comments and documentation that is fully human created.</div><div><br /></div><div>Not surprisingly, Sphinx has an extension that supports extracting data from a JSON schema. The extracted data provides a great synopsis of the data format. It’s very likely that you’ll want to add in a bit of extra description on top of what makes sense to put directly in the schema documentation.</div><div><br /></div><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjyLqGYA4KAKB2iLodt0A-lKIYoYwG9i8pSM26Lcw85XZ5XUlctJtYYZPTec55urV5x_QBb20etVf2M60i0MsWyf7Sip7l_ZDU6hzr4zgSwT7o8il8p5f-FZyGN89VcWuCXnJlDWRK-a6b6RMXli-vlTnF8UW3H_aPcBxUVSGhOb-8IGmQaMlx83Oq4A/s756/sphinx-doc.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="526" data-original-width="756" height="446" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjyLqGYA4KAKB2iLodt0A-lKIYoYwG9i8pSM26Lcw85XZ5XUlctJtYYZPTec55urV5x_QBb20etVf2M60i0MsWyf7Sip7l_ZDU6hzr4zgSwT7o8il8p5f-FZyGN89VcWuCXnJlDWRK-a6b6RMXli-vlTnF8UW3H_aPcBxUVSGhOb-8IGmQaMlx83Oq4A/w640-h446/sphinx-doc.png" width="640" /></a></div><br /><div>The heading and table above are the result of using sphinx-jsonschema to document the covergroupType entity in our coverage schema. All the sub-elements are documented, and complex sub-elements have links to the relevant documentation. The text below the table is description that was manually added to the document. As with most Sphinx plug-ins, the jsonschema plug-in saves the developer from the laborious work of documenting the structure of the document.</div><div><br /></div><div><b>Conclusion</b></div><div>YAML is an excellent textual format structure for capturing structured data in a human readable way. Making use of a few readily-available free and open-source tools can make domain-specific YAML-based file formats much easier and reliable to implement, and can dramatically increase their usability. Next time you start sketching out a YAML-file format to use in your application, I’d encourage you to also reach for some of these tools. Your users will thank you – even if the sole user ends up being you!</div><div><br /></div><div><i><b>References</b></i></div><div><div><b style="font-style: italic;">&nbsp; &nbsp; • </b>PyUCIS GitHub - https://github.com/fvutils/pyucis</div><div>&nbsp; &nbsp; • PyUCIS Docs - https://fvutils.github.io/pyucis/</div><div>&nbsp; &nbsp; • json-schema - https://json-schema.org/</div><div>&nbsp; &nbsp; • jsonschema Python library - https://pypi.org/project/jsonschema/</div><div>&nbsp; &nbsp; • RedHat YAML editor for VSCode - https://github.com/redhat-developer/vscode-yaml</div><div>&nbsp; &nbsp; • sphinx-jsonschema - https://sphinx-jsonschema.readthedocs.io/en/latest/</div><div><br /></div><div><br /></div><div><br /></div><div><div style="text-align: center;">Copyright 2022 Matthew Ballance</div><div><p style="font-variant-east-asian: normal; font-variant-numeric: normal; line-height: 16px; margin-bottom: 0in;"><span style="color: #666666;"><span face="Trebuchet MS, Trebuchet, Verdana, sans-serif"><span style="font-size: 9pt;"><i><span style="background: rgb(255, 255, 255);">The views and opinions expressed above are solely those of the author and do not represent those of my employer or any other party.</span></i></span></span></span></p></div></div><div><br /></div></div><p align="left" style="background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; line-height: 1px; margin-bottom: 0in;"><br /></p><p align="left" style="background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; line-height: 1px; margin-bottom: 0in;"><br /></p><p><br /></p></div></div>]]></content><author><name>Matthew Ballance</name></author><category term="Python" /><category term="YAML" /><category term="JSON" /><category term="Schema" /><summary type="html"><![CDATA[&nbsp;This blog post is a bit of a departure from many that I’ve created for this blog. Most of my blog posts are about things I’ve created. This post is about a collection of tools that I use in developing the things I create.&nbsp;&nbsp;I’ve recently come back to working on some new features in PyUCIS, the Python library for accessing functional coverage data. PyUCIS provides an implementation of the Accellera UCIS, and several back-end implementations. Good tests are critical when developing new functionality and, in the case of PyUCIS, tests rely on having coverage data to manipulate. As it so happens, while the UCIS API is good for providing tools access to coverage data, it’s not a great interface for humans (and, specifically, for test writers). What test writers need is a very concise and easy-to-read mechanism to capture the coverage data on which the library should operate. How should we capture this data? A couple decades ago, I might have toyed with developing a small language grammar to capture exactly the data I needed. Today, using a mark-up language like YAML or JSON to capture such data is my go-to approach.YAML - A Data Format for Everything and NothingThere are many reasons for the popularity of YAML for capturing application-configuration information, such as what we need to capture coverage data. YAML’s structure of a nested series of mappings and lists lends itself to easily capturing all manner of data. Furthermore, support for reading and writing YAML is available the vast majority of programming languages.&nbsp;However, the ease with which we can define new data formats, and create simple processors to accept data captured in these formats can be deceptive. It’s tempting to think that, because YAML defines a standard set of structures for capturing data, users will find it easy and intuitive to capture data in our specific format. It’s tempting to think that our format might be so simple that only a little documentation with a few examples may be more than sufficient. The truth, however, is that making our application-specific data format usable requires us to do many of the same things that we would have to do if we defined a custom language. Our YAML-based format must be fully-documented, our data processors must be robust in accepting valid content, rejecting invalid content, and not silently ignore unrecognized input. I’ve had the painful experience of coming back to a project (yep, one that I created) after a few months away and having to dig into the YAML-processing code to remember the data format.&nbsp;The apparent ease with which we can access data from our application code is also a bit deceptive. Most YAML-reading libraries provide access to the data through a hierarchy of maps and list that mirrors the structure of the data. Depending on how we might want to subsequently process the data, we might first copy it to a set of custom data object, or we might access it by directly querying the maps and lists. In both cases,&nbsp;&nbsp;The really thing about YAML, though, is that many tools exist precisely to help make a custom YAML-based format easy to use and reliable to implement. For the most part, I will focus on tools available in the Python ecosystem. However, many of these tools are equally-useful in when implementing applications in other languages. YAML-processing libraries exist in other language ecosystems as well.PyUCIS Coverage ExampleLet’s look at the following tools in the context of the YAML data format that PyUCIS uses to capture coverage data for testing. Here’s a small example:The root of data in the document is named ‘coverage’. Currently, ‘coverage’ consists of a series of covergroup types under the ‘covergroups’ section. Each covergroup type has a name and a list of instances. A covergroup instance holds coverpoints, which have bins in which hit counts are stored. The format is intended to make it very simple to capture coverage data for use in testing coverage reporting and merging tools. It’s also not a bad format to bring in coverage data from other tools.PyYAMLIt’s incredibly simple to read data from a YAML-formatted file. I’ve tended to use the PyYAML Python library, but there are many other choices. With PyYAML, reading in file like the example above is incredibly simple:import yamlwith open(“coverage.yaml”, “r”) as fp: yaml_data = yaml.load(fp, Loader=yaml.FullLoader)The result is a hierarchy of Python dictionaries and lists containing the data from the file, which we can walk by indexing. For example:for cg in yaml_data[“coverage”][“covergroups”]:&nbsp; print(“Covergroup type: %s” % cg[“name”])JSON SchemaOne thing we will always want to ensure is that a coverage file conforms to the required syntax. One way to do this is to hand-code a validator that walks through the data structure from the parser and confirms that required elements are present and unexpected elements are not. Another is to create a schema for the document and use a validation library.&nbsp;We will create a schema for the coverage file format. Creating a schema is the most efficient way to enable validation of our file format. In addition, once we have a schema, there are many other ways that we can use it.Despite the fact that we are using YAML for our data, we will capture the schema using json-schema.The example above is the first part of the schema for our coverage data. It’s a bit verbose, but notice a few things:The root of our document is an object (a dictionary with keys and values) with a single root element coverageA coverage section is an array of covergroupType. Note that the schema refers to this separate declaration, which allows it to be referenced and reused in multiple locations.covergroupType&nbsp; specifies that it is an object that has three possible sub-entries (name, weight, instances)Of these possible sub-entries, only ‘name’ is requiredThis merely scratches the surface of what is possible to describe with json-schema. There’s a bit of a learning curve, but my experience has been that it’s pretty straightforward once you learn a few fundamentals.Once we have a schema, we can validate the data-structure returned from the YAML parser against the schema using the jsonschema Python library.import yamlimport jsonimport jsonschemawith open(“coverage.yaml”, “r”) as fp: yaml_data = yaml.load(fp, Loader=yaml.FullLoader)with open(“coverage_schema.json”, “r”) as fp:&nbsp; &nbsp; &nbsp;schema = json.load(fp)jsonschema.validate(instance=yaml_data, schema=schema)Validating a document prior to attempting to process the data structure from the YAML parser allows us to simplify our processing code because we can assuming that the structure of the data is correct.Python-JsonSchema-ObjectsThe simplest way to obtain data is to operate directly on the data structure returned by the parser.&nbsp;While&nbsp; this is simple and straightforward, there is at least one significant pitfall: it’s almost never a good idea to use string literals. Consider what happens if we change the name of one of our optional keywords just a bit.&nbsp;weight=1if “weight” in cg.keys():&nbsp; weight = cg[“weight”]If we neglect to update all the locations in our code that use this string literal, some of our data will simply be silently ignored. Clearly, there are some incremental steps we can take – for example, defining a constant for each string literal, making it easier to update.&nbsp;Another approach is to work with classes that are generated from our schema. This approach makes it much more likely that we’ll find data misuse issues earlier, and has the added benefit of giving us actual classes to work with. I recently discovered the python-jsonschema-objects project, and used it on PyUCIS for the first time. Thus far, I’m extremely impressed and plan to use it more broadly.The short version of how it works is as follows. python-jsonschema-objects works off of a JSON-schema document. Each section of the schema (eg covergroupType) should be given a title from which the class name will be derived. Call python-schema-objects to build a Python namespace containing class declarations. Your code can then create classes and populate them – either directly or from parsed data.It looks like this:import python_jsonscehma_objects as pjsbuilder = pjs.ObjectBuilder(schema)ns = builder.build_classes()cov = ns.CoverageData().from_json(json.dumps(yaml_data))if cov.covergroups is not None:&nbsp; for cg in cov.covergroups:&nbsp; &nbsp; print(“cg: %s” % cg.name)The ‘ns’ object above contains the classes derived from the definitions in the schema. We can create an instance of a CoverageData class that contains our schema-compliant data just by loading the JSON representation of that YAML data. From there on, we can directly access our data as class fields.VSCode YAML EditorThus far, we’ve primarily looked at tools that help the developer. The final two tools are focused on improving the user experience. Both leverage our document schema.Visual Studio Code (VSCode) is a free integrated development environment (IDE) produced by Microsoft. In open source terms, it’s free as in beer. My understanding is that there are compatible truly open source versions as well. As with many IDEs, there is an extensive ecosystem of plug-ins available&nbsp; to assist in developing different types of code. One of those plug-ins supports YAML development.So, what does having a schema allow an intelligent editor to do for us? Well, for one thing, it can check the validity of a YAML file as we type it and allow us to fix errors as we go.&nbsp;It can suggest what content is valid based on where we are in the document. For example, the schema states that we can have coverpoints and crosses elements inside an instances section. The editor knows this, and prompts us with what it knows is valid.It can also shows us information about the document section we’re hovering over. Features like these can significantly improve ease of use, making it easier for your users to get started.&nbsp;Sphinx Json SchemaOver time, I’ve really come to love Sphinx-Doc for documenting projects. I really like the way it enables combining human-created content with content extracted from the implementation code. I think it finds a great middle ground between tools that fully-generate documentation from code comments and documentation that is fully human created.Not surprisingly, Sphinx has an extension that supports extracting data from a JSON schema. The extracted data provides a great synopsis of the data format. It’s very likely that you’ll want to add in a bit of extra description on top of what makes sense to put directly in the schema documentation.The heading and table above are the result of using sphinx-jsonschema to document the covergroupType entity in our coverage schema. All the sub-elements are documented, and complex sub-elements have links to the relevant documentation. The text below the table is description that was manually added to the document. As with most Sphinx plug-ins, the jsonschema plug-in saves the developer from the laborious work of documenting the structure of the document.ConclusionYAML is an excellent textual format structure for capturing structured data in a human readable way. Making use of a few readily-available free and open-source tools can make domain-specific YAML-based file formats much easier and reliable to implement, and can dramatically increase their usability. Next time you start sketching out a YAML-file format to use in your application, I’d encourage you to also reach for some of these tools. Your users will thank you – even if the sole user ends up being you!References&nbsp; &nbsp; • PyUCIS GitHub - https://github.com/fvutils/pyucis&nbsp; &nbsp; • PyUCIS Docs - https://fvutils.github.io/pyucis/&nbsp; &nbsp; • json-schema - https://json-schema.org/&nbsp; &nbsp; • jsonschema Python library - https://pypi.org/project/jsonschema/&nbsp; &nbsp; • RedHat YAML editor for VSCode - https://github.com/redhat-developer/vscode-yaml&nbsp; &nbsp; • sphinx-jsonschema - https://sphinx-jsonschema.readthedocs.io/en/latest/Copyright 2022 Matthew BallanceThe views and opinions expressed above are solely those of the author and do not represent those of my employer or any other party.]]></summary></entry><entry><title type="html">PyVSC: Working with Coverage Data</title><link href="https://bitsbytesgates.com/2022/06/12/pyvsc-working-with-coverage-data.html" rel="alternate" type="text/html" title="PyVSC: Working with Coverage Data" /><published>2022-06-12T23:01:00+00:00</published><updated>2022-06-12T23:01:00+00:00</updated><id>https://bitsbytesgates.com/2022/06/12/pyvsc-working-with-coverage-data</id><content type="html" xml:base="https://bitsbytesgates.com/2022/06/12/pyvsc-working-with-coverage-data.html"><![CDATA[<p>&nbsp;</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjq5wnj9yN65OUU7dsp8gm5PRovjwCZlQubAsXkkaEis47rG46JBgh0gaNcbGqXjUX0HbjZQd1nRKrIChY0X6x_pVn_vbvGHNWxKyFDRiEATtKA-HWvSGjqViM03kqSmXfOsXtEI1NrobfqM6Q1E-rAkNKXk4RqeJfAIR3Wi_tLDYutBFSiDxMU2VjOFg/s540/pyvsc_coverage.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="300" data-original-width="540" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjq5wnj9yN65OUU7dsp8gm5PRovjwCZlQubAsXkkaEis47rG46JBgh0gaNcbGqXjUX0HbjZQd1nRKrIChY0X6x_pVn_vbvGHNWxKyFDRiEATtKA-HWvSGjqViM03kqSmXfOsXtEI1NrobfqM6Q1E-rAkNKXk4RqeJfAIR3Wi_tLDYutBFSiDxMU2VjOFg/s16000/pyvsc_coverage.png" /></a></div><br /><p></p><p>I’ve been investing some time in documentation updates this weekend, after a couple of <a href="https://github.com/fvutils/pyvsc">PyVSC</a> users pointed out some under-described aspects of the PyVSC coverage flow. Given that these areas were under-documented in the past, it seemed a good opportunity to highlight what can be done with functional coverage data once it is sampled by a PyVSC covergroup.</p><p>So, we’ve described some functional coverage goals using a PyVSC covergroup and coverpoints, created a covergroup instance, and sampled some coverage data – perhaps it was randomly-generated stimulus or data sampled from a monitor. What now?</p><p><b>Runtime Coverage API</b></p><p>One simple thing we can do is to query coverage achieved using the coverage APIs implemented by PyVSC covergroup classes. The `get_coverage` method returns the coverage achieved by all instances of a covergroup type. The `get_inst_coverage` method returns the coverage achieved by the specified covergroup instance.</p><p>Let’s look at an example:</p><div class="separator" style="clear: both; text-align: left;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgp4sxGCmgvfSB9S5pp78tiKP9ANcuyBKm63jyX6Ba4FzkbWym3MKXtZm7XHklK37Q85x1d-B2WjNvVk4emQP6dQqtWWRBFb1QXRiB2osalTHrhv1_QJqVGejSBiGVBm4-z0QpU_wXceqAAilxYOfPOTI9aFJac3NmpNevdRnePWsu78MXv-8kluu0ppA/s696/CoverageMethodsExample.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="446" data-original-width="696" height="410" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgp4sxGCmgvfSB9S5pp78tiKP9ANcuyBKm63jyX6Ba4FzkbWym3MKXtZm7XHklK37Q85x1d-B2WjNvVk4emQP6dQqtWWRBFb1QXRiB2osalTHrhv1_QJqVGejSBiGVBm4-z0QpU_wXceqAAilxYOfPOTI9aFJac3NmpNevdRnePWsu78MXv-8kluu0ppA/w640-h410/CoverageMethodsExample.png" width="640" /></a></div><br /><p>In the example above, we define a covergroup with a coverpoint that contains four bins (1, 2, 4, 8). We create two instances of this covergroup and sample them with two different values. After each call to sample, we display the coverage achieved by all instances of the covergroup (type coverage) and the coverage achieved by each instance.</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2qk4RnWoLhRiXIiobU0GG5RUXFb-vzkDOOWbZxIQJEE9LIgaPJHgdw39_4zxORd4TArXagA9F8oeCRqPX_huZZNdsISo9EBDq4p2wpwTKlm4_AvW7DLRbrY871A7AWpp7WBCClI1FiPQhtevTqBZKKcNEvvCXTHgGa_Zyk-dINdavQtYNqhb7E8Zvmw/s696/CoverageMethodsExample_output.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="60" data-original-width="696" height="55" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2qk4RnWoLhRiXIiobU0GG5RUXFb-vzkDOOWbZxIQJEE9LIgaPJHgdw39_4zxORd4TArXagA9F8oeCRqPX_huZZNdsISo9EBDq4p2wpwTKlm4_AvW7DLRbrY871A7AWpp7WBCClI1FiPQhtevTqBZKKcNEvvCXTHgGa_Zyk-dINdavQtYNqhb7E8Zvmw/w640-h55/CoverageMethodsExample_output.png" width="640" /></a></div><br /><p>The output from this example is shown above. After sampling the first covergroup, the coverage achieved for that, and all, instances is 25% since one of four bins was hit. After sampling the second covergroup, the coverage achieved for that covergroup instance is also 25%. Because two different bins are hit between the two covergroup instances, two of four bins are hit (50%) for type coverage.</p><p><br /></p><p><b>Runtime Coverage Reports</b></p><p>Another way to look at collected coverage is via a coverage report. PyVSC provides two methods that are nearly identical for obtaining a textual coverage report:</p><p></p><ul style="text-align: left;"><li>get_coverage_report – Returns the report as a string</li><li>report_coverage – Writes the report to a string (stdout by default)</li></ul><p></p><p>Both of these methods accept a keyword parameter named ‘details’ which controls whether bin hits are reported or just the top-level coverage achieved. Let’s look at a derivative of the first example to better understand the textual coverage report options.</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRCPcddPWGxNhpIXiH6rLaOqn_mExL05U8D8FGs2S_j4b6sNzqrzZNGd7oc7DeAAznwyc625pf5FePHsBmfrNXm2iHnbIeCxwU_ivnGg6XRTLA_dtbUyVPRhPK3k1kVmvedG07N5xB_1Won8A0oDH3ECbnODzM4NrETSJ5Mmt2yl73YCOVpxgLQR-mmg/s695/CoverageReportExample.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="396" data-original-width="695" height="364" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRCPcddPWGxNhpIXiH6rLaOqn_mExL05U8D8FGs2S_j4b6sNzqrzZNGd7oc7DeAAznwyc625pf5FePHsBmfrNXm2iHnbIeCxwU_ivnGg6XRTLA_dtbUyVPRhPK3k1kVmvedG07N5xB_1Won8A0oDH3ECbnODzM4NrETSJ5Mmt2yl73YCOVpxgLQR-mmg/w640-h364/CoverageReportExample.png" width="640" /></a></div><br /><p>This example is nearly identical to the first one, but with calls to ‘report_coverage’ instead of calls to the covergroup get_coverage methods.</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdCGkZXcD3sQrlwTpP0rZV5xdOBlI95xSCqcq0mR9pAhpIHb32K82BLuZvwxq7xaFCo7xryzLMcwcZhZSOpHkIYgWdqjisDqq-E7089ddMUZ1vS85LNACJh7JFmz9fXMME4bXGS9gXXWc3CxuJy6VQHCPkI6QFxHJH6DEk2-QbrvJszihsWpkEgtqgqA/s694/CoverageReportExample_output.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="509" data-original-width="694" height="470" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdCGkZXcD3sQrlwTpP0rZV5xdOBlI95xSCqcq0mR9pAhpIHb32K82BLuZvwxq7xaFCo7xryzLMcwcZhZSOpHkIYgWdqjisDqq-E7089ddMUZ1vS85LNACJh7JFmz9fXMME4bXGS9gXXWc3CxuJy6VQHCPkI6QFxHJH6DEk2-QbrvJszihsWpkEgtqgqA/w640-h470/CoverageReportExample_output.png" width="640" /></a></div><br /><p>The output from running this example is shown above. When reporting ‘details’ is enabled, the content of each coverage bin is reported. When reporting ‘details’ is disabled, only the top-level coverage achieved is reported. Displaying a coverage report with details is often helpful for confirming the correctness of a coverage model during development.</p><p><br /></p><p><b>Saving Coverage Data</b></p><p>The <a href="https://github.com/fvutils/pyucis">PyUCIS library</a> implements a Python interface to coverage data via the <a href="https://www.accellera.org/downloads/standards/ucis">Accellera UCIS</a> data model. It implements an object-oriented interface to coverage data, in addition to the Python equivalent of the UCIS C API. PyVSC uses the PyUCIS library to save coverage data, and can do so in a couple of interesting ways. Coverage data is written via the vsc.write_coverage_db method.</p><p>PyVSC can save coverage data to the XML interchange format defined by the UCIS standard. This is the default operation model for write_coverage_db. The example below shows saving it to a file named&nbsp; ‘cov.xml’.&nbsp;</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiv9Ox2BndFzQFeA46VCxqyroz3t-tNew5qvX0tu6xOiqxs83CdOn2jTq6mB_XxE3wsLjewGA1A4TdhpMRBk9SK1FE2uJl5UX-2zKQ9KElZunwOqpCiIKtJR5WgUGBjdn9kmzVap71ITuLTxE1zNrRRAKtR9KETlHXt0lzhEgQ1TDjdK34802Hu0_y_5Q/s698/CoverageSave_xml.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="332" data-original-width="698" height="304" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiv9Ox2BndFzQFeA46VCxqyroz3t-tNew5qvX0tu6xOiqxs83CdOn2jTq6mB_XxE3wsLjewGA1A4TdhpMRBk9SK1FE2uJl5UX-2zKQ9KElZunwOqpCiIKtJR5WgUGBjdn9kmzVap71ITuLTxE1zNrRRAKtR9KETlHXt0lzhEgQ1TDjdK34802Hu0_y_5Q/w640-h304/CoverageSave_xml.png" width="640" /></a></div><br /><p>PyVSC can also save coverage data to a custom database format, provided the tool that implements that database implements the UCIS C API. The example below saves coverage data to a custom database using the UCIS C API implemented in the shared library named ‘libucis.so’.</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1XaHSaOJZFYZ7xXLeQykKEXpAIkqXy69KBbpgxUAbjfjq7LL3hKaYm8UfihtSpZdmWiDVx9MtxhSOBQ28JnD5oQSaMtwXHtsW2DYgK30APCv78PAAW6WW_FtsIDUeiay1Tgd-sCUfRfSHCY4FDVQ7VfBWdL3OkYceMQZDHUgrhNj00A70KeKlaYW_WQ/s699/CoverageSave_ucis.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="330" data-original-width="699" height="302" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1XaHSaOJZFYZ7xXLeQykKEXpAIkqXy69KBbpgxUAbjfjq7LL3hKaYm8UfihtSpZdmWiDVx9MtxhSOBQ28JnD5oQSaMtwXHtsW2DYgK30APCv78PAAW6WW_FtsIDUeiay1Tgd-sCUfRfSHCY4FDVQ7VfBWdL3OkYceMQZDHUgrhNj00A70KeKlaYW_WQ/w640-h302/CoverageSave_ucis.png" width="640" /></a></div><br /><p>Both of these paths to saving coverage may provide ways to bring coverage data collected by PyVSC into coverage-analysis flows implemented by commercial EDA tools. Check your tool’s documentation and/or check with your application engineer to understand which options may be available. Feel free to report what works for you on the <a href="https://github.com/fvutils/pyvsc/discussions">PyVSC discussion forum</a> so that others can benefit as well.</p><p><b>Viewing Coverage Data</b></p><p>Obviously, you can use commercial EDA tools to view coverage data from PyVSC if your tool provides a path to bring UCIS XML in, or if it implements the UCIS C API. <a href="https://github.com/fvutils/pyucis-viewer">PyUCIS Viewer</a> provides a very simple open-source graphical application for viewing coverage in UCIS XML format.&nbsp;</p><p>To use PyUCIS Viewer, save coverage data in UCIS XML interchange format, then run PyUICIS Viewer on that XML file:</p><p><span style="font-family: courier;">% pyucis-viewer cov.xml</span></p><p>A simple tree-based graphical viewer will open to show type and instance coverage.&nbsp;</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyl0Aq9crAVbgEDjUxRerI8F_fGYYudJiKtinT634Xb9HxvehdeaPSIhEPcm2AJhPakC36tHA5EmWFZnkCVKSh7PX0geeo34oaTlByWx1o13cfj8nj29Dj17Omvz-BmVWxQrpM-h26voDzzMpfec7idota5KvUW96RgrVbFMoLqvVVyf--L5g-mH7lNA/s893/RISCV-DV_Coverage.PNG" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="721" data-original-width="893" height="516" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyl0Aq9crAVbgEDjUxRerI8F_fGYYudJiKtinT634Xb9HxvehdeaPSIhEPcm2AJhPakC36tHA5EmWFZnkCVKSh7PX0geeo34oaTlByWx1o13cfj8nj29Dj17Omvz-BmVWxQrpM-h26voDzzMpfec7idota5KvUW96RgrVbFMoLqvVVyf--L5g-mH7lNA/w640-h516/RISCV-DV_Coverage.PNG" width="640" /></a></div><br /><p><b>Conclusion</b></p><p>There are several options for viewing and manipulating coverage once it has been collected via a covergroup modeled with PyVSC. In a future post, we’ll look at some additional manipulation and reporting options being implemented within <a href="https://github.com/fvutils/pyucis">PyUCIS</a>.&nbsp;</p><p>Until then, check out the latest additions to the <a href="https://fvutils.github.io/pyvsc/">PyVSC documentation</a> and raise questions and issues on the<a href="https://github.com/fvutils/pyvsc"> PyVSC GitHub</a> page.</p><p><br /></p><div style="text-align: center;">Copyright 2022 Matthew Ballance</div><div><p style="font-variant-east-asian: normal; font-variant-numeric: normal; line-height: 16px; margin-bottom: 0in;"><span style="color: #666666;"><span face="Trebuchet MS, Trebuchet, Verdana, sans-serif"><span style="font-size: 9pt;"><i><span style="background: rgb(255, 255, 255);">The views and opinions expressed above are solely those of the author and do not represent those of my employer or any other party.</span></i></span></span></span></p></div><div><br /></div>]]></content><author><name>Matthew Ballance</name></author><category term="functional coverage" /><category term="Python" /><category term="Functional Verification" /><summary type="html"><![CDATA[&nbsp;I’ve been investing some time in documentation updates this weekend, after a couple of PyVSC users pointed out some under-described aspects of the PyVSC coverage flow. Given that these areas were under-documented in the past, it seemed a good opportunity to highlight what can be done with functional coverage data once it is sampled by a PyVSC covergroup.So, we’ve described some functional coverage goals using a PyVSC covergroup and coverpoints, created a covergroup instance, and sampled some coverage data – perhaps it was randomly-generated stimulus or data sampled from a monitor. What now?Runtime Coverage APIOne simple thing we can do is to query coverage achieved using the coverage APIs implemented by PyVSC covergroup classes. The `get_coverage` method returns the coverage achieved by all instances of a covergroup type. The `get_inst_coverage` method returns the coverage achieved by the specified covergroup instance.Let’s look at an example:In the example above, we define a covergroup with a coverpoint that contains four bins (1, 2, 4, 8). We create two instances of this covergroup and sample them with two different values. After each call to sample, we display the coverage achieved by all instances of the covergroup (type coverage) and the coverage achieved by each instance.The output from this example is shown above. After sampling the first covergroup, the coverage achieved for that, and all, instances is 25% since one of four bins was hit. After sampling the second covergroup, the coverage achieved for that covergroup instance is also 25%. Because two different bins are hit between the two covergroup instances, two of four bins are hit (50%) for type coverage.Runtime Coverage ReportsAnother way to look at collected coverage is via a coverage report. PyVSC provides two methods that are nearly identical for obtaining a textual coverage report:get_coverage_report – Returns the report as a stringreport_coverage – Writes the report to a string (stdout by default)Both of these methods accept a keyword parameter named ‘details’ which controls whether bin hits are reported or just the top-level coverage achieved. Let’s look at a derivative of the first example to better understand the textual coverage report options.This example is nearly identical to the first one, but with calls to ‘report_coverage’ instead of calls to the covergroup get_coverage methods.The output from running this example is shown above. When reporting ‘details’ is enabled, the content of each coverage bin is reported. When reporting ‘details’ is disabled, only the top-level coverage achieved is reported. Displaying a coverage report with details is often helpful for confirming the correctness of a coverage model during development.Saving Coverage DataThe PyUCIS library implements a Python interface to coverage data via the Accellera UCIS data model. It implements an object-oriented interface to coverage data, in addition to the Python equivalent of the UCIS C API. PyVSC uses the PyUCIS library to save coverage data, and can do so in a couple of interesting ways. Coverage data is written via the vsc.write_coverage_db method.PyVSC can save coverage data to the XML interchange format defined by the UCIS standard. This is the default operation model for write_coverage_db. The example below shows saving it to a file named&nbsp; ‘cov.xml’.&nbsp;PyVSC can also save coverage data to a custom database format, provided the tool that implements that database implements the UCIS C API. The example below saves coverage data to a custom database using the UCIS C API implemented in the shared library named ‘libucis.so’.Both of these paths to saving coverage may provide ways to bring coverage data collected by PyVSC into coverage-analysis flows implemented by commercial EDA tools. Check your tool’s documentation and/or check with your application engineer to understand which options may be available. Feel free to report what works for you on the PyVSC discussion forum so that others can benefit as well.Viewing Coverage DataObviously, you can use commercial EDA tools to view coverage data from PyVSC if your tool provides a path to bring UCIS XML in, or if it implements the UCIS C API. PyUCIS Viewer provides a very simple open-source graphical application for viewing coverage in UCIS XML format.&nbsp;To use PyUCIS Viewer, save coverage data in UCIS XML interchange format, then run PyUICIS Viewer on that XML file:% pyucis-viewer cov.xmlA simple tree-based graphical viewer will open to show type and instance coverage.&nbsp;ConclusionThere are several options for viewing and manipulating coverage once it has been collected via a covergroup modeled with PyVSC. In a future post, we’ll look at some additional manipulation and reporting options being implemented within PyUCIS.&nbsp;Until then, check out the latest additions to the PyVSC documentation and raise questions and issues on the PyVSC GitHub page.Copyright 2022 Matthew BallanceThe views and opinions expressed above are solely those of the author and do not represent those of my employer or any other party.]]></summary></entry><entry><title type="html">TbLink-RPC: Simplifying the Multi-Language Testbench</title><link href="https://bitsbytesgates.com/2022/03/27/tblink-rpc-simplifying-multi-language.html" rel="alternate" type="text/html" title="TbLink-RPC: Simplifying the Multi-Language Testbench" /><published>2022-03-27T20:40:00+00:00</published><updated>2022-03-27T20:40:00+00:00</updated><id>https://bitsbytesgates.com/2022/03/27/tblink-rpc-simplifying-multi-language</id><content type="html" xml:base="https://bitsbytesgates.com/2022/03/27/tblink-rpc-simplifying-multi-language.html"><![CDATA[<div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRHKdkXm_YqfEF4nahvthmhcSyVkvuROeDoDcR-M25X1Gi4KFoG15drTEl7t3H1HOyImflxhYPzMa1pYxQ0wNDNpekQqWBkJA3JjecSNMlZkG8wTEczhFu-1mfHq0PQggd8LWdgvuRiPGEqjRhulETC-0LC-JRH3L7p0ZUDQBepfWBgn6wtw5cznPjhg/s540/MultiLanguageTestbench_splash.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="300" data-original-width="540" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRHKdkXm_YqfEF4nahvthmhcSyVkvuROeDoDcR-M25X1Gi4KFoG15drTEl7t3H1HOyImflxhYPzMa1pYxQ0wNDNpekQqWBkJA3JjecSNMlZkG8wTEczhFu-1mfHq0PQggd8LWdgvuRiPGEqjRhulETC-0LC-JRH3L7p0ZUDQBepfWBgn6wtw5cznPjhg/s16000/MultiLanguageTestbench_splash.png" /></a></div><br /><p></p><p>SystemVerilog/UVM is, by far, the most widely-used language and methodology for block and subsystem-level verification environments today. The simplicity of that statement overlooks the fact that it’s often very common to have other bits of non-SystemVerilog code connected. Maybe it’s some C/C++ code that implements a reference algorithm used by the scoreboard. Maybe it’s an instruction-set simulation model used to implement software behavior. Maybe it’s infrastructure that allows early firmware code to drive behavior in the simulation. Either way, integrating non-SystemVerilog code is a non-trival development and maintenance task, despite the fact that SystemVerilog defines a standard API (the Direct Procedure Interface – DPI).</p><p>This is the first in a series of posts describing a project that I’ve been working with a goal of simplifying this situation. The TbLink-RPC (roughly Testbench Link Remote Procedure Call) provides infrastructure that dramatically reduces the code a testbench developer needs to create in order to integrate code with a simulation environment.&nbsp;</p><p>Two primary experience brought me to working on the TbLink-RPC project. The first was a somewhat long history of feeling like I had to reinvent the wheel every time I needed to integrate non-SystemVerilog code into a testbench environment. The second was my interest in ‘alternative’ testbench languages and my experience using Python as a verification language.</p><p>Back in the 2017/2018 timeframe, I started to get re-involved with open-source hardware design targeting, primarily, FPGAs. When you’re designing gateware (frankly, any software-like thing), it’s imperative to have a good test environment. I did a bit of exploration, trying out bespoke C++ testbench environments and a few other things, before landing on Python and cocotb as my test framework. A lot of this was motivated by open-source tool capabilities and community. I was committed to using open-source tools as much as possible for my open-source gateware, and open-source simulators (eg Icarus and Verilator) didn’t support sufficient SystemVerilog features to be able to use SV-UVM. After looking around a bit, cocotb seemed to have the largest community around it making it the obvious choice.</p><p>I found a lot to like about Python and cocotb for developing testbench environments. Python has a large collection of libraries, and the ability to easily incorporate these in a testbench boosted my productivity. I find the Python language easy to write and use – especially for smaller projects. Having a pure-Python testbench worked for me as a hobbyist. In many ways, that is because I create all my testbench environments from the ground up and don’t use commercial Verification IP (VIP/UVCs).&nbsp;</p><p>This same approach doesn’t work in most commercial environments. Testbench environments must reuse existing VIPs/UVCs (either commercial or developed in-house), and it’s common for a testbench architecture to remain mostly-unchanged across multiple design cycles. Doing a wholesale conversion to Python (or any other ‘alternative’ language) doesn’t make sense. Furthermore, bringing in small amounts of a different language has a high development and maintenance cost.</p><p>What I concluded I really wanted was a framework that would simplify the process of integrating some amount of testbench code written in any language (more precisely, any non-SystemVerilog language since the simulator already supports SystemVerilog) into a simulation environment.</p><p><b>Concept</b></p><p>I’ll get into more detail about the TbLink-RPC architecture in a future post. Fundamentally, though, the idea is to form a point-to-point connection between two environments through which pairs of objects can communicate via method calls.&nbsp;</p><p></p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnNyy_ASmMS639E9gPwbBIMGmmP5wCblEc5CTH-LsMynJ2QRwyJ-CyQvan5OdxKEb8IrAJviaEOw8FVQfExXzvwi-3TKbAWff45bEPh1J_Uut8-kz7n7JVNwKRXp_vRQMhI6oKzWBRbUVRu3ISqhRo31ikIDUG7TDtZB9QlEa8YNDa0qYR1GQEZpBozg/s932/concept_diagram.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="384" data-original-width="932" height="165" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnNyy_ASmMS639E9gPwbBIMGmmP5wCblEc5CTH-LsMynJ2QRwyJ-CyQvan5OdxKEb8IrAJviaEOw8FVQfExXzvwi-3TKbAWff45bEPh1J_Uut8-kz7n7JVNwKRXp_vRQMhI6oKzWBRbUVRu3ISqhRo31ikIDUG7TDtZB9QlEa8YNDa0qYR1GQEZpBozg/w400-h165/concept_diagram.png" width="400" /></a></div><br /><p></p><p>There are a few things that make TbLink-RPC different from other remote-procedure-call solutions. The first is that TbLink-RPC is simulation centric. Or, rather, it is designed to work with environments that maintain a local “simulated” notion of time. To that end, TbLink-RPC supports both blocking (time-consuming) and non-blocking functions, and defines a protocol to ensure that time advances at the intended time.</p><p>TbLink-RPC is designed to support both single OS process and multi OS-process integration. Single-process integration (where both environments run in the same OS process) provide higher performance.&nbsp; &nbsp; But, single-process integration isn’t always feasible. For example, one environment might be an instruction-set simulator (eg QEMU) that must run in its own process in order to manage memory in its own highly-specialized way.&nbsp;</p><p>TbLink-RPC emphasizes automation and modularity. Code-generation automation is used to create the boilerplate code, minimizing user effort to integrate new APIs. The entire system is designed such that integrations created independently can be easily combined. Even better, this is done in such a way that SystemVerilog users don’t need to deal with generated DPI integration code.</p><p><br /></p><p><b>Example</b></p><p>An example is often the simplest way to get across a concept, and I have a very simple one here. For now, I’ll just show the key elements of a typical use case: connecting a Python reference model to a UVM testbench environment.&nbsp;</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjR0Ce9JX3-2lmnyV1vKc13y4QZLFNOWk4pLts4jkI95i_MbQDBCXWqbKAf42gUX5wJWbbInFlBPJlRw-tg69wLKkAzULM-l8aDiHQlgOG36Vs-WMNQpsSy-uhSzDXNk5NcbyEafwMED33RrrAIwjPrTiaLWkjELNmHSEeFl4SRQAFoY3fb-lvB-ylevw/s408/UVM_tb_diagram.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="336" data-original-width="408" height="264" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjR0Ce9JX3-2lmnyV1vKc13y4QZLFNOWk4pLts4jkI95i_MbQDBCXWqbKAf42gUX5wJWbbInFlBPJlRw-tg69wLKkAzULM-l8aDiHQlgOG36Vs-WMNQpsSy-uhSzDXNk5NcbyEafwMED33RrrAIwjPrTiaLWkjELNmHSEeFl4SRQAFoY3fb-lvB-ylevw/s320/UVM_tb_diagram.png" width="320" /></a></div><br /><p>Now, in this case our DUT isn’t that exciting. It’s just an adder, so our reference model is correspondingly trivial.&nbsp;</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6uJXRt5gO1iGAxSMHaHA5XGbuWTVq9QI_plx5v0ErXbeQKM4cippRZZzvM1tu_tffUnafRuJ3BTm9b8ZSpG2kc8mdYA9_cKVCh5Fgi2gxPZQnGMSNOSKmferC2OpWpfnZTwLv92Uvfk3VZ6MHEw0J8Vpkfdeo0wRi2dJj0PxT5AxzvkoUgZkOutbPbg/s646/python_class.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="179" data-original-width="646" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6uJXRt5gO1iGAxSMHaHA5XGbuWTVq9QI_plx5v0ErXbeQKM4cippRZZzvM1tu_tffUnafRuJ3BTm9b8ZSpG2kc8mdYA9_cKVCh5Fgi2gxPZQnGMSNOSKmferC2OpWpfnZTwLv92Uvfk3VZ6MHEw0J8Vpkfdeo0wRi2dJj0PxT5AxzvkoUgZkOutbPbg/s16000/python_class.png" /></a></div><br /><p>Our reference-model class contains a method named ‘add’ that returns the sum of two parameters passed to it. Note that we apply a decorator (tblink_rpc.iftype) to the class. This registers the class with the TbLink-RPC infrastructure. Note, also, that we apply a decorator to the ‘add’ method and specify typing annotations for the parameters and return type. Together, these register the method with TbLink-RPC and specify the parameter types to be used.</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBMcPcMsk5zKpVnNvI3n7ucOU0Er2mEZXdvzsYTnzYbnpH1wWd158FalAlz2jPER6yxqb0GZ38xnlszVf8F7hDGasqmIdh5N4Uc-fRLdGsuvVKKYTkpMtfcPjDT3Bnrv71auuftoD_THlwWekKrETqoyQ5TgHpApOObjeUZTEvVuHFqmoGB8XwWXeS7Q/s529/sv_object_class.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="275" data-original-width="529" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBMcPcMsk5zKpVnNvI3n7ucOU0Er2mEZXdvzsYTnzYbnpH1wWd158FalAlz2jPER6yxqb0GZ38xnlszVf8F7hDGasqmIdh5N4Uc-fRLdGsuvVKKYTkpMtfcPjDT3Bnrv71auuftoD_THlwWekKrETqoyQ5TgHpApOObjeUZTEvVuHFqmoGB8XwWXeS7Q/s16000/sv_object_class.png" /></a></div><br /><p>In order to call our Python class from SystemVerilog, we will need a SV class to call. That class is shown above, and combines a class from the TbLink-RPC library (TbLinkLaunchUvmObj) with some generated implementation classes created from the API definition that in Python. If our class contained methods implemented in SystemVerilog that could be called from Python, then this class would contain the implementation. Since that’s not the case, there isn’t anything to implement here.</p><p>From a UVM testbench perspective, using our Python class involves two steps:</p><p></p><ul style="text-align: left;"><li>Launch an instance of the class in Python connected with the SV class</li><li>Call the API</li></ul><p></p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXGIxM1M5fLJWbtYtnT6xs3t-cJm7qeHzmum5xqxPlYB9Pb2V-_vLjIuTgYgSSgYGwZeDflywQKqhzK9TH7x0-bLzJZg42Dkfw1CR0hyVE7y7gkIxngndkQAW5AamvgM943snUyd8ELt-umJVGpONImT5xDQXbPl6xVTb5LOWZXS5sT0qChz596pKCJw/s599/launch_remote_obj.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="216" data-original-width="599" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXGIxM1M5fLJWbtYtnT6xs3t-cJm7qeHzmum5xqxPlYB9Pb2V-_vLjIuTgYgSSgYGwZeDflywQKqhzK9TH7x0-bLzJZg42Dkfw1CR0hyVE7y7gkIxngndkQAW5AamvgM943snUyd8ELt-umJVGpONImT5xDQXbPl6xVTb5LOWZXS5sT0qChz596pKCJw/s16000/launch_remote_obj.png" /></a></div>The first step, launching, is shown above. The TbLink-RPC library class (TbLinkLaunchUvmObj) that our class inherits from implements the details of starting up and communicating with an embedded or remote environment. We just have to specify the details of how to do this via the configuration object. In this case, and in most cases, we will use a factory method to fill in common details. Because we are starting a Python environment, we must specify the Python module (uvm_python_obj) that contains the Python class we wish to call.<div><br /></div><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhBp3tOO2_5c-WzCUf_VbKSgN1UPwUw4zVeKRrv-jlMnB3cqaOCVkPR5JEpsWBkrOU9i19VZS4Tv9KYwIFgqCjoRiY8KVyiovD90wEY69gOc9fK-xaxtjFZKrMBBu2t7nOYRrAwlq8QQCXYvEsyYJ7Z4HrRgZiBPahTZyxGadnhGOgKq5q1BIc0ZLmpzQ/s653/call_remote_obj.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="201" data-original-width="653" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhBp3tOO2_5c-WzCUf_VbKSgN1UPwUw4zVeKRrv-jlMnB3cqaOCVkPR5JEpsWBkrOU9i19VZS4Tv9KYwIFgqCjoRiY8KVyiovD90wEY69gOc9fK-xaxtjFZKrMBBu2t7nOYRrAwlq8QQCXYvEsyYJ7Z4HrRgZiBPahTZyxGadnhGOgKq5q1BIc0ZLmpzQ/s16000/call_remote_obj.png" /></a></div><br /><div>Finally, we need to call our reference model. Our scoreboard contains a method that accepts operand data from the agent driving the ALU, and a result from the agent monitoring the ALU output. We call the reference model by making a SV class-method call to obtain the expected result from the Python reference model.</div><div><br /></div><div><b>Next Steps</b></div><div><p>TbLink-RPC is designed to simplify integrating code into simulation environment. In the context of a SystemVerilog environment, this makes it much easier and simpler to bring in external models written in non-SystemVerilog. Over the course of the next few posts, I’ll go into a bit more detail on TbLink-RPC architecture and the nuts and bolts of the integration process.</p><p><br /></p><p><b>References</b></p><p></p><ul style="text-align: left;"><li>TbLink-RPC (under construction) -&nbsp;<a href="https://tblink-rpc.github.io/">https://tblink-rpc.github.io/</a></li><li>cocotb -&nbsp;<a href="https://docs.cocotb.org/en/stable/">https://docs.cocotb.org/en/stable/</a></li></ul><div><br /></div><div style="text-align: center;">Copyright 2022 Matthew Ballance</div><div style="text-align: left;"><p style="font-variant-east-asian: normal; font-variant-numeric: normal; line-height: 100%; margin-bottom: 0in;"><span style="color: #666666;"><span style="font-family: Trebuchet MS, Trebuchet, Verdana, sans-serif;"><span style="font-size: 9pt;"><i><span style="background: #ffffff;">The views and opinions expressed above are solely those of the author and do not represent those of my employer or any other party.</span></i></span></span></span></p></div><div><br /></div><p></p></div>]]></content><author><name>Matthew Ballance</name></author><summary type="html"><![CDATA[SystemVerilog/UVM is, by far, the most widely-used language and methodology for block and subsystem-level verification environments today. The simplicity of that statement overlooks the fact that it’s often very common to have other bits of non-SystemVerilog code connected. Maybe it’s some C/C++ code that implements a reference algorithm used by the scoreboard. Maybe it’s an instruction-set simulation model used to implement software behavior. Maybe it’s infrastructure that allows early firmware code to drive behavior in the simulation. Either way, integrating non-SystemVerilog code is a non-trival development and maintenance task, despite the fact that SystemVerilog defines a standard API (the Direct Procedure Interface – DPI).This is the first in a series of posts describing a project that I’ve been working with a goal of simplifying this situation. The TbLink-RPC (roughly Testbench Link Remote Procedure Call) provides infrastructure that dramatically reduces the code a testbench developer needs to create in order to integrate code with a simulation environment.&nbsp;Two primary experience brought me to working on the TbLink-RPC project. The first was a somewhat long history of feeling like I had to reinvent the wheel every time I needed to integrate non-SystemVerilog code into a testbench environment. The second was my interest in ‘alternative’ testbench languages and my experience using Python as a verification language.Back in the 2017/2018 timeframe, I started to get re-involved with open-source hardware design targeting, primarily, FPGAs. When you’re designing gateware (frankly, any software-like thing), it’s imperative to have a good test environment. I did a bit of exploration, trying out bespoke C++ testbench environments and a few other things, before landing on Python and cocotb as my test framework. A lot of this was motivated by open-source tool capabilities and community. I was committed to using open-source tools as much as possible for my open-source gateware, and open-source simulators (eg Icarus and Verilator) didn’t support sufficient SystemVerilog features to be able to use SV-UVM. After looking around a bit, cocotb seemed to have the largest community around it making it the obvious choice.I found a lot to like about Python and cocotb for developing testbench environments. Python has a large collection of libraries, and the ability to easily incorporate these in a testbench boosted my productivity. I find the Python language easy to write and use – especially for smaller projects. Having a pure-Python testbench worked for me as a hobbyist. In many ways, that is because I create all my testbench environments from the ground up and don’t use commercial Verification IP (VIP/UVCs).&nbsp;This same approach doesn’t work in most commercial environments. Testbench environments must reuse existing VIPs/UVCs (either commercial or developed in-house), and it’s common for a testbench architecture to remain mostly-unchanged across multiple design cycles. Doing a wholesale conversion to Python (or any other ‘alternative’ language) doesn’t make sense. Furthermore, bringing in small amounts of a different language has a high development and maintenance cost.What I concluded I really wanted was a framework that would simplify the process of integrating some amount of testbench code written in any language (more precisely, any non-SystemVerilog language since the simulator already supports SystemVerilog) into a simulation environment.ConceptI’ll get into more detail about the TbLink-RPC architecture in a future post. Fundamentally, though, the idea is to form a point-to-point connection between two environments through which pairs of objects can communicate via method calls.&nbsp;There are a few things that make TbLink-RPC different from other remote-procedure-call solutions. The first is that TbLink-RPC is simulation centric. Or, rather, it is designed to work with environments that maintain a local “simulated” notion of time. To that end, TbLink-RPC supports both blocking (time-consuming) and non-blocking functions, and defines a protocol to ensure that time advances at the intended time.TbLink-RPC is designed to support both single OS process and multi OS-process integration. Single-process integration (where both environments run in the same OS process) provide higher performance.&nbsp; &nbsp; But, single-process integration isn’t always feasible. For example, one environment might be an instruction-set simulator (eg QEMU) that must run in its own process in order to manage memory in its own highly-specialized way.&nbsp;TbLink-RPC emphasizes automation and modularity. Code-generation automation is used to create the boilerplate code, minimizing user effort to integrate new APIs. The entire system is designed such that integrations created independently can be easily combined. Even better, this is done in such a way that SystemVerilog users don’t need to deal with generated DPI integration code.ExampleAn example is often the simplest way to get across a concept, and I have a very simple one here. For now, I’ll just show the key elements of a typical use case: connecting a Python reference model to a UVM testbench environment.&nbsp;Now, in this case our DUT isn’t that exciting. It’s just an adder, so our reference model is correspondingly trivial.&nbsp;Our reference-model class contains a method named ‘add’ that returns the sum of two parameters passed to it. Note that we apply a decorator (tblink_rpc.iftype) to the class. This registers the class with the TbLink-RPC infrastructure. Note, also, that we apply a decorator to the ‘add’ method and specify typing annotations for the parameters and return type. Together, these register the method with TbLink-RPC and specify the parameter types to be used.In order to call our Python class from SystemVerilog, we will need a SV class to call. That class is shown above, and combines a class from the TbLink-RPC library (TbLinkLaunchUvmObj) with some generated implementation classes created from the API definition that in Python. If our class contained methods implemented in SystemVerilog that could be called from Python, then this class would contain the implementation. Since that’s not the case, there isn’t anything to implement here.From a UVM testbench perspective, using our Python class involves two steps:Launch an instance of the class in Python connected with the SV classCall the APIThe first step, launching, is shown above. The TbLink-RPC library class (TbLinkLaunchUvmObj) that our class inherits from implements the details of starting up and communicating with an embedded or remote environment. We just have to specify the details of how to do this via the configuration object. In this case, and in most cases, we will use a factory method to fill in common details. Because we are starting a Python environment, we must specify the Python module (uvm_python_obj) that contains the Python class we wish to call.Finally, we need to call our reference model. Our scoreboard contains a method that accepts operand data from the agent driving the ALU, and a result from the agent monitoring the ALU output. We call the reference model by making a SV class-method call to obtain the expected result from the Python reference model.Next StepsTbLink-RPC is designed to simplify integrating code into simulation environment. In the context of a SystemVerilog environment, this makes it much easier and simpler to bring in external models written in non-SystemVerilog. Over the course of the next few posts, I’ll go into a bit more detail on TbLink-RPC architecture and the nuts and bolts of the integration process.ReferencesTbLink-RPC (under construction) -&nbsp;https://tblink-rpc.github.io/cocotb -&nbsp;https://docs.cocotb.org/en/stable/Copyright 2022 Matthew BallanceThe views and opinions expressed above are solely those of the author and do not represent those of my employer or any other party.]]></summary></entry><entry><title type="html">Documenting SystemVerilog with Sphinx</title><link href="https://bitsbytesgates.com/2022/01/24/documenting-systemverilog-with-sphinx.html" rel="alternate" type="text/html" title="Documenting SystemVerilog with Sphinx" /><published>2022-01-24T02:57:00+00:00</published><updated>2022-01-24T02:57:00+00:00</updated><id>https://bitsbytesgates.com/2022/01/24/documenting-systemverilog-with-sphinx</id><content type="html" xml:base="https://bitsbytesgates.com/2022/01/24/documenting-systemverilog-with-sphinx.html"><![CDATA[<p></p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEiNWOsyFL5mQ67LxKR4FU9onpnBqZi9VdXXlN6f3v7jdhDpgpoT6JDNZI0xew3Vs7nw6Si5qG-aiOz9IymD_1j7kuh5n8wfSUthnYDrqTyq4gKhhYCFs62nlqZ0X0ll_DW2kMUjQXw3wnLXYAEWS__DyPdR1Xsq4jpUU9e_TU4u_q-fuMtJ6rnFdH4XCQ=s574" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="300" data-original-width="574" src="https://blogger.googleusercontent.com/img/a/AVvXsEiNWOsyFL5mQ67LxKR4FU9onpnBqZi9VdXXlN6f3v7jdhDpgpoT6JDNZI0xew3Vs7nw6Si5qG-aiOz9IymD_1j7kuh5n8wfSUthnYDrqTyq4gKhhYCFs62nlqZ0X0ll_DW2kMUjQXw3wnLXYAEWS__DyPdR1Xsq4jpUU9e_TU4u_q-fuMtJ6rnFdH4XCQ=s16000" /></a></div><br /><div class="separator" style="clear: both; text-align: center;"><br /></div>I've been digging into a project over the last few months whose value proposition is to simplify the process of connecting simulation-like environments and things like reference models, testbench languages, etc. I'll write more (likely much more) about this project in the future. This post, however, is about documentation and, specifically, documentation for SystemVerilog code.<p></p><p>As you can imagine, SystemVerilog support is critical for a project that connects into simulation environments. SystemVerilog and UVM currently are, after all, the most widely-deployed solution for block- and subsystem-level verification. But, support for other languages (C, C++, Python, etc) are important as well. Consequently, when it comes to documenting APIs, I need to cover a fair amount of ground.</p><p><br /></p><p><b>Sphinx for Documenting Code</b></p><p>I've been using <a href="https://www.sphinx-doc.org/en/master/">Sphinx Documentation Generator</a> for the last couple of years. Sphinx accepts input formatted with <a href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/index.html">reStructuredText</a>&nbsp; mark-up and generates formatted output in HTML, PDF, and several other formats. Sphinx has a wide variety of plug-ins that help to make formatting different types of document content more productive. Sphinx was originally created for documenting Python code, and it shows. That said, its features support documenting far more than just Python code -- as we'll shortly see.</p><p><br /></p><p><b>Leveraging Code Comments</b></p><p>Tools that generate documentation from code comments have been around for a long time. My experience has been that they're a good way to quickly create somewhat-generic documentation, provided the doc-generation tool supports the right coding language and the codebase contains enough comments. My experience has also been that documentation created from API code comments is lacking key insights from the author on the code's architecture.&nbsp;</p><p>One thing that I've liked about Sphinx is that it supports referencing code-comment documentation, but relies on the document author to do so. This approach encourages the documentation author to incorporate API documentation alongside explanatory text (that could be awkward to include in a code comment) instead of having large fully auto-generated API-reference sections. This approach also gives the documentation author much more control over the document structure than fully-automated API-documentation tools can typically afford to provide.</p><p><br /></p><p><b>What About Non-Python Code?</b></p><p>As mentioned previously, Sphinx was originally a tool for documenting Python packages. As such, it provides built-in features for extracting documentation comments from Python code. The <a href="https://www.doxygen.nl/index.html">Doxygen </a>tool is one very popular way of documenting languages such as C and C++ from code comments. The <a href="https://breathe.readthedocs.io/en/latest/">Breathe</a> plug-in for Sphinx processes the XML output files from Doxygen, allowing Sphnix documents to bring in documentation code-comments from any language supported by Doxygen.</p><p>The good news is that Doxygen supports a wide range of languages. Using Doxygen to pull in doc comments from the C++ codebase of my project will work with relative ease. Unfortunately, though, Doxygen doesn't support SystemVerilog.</p><p>&nbsp;</p><p><b>Doxygen and Filters</b></p><p>Believe it or not, a commonly-recommended path to support a new language with Doxygen is to write a filter script that transforms the source language (that Doxygen doesn't support) into the form of a language that Doxygen does.&nbsp;</p><p>As luck would have it, there is a SystemVerilog filter for Doxygen: <a href="https://github.com/SeanOBoyle/DoxygenFilterSystemVerilog">DoxygenFilterSystemVerilog</a>. It's written in PERL and uses regular expressions to recognize SystemVerilog constructs and translate them into C++ equivalents that Doxygen can understand. Doxygen primarily cares about constructs like classes, class fields, and function declarations. This makes an approximate-translation strategy workable.</p><p><b>Dealing with Packages and Namespaces</b></p><p>There's just one small issue that we need to solve in order to have a complete flow. SystemVerilog class-based code relies heavily on the pre-processor to assemble a set of classes under the appropriate package. In C++, a namespace is just a 'tag' that gets applied to content. Any number of files may declare content inside the same namespace with no issues. SystemVerilog, conversely, requires that a given package be declared only once and that all content in that package be declared inside that single package scope. The end result is that users sensibly place different classes in different files and 'glue' the whole thing together using include directives and the pre-processor.</p><p>If we are only documenting code from a single package, this likely wouldn't cause a problem. We could simply run the Doxygen filter on each class file. When documenting a codebase with multiple packages, we need to ensure we stay consistent with the packages in which classes are declared. How do we do this? By pre-processing the code, of course!</p><p><br /></p><p><b>Putting it all Together</b></p><p>There really are two parts to our flow. The first is processing the SystemVerilog code to make it 'look' like C++.</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEjTMqFgnHsQjvDkklhPK7IsqsrdUTThcv4KmlMC3J1qPipseAoPTtFyP51x0MyoCBgZ7h5H22cZZQD5MbI6mCUfdjZdoXi1F41v-OLmXMrCR3cFQ5GSg4oDl9sPAS92JNqZnnwKvzLKI7lAFZRDCh3SC14y_uVZ6BGY_K5X2drvkJEXvMXfFQo6MgLjJQ=s912" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="250" data-original-width="912" height="175" src="https://blogger.googleusercontent.com/img/a/AVvXsEjTMqFgnHsQjvDkklhPK7IsqsrdUTThcv4KmlMC3J1qPipseAoPTtFyP51x0MyoCBgZ7h5H22cZZQD5MbI6mCUfdjZdoXi1F41v-OLmXMrCR3cFQ5GSg4oDl9sPAS92JNqZnnwKvzLKI7lAFZRDCh3SC14y_uVZ6BGY_K5X2drvkJEXvMXfFQo6MgLjJQ=w640-h175" width="640" /></a></div><br /><p>That part of the flow is shown above. In this portion of the flow I'm using <a href="https://github.com/verilator/verilator">Verilator</a> for the pre-processor <br />because of a very unique feature. Most pre-processors that I'm aware of strip out comments as part of preprocessing. Verilator allows the user to (optionally) retain comments. Since the vast majority of the 'interesting' content is found in documentation comments, this is a critical feature for this flow.</p><p><br /></p><p></p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEirdJh-jO91obP_OQyW5CrnT1y2GyZP4w6luiuBMp_o91e0TYK2byPTAI0VBTzCt6UAfz69wMpSjLEk6Vtjd5322mq5FnogyKKeMQFHtsUMl6_9qRWgEDcagdTaRgBQmC7NmLetOd4L0UNMPita_wW-9SYDQLkbFjUP8Lcv51xoyJ4GzldrvD4wE2terA=s865" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="384" data-original-width="865" height="285" src="https://blogger.googleusercontent.com/img/a/AVvXsEirdJh-jO91obP_OQyW5CrnT1y2GyZP4w6luiuBMp_o91e0TYK2byPTAI0VBTzCt6UAfz69wMpSjLEk6Vtjd5322mq5FnogyKKeMQFHtsUMl6_9qRWgEDcagdTaRgBQmC7NmLetOd4L0UNMPita_wW-9SYDQLkbFjUP8Lcv51xoyJ4GzldrvD4wE2terA=w640-h285" width="640" /></a></div><p></p><p>Once we have our "c++-ified" SystemVerilog, we can run that through Doxygen. This will result in some XML files containing the information and relationships Doxygen extracted from the XML code. These XML files are what the Breathe plug-in reads, and exposes to Sphinx.</p><p>As I mentioned earlier, Sphinx brings in documentation code-comment content on demand instead of automatically assembling it into a document or document section.&nbsp;</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhZWM3-FBZzqoe7ks4iq2PQ-CcFsac1KN46H0GAup0GhvjvPpQPFGbd6uipsRNj_LBSfjinXF0ltdd5PYBTUkKWgnHN6dAA16zHFTtBz82eXE-ToHDXm5-7uAwFCb6bH2m1pU4HJkIdJpc-9C84Lfm6ngL0AhP-K6USUT9_4yBC5GxPCqHumu8N6YQ3eg=s320" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="79" data-original-width="320" src="https://blogger.googleusercontent.com/img/a/AVvXsEhZWM3-FBZzqoe7ks4iq2PQ-CcFsac1KN46H0GAup0GhvjvPpQPFGbd6uipsRNj_LBSfjinXF0ltdd5PYBTUkKWgnHN6dAA16zHFTtBz82eXE-ToHDXm5-7uAwFCb6bH2m1pU4HJkIdJpc-9C84Lfm6ngL0AhP-K6USUT9_4yBC5GxPCqHumu8N6YQ3eg=s16000" /></a></div><div><br /></div>The snippet above shows bringing in the code-comment documentation for all fields and methods within the specified class (in this case tblink_rpc::IEndpoint). The result in the document looks something like what is shown below:<div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEg_qUdw_b1UWVJd5O5nhIU2NzQhSkJILiiYcksQVPmKPHtYaF-LC6Ghelx60Gi1dpXIwwPpTK2V-vuWHFLKp0uNEmSa55ZsqrdlbfBrJ4pBxI3pCrtvRuo2ZKJzOP9e1bYu0dSEe0S97v2rmcgU0JZDAVLEhS8cCe0-tefGoNnhEF81uHhRDnNr_O8O6g=s712" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="391" data-original-width="712" src="https://blogger.googleusercontent.com/img/a/AVvXsEg_qUdw_b1UWVJd5O5nhIU2NzQhSkJILiiYcksQVPmKPHtYaF-LC6Ghelx60Gi1dpXIwwPpTK2V-vuWHFLKp0uNEmSa55ZsqrdlbfBrJ4pBxI3pCrtvRuo2ZKJzOP9e1bYu0dSEe0S97v2rmcgU0JZDAVLEhS8cCe0-tefGoNnhEF81uHhRDnNr_O8O6g=s16000" /></a></div><br /><div><div><br /></div><div>There are other Sphinx doc tags that allow referencing a single function, etc.<div><br /><p><br /></p><p><b>Looking Forward</b></p><p>In my current work, I'm focused on SystemVerilog classes. The approach above works well for documenting class structures, but doesn't work so well for document SystemVerilog modules and interfaces. My current thinking is to use the <a href="https://github.com/SymbiFlow/sphinx-verilog-domain">Sphinx Verilog Domain</a> plug-in to document modules, and continue to use the flow above for classes. Would a single tool be better? Yes. But having two complementary tools is just fine.</p><p>Now that I have a way to document SystemVerilog class code, I'm digging into that process. If you're curious about the commands/scripts I'm using, you can have a look at the <a href="https://github.com/tblink-rpc/tblink-rpc-docs/">tblink-rpc-docs</a> project and the <a href="https://github.com/tblink-rpc/tblink-rpc-docs/blob/main/Makefile">Makefile</a> in that project.</p><p>Hope you find this helpful, and feel free to comment back on your approach to documenting SystemVerilog classes.</p><p><br /></p></div></div></div>]]></content><author><name>Matthew Ballance</name></author><category term="Documentation" /><category term="SystemVerilog" /><category term="Sphinx" /><summary type="html"><![CDATA[I've been digging into a project over the last few months whose value proposition is to simplify the process of connecting simulation-like environments and things like reference models, testbench languages, etc. I'll write more (likely much more) about this project in the future. This post, however, is about documentation and, specifically, documentation for SystemVerilog code.As you can imagine, SystemVerilog support is critical for a project that connects into simulation environments. SystemVerilog and UVM currently are, after all, the most widely-deployed solution for block- and subsystem-level verification. But, support for other languages (C, C++, Python, etc) are important as well. Consequently, when it comes to documenting APIs, I need to cover a fair amount of ground.Sphinx for Documenting CodeI've been using Sphinx Documentation Generator for the last couple of years. Sphinx accepts input formatted with reStructuredText&nbsp; mark-up and generates formatted output in HTML, PDF, and several other formats. Sphinx has a wide variety of plug-ins that help to make formatting different types of document content more productive. Sphinx was originally created for documenting Python code, and it shows. That said, its features support documenting far more than just Python code -- as we'll shortly see.Leveraging Code CommentsTools that generate documentation from code comments have been around for a long time. My experience has been that they're a good way to quickly create somewhat-generic documentation, provided the doc-generation tool supports the right coding language and the codebase contains enough comments. My experience has also been that documentation created from API code comments is lacking key insights from the author on the code's architecture.&nbsp;One thing that I've liked about Sphinx is that it supports referencing code-comment documentation, but relies on the document author to do so. This approach encourages the documentation author to incorporate API documentation alongside explanatory text (that could be awkward to include in a code comment) instead of having large fully auto-generated API-reference sections. This approach also gives the documentation author much more control over the document structure than fully-automated API-documentation tools can typically afford to provide.What About Non-Python Code?As mentioned previously, Sphinx was originally a tool for documenting Python packages. As such, it provides built-in features for extracting documentation comments from Python code. The Doxygen tool is one very popular way of documenting languages such as C and C++ from code comments. The Breathe plug-in for Sphinx processes the XML output files from Doxygen, allowing Sphnix documents to bring in documentation code-comments from any language supported by Doxygen.The good news is that Doxygen supports a wide range of languages. Using Doxygen to pull in doc comments from the C++ codebase of my project will work with relative ease. Unfortunately, though, Doxygen doesn't support SystemVerilog.&nbsp;Doxygen and FiltersBelieve it or not, a commonly-recommended path to support a new language with Doxygen is to write a filter script that transforms the source language (that Doxygen doesn't support) into the form of a language that Doxygen does.&nbsp;As luck would have it, there is a SystemVerilog filter for Doxygen: DoxygenFilterSystemVerilog. It's written in PERL and uses regular expressions to recognize SystemVerilog constructs and translate them into C++ equivalents that Doxygen can understand. Doxygen primarily cares about constructs like classes, class fields, and function declarations. This makes an approximate-translation strategy workable.Dealing with Packages and NamespacesThere's just one small issue that we need to solve in order to have a complete flow. SystemVerilog class-based code relies heavily on the pre-processor to assemble a set of classes under the appropriate package. In C++, a namespace is just a 'tag' that gets applied to content. Any number of files may declare content inside the same namespace with no issues. SystemVerilog, conversely, requires that a given package be declared only once and that all content in that package be declared inside that single package scope. The end result is that users sensibly place different classes in different files and 'glue' the whole thing together using include directives and the pre-processor.If we are only documenting code from a single package, this likely wouldn't cause a problem. We could simply run the Doxygen filter on each class file. When documenting a codebase with multiple packages, we need to ensure we stay consistent with the packages in which classes are declared. How do we do this? By pre-processing the code, of course!Putting it all TogetherThere really are two parts to our flow. The first is processing the SystemVerilog code to make it 'look' like C++.That part of the flow is shown above. In this portion of the flow I'm using Verilator for the pre-processor because of a very unique feature. Most pre-processors that I'm aware of strip out comments as part of preprocessing. Verilator allows the user to (optionally) retain comments. Since the vast majority of the 'interesting' content is found in documentation comments, this is a critical feature for this flow.Once we have our "c++-ified" SystemVerilog, we can run that through Doxygen. This will result in some XML files containing the information and relationships Doxygen extracted from the XML code. These XML files are what the Breathe plug-in reads, and exposes to Sphinx.As I mentioned earlier, Sphinx brings in documentation code-comment content on demand instead of automatically assembling it into a document or document section.&nbsp;The snippet above shows bringing in the code-comment documentation for all fields and methods within the specified class (in this case tblink_rpc::IEndpoint). The result in the document looks something like what is shown below:There are other Sphinx doc tags that allow referencing a single function, etc.Looking ForwardIn my current work, I'm focused on SystemVerilog classes. The approach above works well for documenting class structures, but doesn't work so well for document SystemVerilog modules and interfaces. My current thinking is to use the Sphinx Verilog Domain plug-in to document modules, and continue to use the flow above for classes. Would a single tool be better? Yes. But having two complementary tools is just fine.Now that I have a way to document SystemVerilog class code, I'm digging into that process. If you're curious about the commands/scripts I'm using, you can have a look at the tblink-rpc-docs project and the Makefile in that project.Hope you find this helpful, and feel free to comment back on your approach to documenting SystemVerilog classes.]]></summary></entry><entry><title type="html">SoC Integration Testing: Hw/Sw Coordination (Part 2)</title><link href="https://bitsbytesgates.com/2021/04/18/soc-integration-testing-hwsw.html" rel="alternate" type="text/html" title="SoC Integration Testing: Hw/Sw Coordination (Part 2)" /><published>2021-04-18T16:01:00+00:00</published><updated>2021-04-18T16:01:00+00:00</updated><id>https://bitsbytesgates.com/2021/04/18/soc-integration-testing-hwsw</id><content type="html" xml:base="https://bitsbytesgates.com/2021/04/18/soc-integration-testing-hwsw.html"><![CDATA[<p style="text-align: center;"></p><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-ZBoJb1TpY0c/YHs7BEuD-CI/AAAAAAAADpQ/-6mLIuhpgRA-EWwhuCzgZf8AZAwIXyQagCLcBGAsYHQ/s540/splash.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="300" data-original-width="540" src="https://1.bp.blogspot.com/-ZBoJb1TpY0c/YHs7BEuD-CI/AAAAAAAADpQ/-6mLIuhpgRA-EWwhuCzgZf8AZAwIXyQagCLcBGAsYHQ/s16000/splash.png" /></a></div><br /><p></p><p></p><div>Controlling the outside world -- specifically interface BFMs -- from embedded software is critical to SoC integration tests that exercise interface IP. <a href="https://bitsbytesgates.blogspot.com/2021/03/soc-integration-testing-hwsw-test.html">In the last post</a>, we showed how to pass data from embedded software to Python by tracing execution of the processor core and reading the mirrored values of registers and memory to obtain parameter values. While functional, doing things in this way is highly specific to one message-passing approach and is pretty labor intensive. In this post, we'll add some abstraction and automation to improve usability and scalability.</div><div><br /></div><div><br /></div><div><b>Design Goals</b></div><div><b><br /></b></div><div><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-dUWSpyHCvZM/YHtJyOtv5LI/AAAAAAAADqA/TjNZGDS5mlEztLbKRmkwP9oJynFYFSjcwCLcBGAsYHQ/s384/HvlRpc_Diagram.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="363" data-original-width="384" src="https://1.bp.blogspot.com/-dUWSpyHCvZM/YHtJyOtv5LI/AAAAAAAADqA/TjNZGDS5mlEztLbKRmkwP9oJynFYFSjcwCLcBGAsYHQ/s16000/HvlRpc_Diagram.png" /></a></div><br /><b><br /></b></div><div>While we're initially focused on providing a nice automated way to communicate between embedded software and the test harness in a simulation environment, the design goals go beyond this. The diagram above shows the basic architecture. <i>Endpoints&nbsp;</i>provide a portal for one environment to call APIs in another environment. Each <i>endpoint</i>&nbsp;supports a known set of APIs, and different endpoints will support different sets of APIs.&nbsp;</div><div><br /></div><div>Each environment interacts with APIs on an endpoint without needing to know how communication is implemented. For example, execution trace might be used to implement processor to Python communication in a simulation-based environment. When the design is synthesized to FPGA, communication might be implemented via an external interface. With appropriate abstraction, neither the test software running on the processor nor the Python test code should need to change despite the fact that data is being moved in very different ways.&nbsp;</div><div><br /></div><div>In order for this to be feasible, we'll need to collect some meta-data about the APIs.</div><div><br /></div><div><br /></div><div><b>Example</b></div><div><b><br /></b></div><div>I always find an example to be helpful, so let's look at the enhancements to the flow in the context of a simple example.</div><div>&nbsp;</div><div><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-LNqDqp1S2pU/YHtC6-hfN6I/AAAAAAAADpk/L_J_vgTSKC8JzKu_f3xusMbk0BKkHRcQACLcBGAsYHQ/s276/Example_Diagram.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="276" data-original-width="264" src="https://1.bp.blogspot.com/-LNqDqp1S2pU/YHtC6-hfN6I/AAAAAAAADpk/L_J_vgTSKC8JzKu_f3xusMbk0BKkHRcQACLcBGAsYHQ/s0/Example_Diagram.png" /></a></div><br /><div class="separator" style="clear: both; text-align: center;"><br /></div>The diagram above shows the key elements of a very small SoC called <a href="https://github.com/mballance/tiny-soc">Tiny SoC</a>. We can test many aspects of integration using just software on the processor. For example, we can read registers in the peripheral devices and check that they are correct. We can carry out DMA transfers. But, we need to control the outside world when testing the full path from software through the UART and SPI devices.</div><div><br /></div><div>Bus functional models (BFMs) or Verification IP (VIP) provide very effective ways to interact with interface protocols from testbench code. What we need in addition is a way to control these BFMs from the software running on the core in the design.</div><div><b><br /></b></div><div><b><br /></b></div><div><b>Capturing the API</b></div><div><br /></div><div>Let's focus on the UART for now. Our <a href="https://github.com/pybfms/pybfms-uart">UART BFM</a> provides a detailed API for configuring individual attributes of the UART protocol (eg baud-rate divisor) and for interacting with the UART protocol a <a href="https://github.com/pybfms/pybfms-uart/blob/9a8ae924a8a599239e5efdde42f2b66f6d4d2440/src/uart_bfms/uart_bfm.py#L31-L52">byte at a time</a>. That's fine for IP-level testing, but is a bit too low-level for software-driven testing.</div><div><br /></div><div>For software-driven testing, we want to instruct the BFM to do some reasonable amount of work and let it go. To help with this, the UART BFM defines a <a href="https://github.com/pybfms/pybfms-uart/blob/master/src/uart_bfms/uart_bfm_sw_api.py">higher-level API</a> intended for use by software.&nbsp;</div><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-4GIcjAmaI3g/YHtF85dMLLI/AAAAAAAADpw/h4Nc63Uaaswm_ul2AnSNK99cObt56LygwCLcBGAsYHQ/s549/HigherLevel_Tx.PNG" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="134" data-original-width="549" src="https://1.bp.blogspot.com/-4GIcjAmaI3g/YHtF85dMLLI/AAAAAAAADpw/h4Nc63Uaaswm_ul2AnSNK99cObt56LygwCLcBGAsYHQ/s16000/HigherLevel_Tx.PNG" /></a></div><br /><div>An example of that higher-level API is shown above. Calling the <i>uart_bfm_tx_bytes_incr</i>&nbsp;API causes the BFM to begin sending a stream of bytes starting with a specific value and incrementing. There is another API that instructs the BFM to expect to receive a stream of bytes sent by the software running on the processor.</div><div><br /></div><div>To enable automation, we describe the Python API that we will call from embedded software using special annotations. We collect related APIs together in a class, and identify whether these methods are <i>exported</i>&nbsp;by the Python environment and will be called by the embedded software, or are <i>imported </i>by the Python and will be called by Python code.&nbsp;</div><div><br /></div><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-IBOvEpdd1XQ/YHtIA434IzI/AAAAAAAADp4/iGDmoLXzeow-JvqBmClH2aUUm8WG4nEsQCLcBGAsYHQ/s641/ApiClass.PNG" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="436" data-original-width="641" src="https://1.bp.blogspot.com/-IBOvEpdd1XQ/YHtIA434IzI/AAAAAAAADp4/iGDmoLXzeow-JvqBmClH2aUUm8WG4nEsQCLcBGAsYHQ/s16000/ApiClass.PNG" /></a></div><br /><div>Since we want embedded software to call this API, the API is considered to be <i>exported</i>&nbsp;by Python. You can also see the configuration function that updates the UART's configuration (eg baud rate).</div><div><br /></div><div>Each of the method parameters is given a Python3 type annotation. This enables the Python libraries to know the type of each parameter and collect the right data to pass when the functions are called.&nbsp;</div><div><br /></div><div>On the C side, we simply need to have functions with the same signature as what we've captured in the Python API definition.</div><div><br /></div><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-BcFSME3qtEM/YHtSAPXzGGI/AAAAAAAADqQ/rTs8AdskVfE-hvhdKPerdP_Mm3jK4D28ACLcBGAsYHQ/s446/C_API.PNG" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="446" data-original-width="411" src="https://1.bp.blogspot.com/-BcFSME3qtEM/YHtSAPXzGGI/AAAAAAAADqQ/rTs8AdskVfE-hvhdKPerdP_Mm3jK4D28ACLcBGAsYHQ/s16000/C_API.PNG" /></a></div><div><br /></div>While the code shown above (<a href="https://github.com/pybfms/pybfms-uart/blob/master/src/uart_bfms/share/sw/c/uart_bfm.c">link</a>) is hand-coded, we could generate it automatically based on what is specified in the Python API definition.&nbsp;<br /><div><br /></div><div><br /></div><div><b>Connecting to Implementation: Python</b></div><div><b><br /></b></div><div>Connecting all of this up on the Python side involves connecting the relevant BFMs and API implementations together.&nbsp;</div><div><br /></div><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-4fZcuLXRITI/YHtP3vlx8fI/AAAAAAAADqI/yNvW6xx40_AwxSEqmr5QJpUmGyG_xuSqwCLcBGAsYHQ/s498/Connect_Python.PNG" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="322" data-original-width="498" src="https://1.bp.blogspot.com/-4fZcuLXRITI/YHtP3vlx8fI/AAAAAAAADqI/yNvW6xx40_AwxSEqmr5QJpUmGyG_xuSqwCLcBGAsYHQ/s16000/Connect_Python.PNG" /></a></div><div><br /></div>The snippet above is from the cocotb test that runs when a baremetal software test is run (<a href="https://github.com/mballance/tiny-soc/blob/main/verilog/common/python/tiny_soc_tests/baremetal.py">link</a>). At the beginning of simulation, the test locates the relevant BFMs. The <i>u_dbg_bfm</i>&nbsp;is the tracer BFM that monitors execution of software on the processor core. This BFM implements an <i>Endpoint</i>, as shown in the diagram at the beginning of the post. The <i>u_uart_bfm</i>&nbsp;is the BFM connected to the UART interface on TinySoC.&nbsp;<div><br /></div><div>Once we have all the BFMs, we can create an instance of the higher-level UART BFM API (<i>uart_bfm_sw</i>) and tell the debug BFM that it should handle the embedded software calling these APIs.<br /><div><br /></div><div><br /></div><div><b>Example C-Test</b></div><div>With the BFMs connected on the Python side, we can now focus on how to interact with the BFM from the software test.</div><div><br /></div><div><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-2JkVBKqE8Ok/YHtS02o9uUI/AAAAAAAADqY/v_kJYqUAKJwyWcLyPikPhfHHHL19A3tTgCLcBGAsYHQ/s557/RxTest.PNG" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="557" data-original-width="536" src="https://1.bp.blogspot.com/-2JkVBKqE8Ok/YHtS02o9uUI/AAAAAAAADqY/v_kJYqUAKJwyWcLyPikPhfHHHL19A3tTgCLcBGAsYHQ/s16000/RxTest.PNG" /></a></div>The software test snippet above transmits some data via the UART to the waiting UART BFM to check (<a href="https://github.com/pybfms/pybfms-uart/blob/master/src/uart_bfms/share/sw/c/uart_bfm.c">link</a>). Before we can send data, both the UART IP and the external BFM need to be configured in the same way. We program the UART IP via its registers, and call the <i>uart_bfm_config</i>&nbsp;function to cause the corresponding Python method to be invoked. This will cause the UART BFM mode to be configured.</div><div><br /></div><div>Next, we call the <i>uart_bfm_rx_bytes_incr </i>to tell the UART BFM that it should expect to receive 20 bytes. It should expect the first byte to have a value 10 and subsequent bytes to increment by one. By telling the BFM what to expect, our test is self-checking and the required amount of interaction is small.</div><div><br /></div><div>Finally, we again interact with the UART IP actually send the data that the BFM is expecting.&nbsp;<br /></div><div><br /></div><div><b>Next Steps</b></div><div>The API definition and Endpoint architecture described in the post above provides a modular way to capture the APIs used to communicate across environments. Because the API signature is captured in machine-readable way, it also enables the use of automation when implementing the APIs for different environments.&nbsp;</div><div><br /></div><div>As I mentioned at the beginning of the post, the API and Endpoint architecture is designed so it can be applied in many verification environments -- it's certainly not restricted to just communicating between embedded software test and the test harness. I've been interested for a while in methodology for creating and verifying firmware along with the IP that it controls such that it's ready to go when SoC-integration testing begins. My next post will begin exploring how to create, verify, and deliver firmware along with an IP.</div><div><br /></div><div><b><i>References</i></b></div><p></p><ul style="text-align: left;"><li>pybfms-uart --&nbsp;<a href="https://github.com/pybfms/pybfms-uart">https://github.com/pybfms/pybfms-uart</a></li><li>hvl-rpc --&nbsp;<a href="https://github.com/fvutils/pyhvl-rpc">https://github.com/fvutils/pyhvl-rpc</a></li><li>tiny-soc --&nbsp;<a href="https://github.com/mballance/tiny-soc">https://github.com/mballance/tiny-soc</a></li></ul><p></p><p><br /></p><div class="separator" style="background-color: white; clear: both; color: #666666; font-family: &quot;Trebuchet MS&quot;, Trebuchet, Verdana, sans-serif; font-size: 13.2px;"><b><i>Disclaimer</i></b></div><div><div class="separator" style="background-color: white; clear: both; color: #666666; font-family: &quot;Trebuchet MS&quot;, Trebuchet, Verdana, sans-serif; font-size: 13.2px;"><i>The views and opinions expressed above are solely those of the author and do not represent those of my employer or any other party.</i></div></div><div class="separator" style="background-color: white; clear: both; color: #666666; font-family: &quot;Trebuchet MS&quot;, Trebuchet, Verdana, sans-serif; font-size: 13.2px;"><i><br /></i></div></div>]]></content><author><name>Matthew Ballance</name></author><category term="PyBFMs" /><category term="SoC" /><category term="BFMs" /><category term="Software-Driven Verification" /><category term="Bus Functional Models" /><category term="System-Level Verification" /><summary type="html"><![CDATA[Controlling the outside world -- specifically interface BFMs -- from embedded software is critical to SoC integration tests that exercise interface IP. In the last post, we showed how to pass data from embedded software to Python by tracing execution of the processor core and reading the mirrored values of registers and memory to obtain parameter values. While functional, doing things in this way is highly specific to one message-passing approach and is pretty labor intensive. In this post, we'll add some abstraction and automation to improve usability and scalability.Design GoalsWhile we're initially focused on providing a nice automated way to communicate between embedded software and the test harness in a simulation environment, the design goals go beyond this. The diagram above shows the basic architecture. Endpoints&nbsp;provide a portal for one environment to call APIs in another environment. Each endpoint&nbsp;supports a known set of APIs, and different endpoints will support different sets of APIs.&nbsp;Each environment interacts with APIs on an endpoint without needing to know how communication is implemented. For example, execution trace might be used to implement processor to Python communication in a simulation-based environment. When the design is synthesized to FPGA, communication might be implemented via an external interface. With appropriate abstraction, neither the test software running on the processor nor the Python test code should need to change despite the fact that data is being moved in very different ways.&nbsp;In order for this to be feasible, we'll need to collect some meta-data about the APIs.ExampleI always find an example to be helpful, so let's look at the enhancements to the flow in the context of a simple example.&nbsp;The diagram above shows the key elements of a very small SoC called Tiny SoC. We can test many aspects of integration using just software on the processor. For example, we can read registers in the peripheral devices and check that they are correct. We can carry out DMA transfers. But, we need to control the outside world when testing the full path from software through the UART and SPI devices.Bus functional models (BFMs) or Verification IP (VIP) provide very effective ways to interact with interface protocols from testbench code. What we need in addition is a way to control these BFMs from the software running on the core in the design.Capturing the APILet's focus on the UART for now. Our UART BFM provides a detailed API for configuring individual attributes of the UART protocol (eg baud-rate divisor) and for interacting with the UART protocol a byte at a time. That's fine for IP-level testing, but is a bit too low-level for software-driven testing.For software-driven testing, we want to instruct the BFM to do some reasonable amount of work and let it go. To help with this, the UART BFM defines a higher-level API intended for use by software.&nbsp;An example of that higher-level API is shown above. Calling the uart_bfm_tx_bytes_incr&nbsp;API causes the BFM to begin sending a stream of bytes starting with a specific value and incrementing. There is another API that instructs the BFM to expect to receive a stream of bytes sent by the software running on the processor.To enable automation, we describe the Python API that we will call from embedded software using special annotations. We collect related APIs together in a class, and identify whether these methods are exported&nbsp;by the Python environment and will be called by the embedded software, or are imported by the Python and will be called by Python code.&nbsp;Since we want embedded software to call this API, the API is considered to be exported&nbsp;by Python. You can also see the configuration function that updates the UART's configuration (eg baud rate).Each of the method parameters is given a Python3 type annotation. This enables the Python libraries to know the type of each parameter and collect the right data to pass when the functions are called.&nbsp;On the C side, we simply need to have functions with the same signature as what we've captured in the Python API definition.While the code shown above (link) is hand-coded, we could generate it automatically based on what is specified in the Python API definition.&nbsp;Connecting to Implementation: PythonConnecting all of this up on the Python side involves connecting the relevant BFMs and API implementations together.&nbsp;The snippet above is from the cocotb test that runs when a baremetal software test is run (link). At the beginning of simulation, the test locates the relevant BFMs. The u_dbg_bfm&nbsp;is the tracer BFM that monitors execution of software on the processor core. This BFM implements an Endpoint, as shown in the diagram at the beginning of the post. The u_uart_bfm&nbsp;is the BFM connected to the UART interface on TinySoC.&nbsp;Once we have all the BFMs, we can create an instance of the higher-level UART BFM API (uart_bfm_sw) and tell the debug BFM that it should handle the embedded software calling these APIs.Example C-TestWith the BFMs connected on the Python side, we can now focus on how to interact with the BFM from the software test.The software test snippet above transmits some data via the UART to the waiting UART BFM to check (link). Before we can send data, both the UART IP and the external BFM need to be configured in the same way. We program the UART IP via its registers, and call the uart_bfm_config&nbsp;function to cause the corresponding Python method to be invoked. This will cause the UART BFM mode to be configured.Next, we call the uart_bfm_rx_bytes_incr to tell the UART BFM that it should expect to receive 20 bytes. It should expect the first byte to have a value 10 and subsequent bytes to increment by one. By telling the BFM what to expect, our test is self-checking and the required amount of interaction is small.Finally, we again interact with the UART IP actually send the data that the BFM is expecting.&nbsp;Next StepsThe API definition and Endpoint architecture described in the post above provides a modular way to capture the APIs used to communicate across environments. Because the API signature is captured in machine-readable way, it also enables the use of automation when implementing the APIs for different environments.&nbsp;As I mentioned at the beginning of the post, the API and Endpoint architecture is designed so it can be applied in many verification environments -- it's certainly not restricted to just communicating between embedded software test and the test harness. I've been interested for a while in methodology for creating and verifying firmware along with the IP that it controls such that it's ready to go when SoC-integration testing begins. My next post will begin exploring how to create, verify, and deliver firmware along with an IP.Referencespybfms-uart --&nbsp;https://github.com/pybfms/pybfms-uarthvl-rpc --&nbsp;https://github.com/fvutils/pyhvl-rpctiny-soc --&nbsp;https://github.com/mballance/tiny-socDisclaimerThe views and opinions expressed above are solely those of the author and do not represent those of my employer or any other party.]]></summary></entry><entry><title type="html">SoC Integration Testing: Hw/Sw Test Coordination (Part 1)</title><link href="https://bitsbytesgates.com/2021/03/28/soc-integration-testing-hwsw-test.html" rel="alternate" type="text/html" title="SoC Integration Testing: Hw/Sw Test Coordination (Part 1)" /><published>2021-03-28T17:58:00+00:00</published><updated>2021-03-28T17:58:00+00:00</updated><id>https://bitsbytesgates.com/2021/03/28/soc-integration-testing-hwsw-test</id><content type="html" xml:base="https://bitsbytesgates.com/2021/03/28/soc-integration-testing-hwsw-test.html"><![CDATA[<p style="text-align: center;">&nbsp;</p><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-WV71pb0_ZF4/YF9cvK_8etI/AAAAAAAADio/uJ5BsUYxoj8Z91U1OQGBdT6OzD3J0dSAACLcBGAsYHQ/s540/splash.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="300" data-original-width="540" src="https://1.bp.blogspot.com/-WV71pb0_ZF4/YF9cvK_8etI/AAAAAAAADio/uJ5BsUYxoj8Z91U1OQGBdT6OzD3J0dSAACLcBGAsYHQ/s16000/splash.png" /></a></div><br /><p></p><p><br /></p><p>IP- and subsystem-level testbenches are quite monolithic. There is a single entity (the testbench) that applies stimulus to the design, collects metrics, and checks results. In contrast, an SoC-level testbench is composed of at least two islands: the software running on the design’s processor and the external testbench connected to the design interfaces. Efficiently developing SoC tests involving both islands requires the ability to easily and efficiently coordinate their activity.</p><p>There are a two times when it’s imperative that the behavior of the test island(s) inside the design and the test island outside the design are coordinated – specifically, the beginning and end of the test when all islands must be in agreement. But, there are many other points in time where it is advantageous to be able communicate between the test islands.&nbsp;</p><p>Especially when running in simulation, the ability to efficiently pass debug information from software out to the test harness dramatically speeds debug.&nbsp;</p><p>It’s often useful to collect metrics on what’s happening in the software environment during test – think of this as functional coverage for software.&nbsp;</p><p>Verifying our design requires applying external stimulus to prove that the design (including firmware) reacts appropriately. This requires the ability to coordinate between initiating traffic on external interfaces and running firmware on the design processors to react – another excellent application of hardware/software coordination.&nbsp;</p><p>Often, checking results consumes a particularly-large portion of the software-test’s time. The ability to offload this to the test harness (which runs on the host server) can shorten our simulation times significantly.&nbsp;</p><p><b>Key Care-Abouts</b></p><p>When it comes to our key requirements for communication, one of the biggest is efficiency – at least while we’re in simulation. The key metric being how many clock cycles it takes to transfer data from software to testbench. When we look at a simulation log, we want to see most activity (and simulation time) focused on actually testing our SoC, and not on sending debug messages back to the test harness. A mechanism with a low overhead will allow us to collect more debug data, check more results, and generally have more flexibility and freedom in transferring data between the two islands.</p><p><b><i>Non-Invasive</i></b></p><p>One approach to efficiency is to use custom hardware for communication. Currently, though this may change, building the communication path into the design seems to be disfavored. So, having the communication path be non-invasive is a big plus.</p><p><b><i>Portable</i></b></p><p>Designs, of course, don’t stay in simulation forever. The end goal is to run them in emulation and prototyping for performance validation, then eventually on real silicon where validation continues -- just at much higher execution speed. Ideally, our communication path will be portable across these changes in environment. The low-level transport may change – for example, we may move from a shared-memory mailbox to using an external interface – but we shouldn’t need to fundamentally change our embedded software tests or the test behavior running on the test harness.</p><p><b><i>Scalable</i></b></p><p></p><p>A key consideration – which really has nothing to do with the communication medium at all – is how scalable the solution is in general. How much work is required to add a piece of data (message, function, etc) that will be communicated? How much specialized expertise is required? The simpler the process is to incrementally enhance the data communicated, the greater the likelihood that it will be used.</p><p><b>Current Approaches</b></p><p>Of the approaches that I’ve seen in use, most involve either software-accessible memory or the use of an existing external interface as the transport mechanism between software and the external test harness. In fact, one of the earliest cases of hardware/software interaction that I used was the Arm Trickbox – a memory-mapped special-purpose hardware device that supported sending messages to the simulation transcript and terminating the test, among other actions.</p><p></p><p>In both of these cases, some amount of code will run on the processor to format messages and put them in the mailbox or send them via the interface.&nbsp;</p><p><b>Challenges</b></p><p>Using a memory-based communication is generally possible in a simulation-based environment – provided we can snoop writes to memory, and/or read memory contents directly from the test harness. That doesn’t mean that memory-based communication is efficient, though, and in simulation, we care a lot about efficiency due to the speed of hardware simulators.</p><p>Our first challenge comes from the fact that all data coming from the software environment needs to be copied from its original location in memory into the shared-memory mailbox. This is because the test harness only has access to portions of the address space, and generally can’t piece together data stored in caches. The result is that we have to copy all data sent from software to the test harness out to main (non-cached) memory. Accessing main memory is slow, and thus communication between software and the test harness significantly lengthens our simulations.</p><p>Our second challenge comes from the fact that the mailbox is likely to be smaller than the largest message we wish to send. This means that our libraries on both sides of the mailbox need to manage synchronizing data transmission with available space in the mailbox. This means that one of the first tasks we need to undertake when bringing up our SoC is to test the communication path between software and test harness.</p><p></p><p>A final challenge, which really ought not to be a challenge, is that we’ll often end up custom-developing the communication mechanism since there aren’t readily-available reusable libraries that we can easily deploy. More about that later.</p><p><b>Making use of Execution Trace</b></p><p><a href="https://bitsbytesgates.blogspot.com/2021/01/soc-integration-testing-higher-level.html">In a previous post</a>, I wrote about using processor-execution trace for enhanced debug. I've also used processor trace as a simple way to detect test termination. For example, here is the Python test-harness code that terminates the test when one of 'test_pass' or 'test_fail' are invoked:</p><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-bKYGmYKAeKg/YF9gH4FjidI/AAAAAAAADiw/EGsI42J_cOUsZ1frVsi_EcVxpVZaAuh5gCLcBGAsYHQ/s613/TestTermination_1.PNG" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="204" data-original-width="613" src="https://1.bp.blogspot.com/-bKYGmYKAeKg/YF9gH4FjidI/AAAAAAAADiw/EGsI42J_cOUsZ1frVsi_EcVxpVZaAuh5gCLcBGAsYHQ/s16000/TestTermination_1.PNG" /></a></div><p>In order to support test-result checking, the processor-execution trace BFM has the ability to track both the register state and memory state as execution proceeds.</p><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-zwqFc3WrFhs/YF9g2huL96I/AAAAAAAADi4/5FWlKf--AHsMJbFWZZvInHlapMq-OM9zQCLcBGAsYHQ/s288/CoreBFM_diagram.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="264" data-original-width="288" src="https://1.bp.blogspot.com/-zwqFc3WrFhs/YF9g2huL96I/AAAAAAAADi4/5FWlKf--AHsMJbFWZZvInHlapMq-OM9zQCLcBGAsYHQ/s0/CoreBFM_diagram.png" /></a></div><div class="separator" style="clear: both; text-align: center;"><br /></div><div class="separator" style="clear: both; text-align: left;">The memory mirror is a sparse memory model that contains only the data that the core is actively using. It's initialized from the software image loaded into simulation memory, and updated when the core performs a write. The memory mirror provides the view of memory from the processor core's perspective -- in other words, pre-cache.&nbsp;</div><p>Our test harness has access to the processor core's view of register values and memory content at the point that a function is called. As it turns out, we can build on this to create a very efficient way to transferring data from software to the test harness.</p><p>In order to access the value of function parameters, we need to know the calling convention for our processor core. Here's the table describing register usage in the RISC-V calling convention:</p><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-4X4Te8faB1g/YF9j89Ht-QI/AAAAAAAADjA/8w8tX8Nlho8zfW03MohsxsBXPPfCehdXQCLcBGAsYHQ/s469/CallingConvention_1.PNG" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="412" data-original-width="469" height="351" src="https://1.bp.blogspot.com/-4X4Te8faB1g/YF9j89Ht-QI/AAAAAAAADjA/8w8tX8Nlho8zfW03MohsxsBXPPfCehdXQCLcBGAsYHQ/w400-h351/CallingConvention_1.PNG" width="400" /></a></div><div class="separator" style="clear: both; text-align: left;">Note that x10-17 are used to pass the first eight function arguments.&nbsp;</div><div class="separator" style="clear: both; text-align: left;"><br /></div><div class="separator" style="clear: both; text-align: left;"><b><i>Creating Abstraction</i></b></div><div class="separator" style="clear: both; text-align: left;">We could, of course, directly access registers and memory from our test-harness code to get the value of function parameters. But, a little abstraction will help us out in the long run.</div><div class="separator" style="clear: both; text-align: left;"><br /></div><div class="separator" style="clear: both; text-align: left;">The architecture-independent core-debug BFM defines a class API for accessing the value of function parameters. This is very similar to the varadic-argument API used in C programming:</div><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-eH4tfBi7W7U/YF9lf0svRBI/AAAAAAAADjI/iVlvGCKrdzUtEPDtTvH666fU2Y4lnQm4QCLcBGAsYHQ/s498/ParamsIterator_1.PNG" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="296" data-original-width="498" height="238" src="https://1.bp.blogspot.com/-eH4tfBi7W7U/YF9lf0svRBI/AAAAAAAADjI/iVlvGCKrdzUtEPDtTvH666fU2Y4lnQm4QCLcBGAsYHQ/w400-h238/ParamsIterator_1.PNG" width="400" /></a></div><br /><div class="separator" style="clear: both; text-align: left;"><br /></div>Now, we just need to implement a RISC-V specific version of this API in order to simplify accessing function parameter values:<div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-jwc45mpxSgE/YF9l5xwZdsI/AAAAAAAADjQ/Xi4l_A9Zle0qR2pgZJymqXYuWZQCPPlyACLcBGAsYHQ/s532/RiscVParamsIterator_1.PNG" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="470" data-original-width="532" height="354" src="https://1.bp.blogspot.com/-jwc45mpxSgE/YF9l5xwZdsI/AAAAAAAADjQ/Xi4l_A9Zle0qR2pgZJymqXYuWZQCPPlyACLcBGAsYHQ/w400-h354/RiscVParamsIterator_1.PNG" width="400" /></a></div><br /><div>Here's how we use this implementation. Assume we have a embedded-software function like this:</div><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-qPTNifjMFhU/YF9nn2HFzyI/AAAAAAAADjY/xOoAXIK_ZPwje_s_py0sadnzaSH9atddQCLcBGAsYHQ/s235/FuncS.PNG" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="65" data-original-width="235" src="https://1.bp.blogspot.com/-qPTNifjMFhU/YF9nn2HFzyI/AAAAAAAADjY/xOoAXIK_ZPwje_s_py0sadnzaSH9atddQCLcBGAsYHQ/s0/FuncS.PNG" /></a></div>When we detect that this function has been called, we can access the value of the string passed to the function from the test harness like this:<div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-SNbqbC6mvrs/YF9nzR5GHZI/AAAAAAAADjc/B_BIXMpOt9Yu3tCslNBdoFehF3DYcI5wQCLcBGAsYHQ/s344/FuncS_TB.PNG" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="55" data-original-width="344" src="https://1.bp.blogspot.com/-SNbqbC6mvrs/YF9nzR5GHZI/AAAAAAAADjc/B_BIXMpOt9Yu3tCslNBdoFehF3DYcI5wQCLcBGAsYHQ/s320/FuncS_TB.PNG" width="320" /></a></div><div class="separator" style="clear: both; text-align: left;"><br /></div><div class="separator" style="clear: both; text-align: left;"><br /></div><div><div><b><i>Advantages</i></b></div><div>There are several advantages to using a trace-driven approach to data communication between processor core and test harness. Because the trace BFM sees the processor's view of memory, there's no need to (slowly) copy data out to main memory in order for the test harness to see it. This allows data to stay in caches and avoids unnecessary copying.</div><div><br /></div><div>Perhaps more importantly, our trace-based communication mechanism allow us to offload data processing to the test harness. Take, for example, the very-common debug printf:</div><div><br /></div><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-_CsvguYOpeg/YF9pRHnsVcI/AAAAAAAADjo/WMq5h1EOrBAZathYfTwE-olWKZPyqwCQwCLcBGAsYHQ/s345/Print.PNG" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="77" data-original-width="345" src="https://1.bp.blogspot.com/-_CsvguYOpeg/YF9pRHnsVcI/AAAAAAAADjo/WMq5h1EOrBAZathYfTwE-olWKZPyqwCQwCLcBGAsYHQ/s320/Print.PNG" width="320" /></a></div><div><br /></div>The user passes a format string and then a variable number of arguments that will all be converted to string representations that can be displayed. If our communication mechanism is an in-memory mailbox or external interface, we need to perform the string formatting on the design's processor core. If, however, we use the trace-based mechanism for communication, the string formatting can all be done by the test harness in zero simulation time. This allows us to keep our simulations shorter and more-focused on the test at hand, while maximizing the debug and metrics data we collect.</div><div><p><br /></p><p><b>Next Steps</b></p><p>SoC integration tests are distributed tests carried out by islands of test behavior running on the processor(s) and on the test harness controlling the external interfaces. Testing more-interesting scenarios requires coordinating these islands of test functionality.&nbsp;</p><p>In this post, we’ve looked at using execution-trace to implement a high-efficiency mechanism for communicating from embedded test software back to the test harness. While this mechanism is mostly-specific to simulation, it has the advantage of simplifying communication, debug, and metrics collection at this early phase of integration testing when, arguably, we most-need a high degree of visibility.&nbsp;</p><p>While we have an efficient mechanism, we don’t yet has a mechanism that makes it easy to add new APIs (scalable) nor a mechanism that is easily portable to environments that need to use a different transport mechanism.</p><p></p><p>In the next post, we’ll have a look at putting some structure and abstraction around communication that will help with both of these points.</p><div><b><i>References</i></b></div><div><ul style="text-align: left;"><li>RISC-V Calling Conventions (ABI) – <a href="https://riscv.org/wp-content/uploads/2015/01/riscv-calling.pdf">https://riscv.org/wp-content/uploads/2015/01/riscv-calling.pdf</a></li><li>pybfms-core-debug-common – <a href="https://github.com/pybfms/pybfms-core-debug-common">https://github.com/pybfms/pybfms-core-debug-common</a></li><li>pybfms-riscv-debug – <a href="https://github.com/pybfms/pybfms_riscv_debug">https://github.com/pybfms/pybfms_riscv_debug</a>&nbsp;</li><li>pyhvl-rpc --<i>&nbsp;</i><a href="https://github.com/fvutils/pyhvl-rpc">https://github.com/fvutils/pyhvl-rpc</a></li></ul><div><i><br /></i></div></div><div><br /></div><div><div class="separator" style="background-color: white; clear: both; color: #666666; font-family: &quot;Trebuchet MS&quot;, Trebuchet, Verdana, sans-serif; font-size: 13.2px;"><b><i>Disclaimer</i></b></div><div><div class="separator" style="background-color: white; clear: both; color: #666666; font-family: &quot;Trebuchet MS&quot;, Trebuchet, Verdana, sans-serif; font-size: 13.2px;"><i>The views and opinions expressed above are solely those of the author and do not represent those of my employer or any other party.</i></div></div></div><div class="separator" style="background-color: white; clear: both; color: #666666; font-family: &quot;Trebuchet MS&quot;, Trebuchet, Verdana, sans-serif; font-size: 13.2px;"><i><br /></i></div><div class="separator" style="background-color: white; clear: both; color: #666666; font-family: &quot;Trebuchet MS&quot;, Trebuchet, Verdana, sans-serif; font-size: 13.2px;"><i><br /></i></div><p></p></div>]]></content><author><name>Matthew Ballance</name></author><category term="SoC" /><category term="BFMs" /><category term="Software-Driven Verification" /><category term="RISC-V" /><category term="Design Verification" /><summary type="html"><![CDATA[&nbsp;IP- and subsystem-level testbenches are quite monolithic. There is a single entity (the testbench) that applies stimulus to the design, collects metrics, and checks results. In contrast, an SoC-level testbench is composed of at least two islands: the software running on the design’s processor and the external testbench connected to the design interfaces. Efficiently developing SoC tests involving both islands requires the ability to easily and efficiently coordinate their activity.There are a two times when it’s imperative that the behavior of the test island(s) inside the design and the test island outside the design are coordinated – specifically, the beginning and end of the test when all islands must be in agreement. But, there are many other points in time where it is advantageous to be able communicate between the test islands.&nbsp;Especially when running in simulation, the ability to efficiently pass debug information from software out to the test harness dramatically speeds debug.&nbsp;It’s often useful to collect metrics on what’s happening in the software environment during test – think of this as functional coverage for software.&nbsp;Verifying our design requires applying external stimulus to prove that the design (including firmware) reacts appropriately. This requires the ability to coordinate between initiating traffic on external interfaces and running firmware on the design processors to react – another excellent application of hardware/software coordination.&nbsp;Often, checking results consumes a particularly-large portion of the software-test’s time. The ability to offload this to the test harness (which runs on the host server) can shorten our simulation times significantly.&nbsp;Key Care-AboutsWhen it comes to our key requirements for communication, one of the biggest is efficiency – at least while we’re in simulation. The key metric being how many clock cycles it takes to transfer data from software to testbench. When we look at a simulation log, we want to see most activity (and simulation time) focused on actually testing our SoC, and not on sending debug messages back to the test harness. A mechanism with a low overhead will allow us to collect more debug data, check more results, and generally have more flexibility and freedom in transferring data between the two islands.Non-InvasiveOne approach to efficiency is to use custom hardware for communication. Currently, though this may change, building the communication path into the design seems to be disfavored. So, having the communication path be non-invasive is a big plus.PortableDesigns, of course, don’t stay in simulation forever. The end goal is to run them in emulation and prototyping for performance validation, then eventually on real silicon where validation continues -- just at much higher execution speed. Ideally, our communication path will be portable across these changes in environment. The low-level transport may change – for example, we may move from a shared-memory mailbox to using an external interface – but we shouldn’t need to fundamentally change our embedded software tests or the test behavior running on the test harness.ScalableA key consideration – which really has nothing to do with the communication medium at all – is how scalable the solution is in general. How much work is required to add a piece of data (message, function, etc) that will be communicated? How much specialized expertise is required? The simpler the process is to incrementally enhance the data communicated, the greater the likelihood that it will be used.Current ApproachesOf the approaches that I’ve seen in use, most involve either software-accessible memory or the use of an existing external interface as the transport mechanism between software and the external test harness. In fact, one of the earliest cases of hardware/software interaction that I used was the Arm Trickbox – a memory-mapped special-purpose hardware device that supported sending messages to the simulation transcript and terminating the test, among other actions.In both of these cases, some amount of code will run on the processor to format messages and put them in the mailbox or send them via the interface.&nbsp;ChallengesUsing a memory-based communication is generally possible in a simulation-based environment – provided we can snoop writes to memory, and/or read memory contents directly from the test harness. That doesn’t mean that memory-based communication is efficient, though, and in simulation, we care a lot about efficiency due to the speed of hardware simulators.Our first challenge comes from the fact that all data coming from the software environment needs to be copied from its original location in memory into the shared-memory mailbox. This is because the test harness only has access to portions of the address space, and generally can’t piece together data stored in caches. The result is that we have to copy all data sent from software to the test harness out to main (non-cached) memory. Accessing main memory is slow, and thus communication between software and the test harness significantly lengthens our simulations.Our second challenge comes from the fact that the mailbox is likely to be smaller than the largest message we wish to send. This means that our libraries on both sides of the mailbox need to manage synchronizing data transmission with available space in the mailbox. This means that one of the first tasks we need to undertake when bringing up our SoC is to test the communication path between software and test harness.A final challenge, which really ought not to be a challenge, is that we’ll often end up custom-developing the communication mechanism since there aren’t readily-available reusable libraries that we can easily deploy. More about that later.Making use of Execution TraceIn a previous post, I wrote about using processor-execution trace for enhanced debug. I've also used processor trace as a simple way to detect test termination. For example, here is the Python test-harness code that terminates the test when one of 'test_pass' or 'test_fail' are invoked:In order to support test-result checking, the processor-execution trace BFM has the ability to track both the register state and memory state as execution proceeds.The memory mirror is a sparse memory model that contains only the data that the core is actively using. It's initialized from the software image loaded into simulation memory, and updated when the core performs a write. The memory mirror provides the view of memory from the processor core's perspective -- in other words, pre-cache.&nbsp;Our test harness has access to the processor core's view of register values and memory content at the point that a function is called. As it turns out, we can build on this to create a very efficient way to transferring data from software to the test harness.In order to access the value of function parameters, we need to know the calling convention for our processor core. Here's the table describing register usage in the RISC-V calling convention:Note that x10-17 are used to pass the first eight function arguments.&nbsp;Creating AbstractionWe could, of course, directly access registers and memory from our test-harness code to get the value of function parameters. But, a little abstraction will help us out in the long run.The architecture-independent core-debug BFM defines a class API for accessing the value of function parameters. This is very similar to the varadic-argument API used in C programming:Now, we just need to implement a RISC-V specific version of this API in order to simplify accessing function parameter values:Here's how we use this implementation. Assume we have a embedded-software function like this:When we detect that this function has been called, we can access the value of the string passed to the function from the test harness like this:AdvantagesThere are several advantages to using a trace-driven approach to data communication between processor core and test harness. Because the trace BFM sees the processor's view of memory, there's no need to (slowly) copy data out to main memory in order for the test harness to see it. This allows data to stay in caches and avoids unnecessary copying.Perhaps more importantly, our trace-based communication mechanism allow us to offload data processing to the test harness. Take, for example, the very-common debug printf:The user passes a format string and then a variable number of arguments that will all be converted to string representations that can be displayed. If our communication mechanism is an in-memory mailbox or external interface, we need to perform the string formatting on the design's processor core. If, however, we use the trace-based mechanism for communication, the string formatting can all be done by the test harness in zero simulation time. This allows us to keep our simulations shorter and more-focused on the test at hand, while maximizing the debug and metrics data we collect.Next StepsSoC integration tests are distributed tests carried out by islands of test behavior running on the processor(s) and on the test harness controlling the external interfaces. Testing more-interesting scenarios requires coordinating these islands of test functionality.&nbsp;In this post, we’ve looked at using execution-trace to implement a high-efficiency mechanism for communicating from embedded test software back to the test harness. While this mechanism is mostly-specific to simulation, it has the advantage of simplifying communication, debug, and metrics collection at this early phase of integration testing when, arguably, we most-need a high degree of visibility.&nbsp;While we have an efficient mechanism, we don’t yet has a mechanism that makes it easy to add new APIs (scalable) nor a mechanism that is easily portable to environments that need to use a different transport mechanism.In the next post, we’ll have a look at putting some structure and abstraction around communication that will help with both of these points.ReferencesRISC-V Calling Conventions (ABI) – https://riscv.org/wp-content/uploads/2015/01/riscv-calling.pdfpybfms-core-debug-common – https://github.com/pybfms/pybfms-core-debug-commonpybfms-riscv-debug – https://github.com/pybfms/pybfms_riscv_debug&nbsp;pyhvl-rpc --&nbsp;https://github.com/fvutils/pyhvl-rpcDisclaimerThe views and opinions expressed above are solely those of the author and do not represent those of my employer or any other party.]]></summary></entry><entry><title type="html">SoC Integration Testing: IP-Integrated Debug and Analysis</title><link href="https://bitsbytesgates.com/2021/02/28/soc-integration-testing-ip-integrated.html" rel="alternate" type="text/html" title="SoC Integration Testing: IP-Integrated Debug and Analysis" /><published>2021-02-28T18:27:00+00:00</published><updated>2021-02-28T18:27:00+00:00</updated><id>https://bitsbytesgates.com/2021/02/28/soc-integration-testing-ip-integrated</id><content type="html" xml:base="https://bitsbytesgates.com/2021/02/28/soc-integration-testing-ip-integrated.html"><![CDATA[<p>&nbsp;</p><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-Uv3Qm-tHLPY/YDrk77i_u0I/AAAAAAAADgk/nBkE_Qvibsc8eUvE_0sC13PQEKEJpRr0ACLcBGAsYHQ/s540/splash.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="300" data-original-width="540" src="https://1.bp.blogspot.com/-Uv3Qm-tHLPY/YDrk77i_u0I/AAAAAAAADgk/nBkE_Qvibsc8eUvE_0sC13PQEKEJpRr0ACLcBGAsYHQ/s16000/splash.png" /></a></div><div><br /></div>One of the things I've always liked about side projects is the freedom to stop and explore a topic of interest as it comes up. One such topic that came up for me recently is IP-integrated debug and analysis instrumentation. I started thinking about this after the last post (<a href="https://bitsbytesgates.blogspot.com/2021/01/soc-integration-testing-higher-level.html">link</a>) which focused on exposing a higher-abstraction-level view of processor-core execution. My initial approach to doing this involved a separate bus-functional model (BFM) intended to connect to any RISC-V processor core via an interface. After my initial work on this bus-functional model that could be bolted onto a RISC-V core, two things occurred to me:<div><ul style="text-align: left;"><li>Wouldn't it be helpful if processor cores came with this type of visibility built-in instead of as a separate bolt-on tool?</li><li>Wouldn't SoC bring-up be simpler if more of the IPs within the SoC exposed an abstracted view of what they were doing internally instead of forcing us to squint at (nearly) meaningless signals and guess?</li></ul><div>And, with that, I decided to take a detour to explore this a bit more. Now, it's not unheard of to create an abstracted view of an IP's operation during block-level verification. Often, external monitors that are used to reconstruct aspects of the design state, and that information is used to guide stimulus generation, or as part of correctness checking. Some amount of probing down into the design may also be done.</div><div><br /></div><div>While this is great for block-level verification, none of this infrastructure can reasonably move forward to the SoC level. That leaves us with extremely limited visibility when trying to debug a failure at SoC level.</div><div><br /></div><div>If debug and analysis instrumentation were embedded into the IP during its development, an abstracted view of the IP's operation would consistently be available independent of whether it's being verified at block level or whether it's part of a much larger system.</div><div><br /></div><div><b>Approach</b></div><div><br /></div><div>After experimenting with this a bit, I've concluded that the process of embedding debug and analysis instrumentation within an IP is actually pretty straightforward. The key goals guiding the approach are:</div><ul style="text-align: left;"><li>Adding instrumentation must impose no overhead when the design is synthesized.&nbsp;</li><li>Exposing debug and analysis information must be optional. We don't want to slow down simulation unnecessarily if we're not even taking advantage of the debug information</li></ul><div>When adding embedded debug and analysis instrumentation to an IP, our first step is to create a 'socket' within the IP to which we can route the lower-level signals from which we'll construct the higher-level view of the IP's operation. From a design RTL perspective, this socket is an empty module whose ports are all inputs. We instance this 'debug-socket' module in the design and connect the signals of interest to it.</div><div><br /></div><div>Because the module contains no implementation and only accepts inputs, synthesis tools very efficiently optimize it out. This means that having the debug socket imposes no overhead on the synthesized result.</div><div><br /></div><div>Of course, we need to plug something into the debug socket. In the example we're about to see, what we put in the socket is a Python-based bus functional model. The same thing could, of course, be done with a SystemVerilog/UVM agent as well.</div><div><br /></div><b>Example - DMA Engine</b></div><div><b><br /></b></div><div>Let's look at a simple example of adding instrumentation to an existing IP. Over the years, I've frequently used the&nbsp;<a href="https://opencores.org/projects/wb_dma">wb_dma core from opencores.org</a>&nbsp;as a learning vehicle, and when creating examples. I created my first OVM testbench around the wb_dma core, learned how to migrate to UVM with it, and have even used it in SoC-level examples.&nbsp;</div><div><br /></div><div><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-jEEt3dPpqwc/YDqdSwhj6PI/AAAAAAAADgM/2Gv1iB1tavIbFg4DBvajQWnLOXxWtIrawCLcBGAsYHQ/s376/DMA_block_diagram.PNG" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="296" data-original-width="376" height="315" src="https://1.bp.blogspot.com/-jEEt3dPpqwc/YDqdSwhj6PI/AAAAAAAADgM/2Gv1iB1tavIbFg4DBvajQWnLOXxWtIrawCLcBGAsYHQ/w400-h315/DMA_block_diagram.PNG" width="400" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">DMA Block Diagram</td></tr></tbody></table><br /><div>The wb_dma IP supports up to 31 DMA channels internally that all communicate with the outside world via two initiator interfaces and are controlled by a register interface. It isn't overly complex, but determining what the DMA engine is attempting to do by observing traffic on the interfaces is a real challenge!</div><div><br /></div><div>When debugging a potential issue with the DMA, the key pieces of information to have are:</div><ul style="text-align: left;"><li>When is a channel active? In other words, when does it have pending transfers to perform?</li><li>When a channel is active, what is it's configuration? In other words, source/destination address, transfer size, etc.</li><li>When is a channel actually performing transfers?</li></ul><div>While there may be additional things we'd like to know, this is a good start.</div><div><br /></div><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-4zzJw_MbDNA/YDrDbP51fnI/AAAAAAAADgc/uIMARWX9sG43RU6ZpZu70Va-bB4SmqxigCLcBGAsYHQ/s833/TransferDetail.PNG" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="569" data-original-width="833" height="438" src="https://1.bp.blogspot.com/-4zzJw_MbDNA/YDrDbP51fnI/AAAAAAAADgc/uIMARWX9sG43RU6ZpZu70Va-bB4SmqxigCLcBGAsYHQ/w640-h438/TransferDetail.PNG" width="640" /></a></div><br /><div><br /></div><div>The waveform trace above shows the abstracted view of operation produced for the DMA engine. Note the groups of traces that each describe what one channel is doing. The <i>dst</i>, <i>src</i>, and <i>sz</i>&nbsp;traces describe how an active channel is configured. If the channel is inactive, these traces are blanked out. The <i>active</i>&nbsp;signal is high when the channel is actually performing transfers. Looking at the duty cycle of the <i>active</i>&nbsp;signals across simultaneously-active channels gives us a good sense for whether a given channel is being given sufficient access to the initiator interfaces.&nbsp;</div><div><br /></div><div>Let's dig into the details a bit more on how this is implemented.</div><div><div class="separator" style="clear: both; text-align: left;"><br /></div></div><div class="separator" style="clear: both; text-align: left;"><b><i>DMA Debug/Analysis Socket</i></b></div><div class="separator" style="clear: both; text-align: left;">We first need to establish a debug/analysis "socket" -- an empty module -- that has access to all the signals we need. In the <i><a href="https://github.com/Featherweight-IP/fwperiph-dma">fwperiph-dma</a></i>&nbsp;IP (a derivative of the original <i>wb_dma</i>&nbsp;project), this socket is implemented by the <a href="https://github.com/Featherweight-IP/fwperiph-dma/blob/main/verilog/rtl/fwperiph_dma_dbg.v">fwperiph_dma_debug module</a>.&nbsp;</div><div class="separator" style="clear: both; text-align: left;"><br /></div><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-kgXALJqCWRQ/YDrx4z8YizI/AAAAAAAADg0/_4q2BypkDRQ5FauJ8LRgFytRyZHoRhEeQCLcBGAsYHQ/s464/fwperiph_dma_dbg.PNG" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="464" data-original-width="386" height="400" src="https://1.bp.blogspot.com/-kgXALJqCWRQ/YDrx4z8YizI/AAAAAAAADg0/_4q2BypkDRQ5FauJ8LRgFytRyZHoRhEeQCLcBGAsYHQ/w333-h400/fwperiph_dma_dbg.PNG" width="333" /></a></div><br /><div class="separator" style="clear: both; text-align: left;">And, that's all we need. The debug/analysis socket has access to:</div><ul style="text-align: left;"><li>Register writes (adr, dat_w, we)</li><li>Information on which channel is active (ch_sel, dma_busy)</li><li>Information on when a transfer completes (dma_done_all)</li></ul><div>Note that, within the module, we have an `ifdef block allowing us to instance a module. This is the mechanism via which we insert the actual debug BFM into the design. Ideally, we would use the SystemVerilog <i>bind</i>&nbsp;construct, but this IP is designed to support a pure-Verilog flow. The `ifdef block accomplishes roughly the same thing as a type bind.</div><div><br /></div><div><b><i>Debug/Analysis BFM</i></b></div><div><br /></div><div>The debug/analysis BFM has two components. One is a <a href="https://github.com/Featherweight-IP/fwperiph-dma/blob/main/verilog/dbg/python/fwperiph_dma_bfms/hdl/fwperiph_dma_dbg_bfm.v">Verilog module</a> that translates from the low-level signals up to operations such as "write channel 2 CSR" and "transfer on channel 3 complete". This module is about 250 lines of code, much of it of low complexity.&nbsp;</div><div><br /></div><div>The other component of the BFM is the <a href="https://github.com/Featherweight-IP/fwperiph-dma/blob/main/verilog/dbg/python/fwperiph_dma_bfms/fwperiph_dma_dbg_bfm.py">Python class</a>&nbsp;that tracks the higher-level view of what channels are active, how they are configured, and ensures that the debug information exposed in signal traces is updated. The Python BFM can also provide callbacks to enable higher-level analysis in Python. The Python BFM is around 150 lines of code.&nbsp;</div><div><br /></div><div>So, in total we have ~400 lines of code dedicated to debug and analysis -- a similar amount and style to what might be present in a block-level verification environment. The difference, here, is that this same code is reusable when we move to SoC level.&nbsp;</div><div><br /></div><div><b>Results</b></div><div><b><br /></b></div><div>Thus far, I've mostly used the waveform-centric view provided by the DMA-controller integrated debug. Visual inspection isn't the most-efficient way to do analysis, but I've already had a couple of 'ah-ha' moments while developing some cocotb-based tests for the DMA controller.&nbsp;</div><div><br /></div><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-M-nBbyLdMOE/YDqcfJ-zGYI/AAAAAAAADgE/yTnf_qBc4HcI_iopjHx22bPLVssrSoRigCLcBGAsYHQ/s1490/AlignedStart.PNG" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="550" data-original-width="1490" height="236" src="https://1.bp.blogspot.com/-M-nBbyLdMOE/YDqcfJ-zGYI/AAAAAAAADgE/yTnf_qBc4HcI_iopjHx22bPLVssrSoRigCLcBGAsYHQ/w640-h236/AlignedStart.PNG" width="640" /></a></div><div><br /></div>I was developing a full-traffic test that was intended to keep all DMA channels busy for most of the time when I saw the pattern in the image above. Notice that a transfer starts on each channel (left-hand side), and no other transfers start until all the previously-started transfers are complete (center-screen). Something similar happens on the right-hand side of the trace. Seeing this pattern graphically alerted me that my test was unintentionally waiting for all transfers to complete before starting the next batch, and thus artificially throttling activity on the DMA engine.</div><div><div><br /></div><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-F8aUPl0vfpo/YDrBabTRY5I/AAAAAAAADgU/aqoAAV2EyV4HLrqvs4WDfC6SN1mFSgHTgCLcBGAsYHQ/s1567/RandomStart.PNG" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="550" data-original-width="1567" height="224" src="https://1.bp.blogspot.com/-F8aUPl0vfpo/YDrBabTRY5I/AAAAAAAADgU/aqoAAV2EyV4HLrqvs4WDfC6SN1mFSgHTgCLcBGAsYHQ/w640-h224/RandomStart.PNG" width="640" /></a></div><div><br /></div>With the test issue corrected, the image above shows expected behavior where new transfers start while other channels are still busy.<br /><div><br /></div><div><br /></div><div><b>Looking Forward</b></div><div><b><br /></b></div><div>I've found the notion of IP-integrated debug and analysis instrumentation very intriguing, and early experience indicates that it's useful in practice. It's certainly true that not all IPs benefit from exposing this type of information, but my feeling is that many that contain complex, potentially-parallel, operations exposed via simple interfaces will. Examples, such as DMA engines, processor cores, and PCIe/USB/Ethernet controllers come to mind. And, think how nice it would be to have IP with this capability built-in!</div><div><br /></div><div>In this blog post, we've looked at the information exposed via the waveform trace. This is great to debug the IP's behavior -- while it's being verified on its own or during SoC bring-up. At the SoC level, the higher-level information exposed by at the Python level may be even more important. As we move to SoC level, we become increasingly interested in validation -- specifically, confirming that we have configured the various IPs in the design to support the intended use, but not over-configured them and, thus, incurred excess implementation costs. My feeling is that the information exposed at the Python level can help to derive performance metrics to help answer these questions.</div><div><br /></div><div>This has been a fun detour, and I plan to continue exploring it in the future -- especially, how it can enable higher-level analysis in Python. But, now it's time to look at how we can bring the embedded-software and hardware (Python)&nbsp; portions of our SoC testbench closer together. Look for that in the new few weeks.</div><div><br /></div><div><b><i>References</i></b></div><div><ul style="text-align: left;"><li>wb_dma IP (original Wishbone DMA IP) --&nbsp;<a href="https://opencores.org/projects/wb_dma">https://opencores.org/projects/wb_dma</a></li><li>fwperiph-dma IP (Modified DMA IP) --&nbsp;<a href="https://github.com/Featherweight-IP/fwperiph-dma">https://github.com/Featherweight-IP/fwperiph-dma</a></li></ul></div><div><br /></div><div><div class="separator" style="background-color: white; clear: both; color: #666666; font-family: &quot;Trebuchet MS&quot;, Trebuchet, Verdana, sans-serif; font-size: 13.2px;"><b><i>Disclaimer</i></b></div><div><div class="separator" style="background-color: white; clear: both; color: #666666; font-family: &quot;Trebuchet MS&quot;, Trebuchet, Verdana, sans-serif; font-size: 13.2px;"><i>The views and opinions expressed above are solely those of the author and do not represent those of my employer or any other party.</i></div></div></div><div><br /></div><div><br /></div><div><br /></div><div><br /></div><p></p></div>]]></content><author><name>Matthew Ballance</name></author><category term="PyBFMs" /><category term="SoC" /><category term="Python" /><category term="Cocotb" /><summary type="html"><![CDATA[&nbsp;One of the things I've always liked about side projects is the freedom to stop and explore a topic of interest as it comes up. One such topic that came up for me recently is IP-integrated debug and analysis instrumentation. I started thinking about this after the last post (link) which focused on exposing a higher-abstraction-level view of processor-core execution. My initial approach to doing this involved a separate bus-functional model (BFM) intended to connect to any RISC-V processor core via an interface. After my initial work on this bus-functional model that could be bolted onto a RISC-V core, two things occurred to me:Wouldn't it be helpful if processor cores came with this type of visibility built-in instead of as a separate bolt-on tool?Wouldn't SoC bring-up be simpler if more of the IPs within the SoC exposed an abstracted view of what they were doing internally instead of forcing us to squint at (nearly) meaningless signals and guess?And, with that, I decided to take a detour to explore this a bit more. Now, it's not unheard of to create an abstracted view of an IP's operation during block-level verification. Often, external monitors that are used to reconstruct aspects of the design state, and that information is used to guide stimulus generation, or as part of correctness checking. Some amount of probing down into the design may also be done.While this is great for block-level verification, none of this infrastructure can reasonably move forward to the SoC level. That leaves us with extremely limited visibility when trying to debug a failure at SoC level.If debug and analysis instrumentation were embedded into the IP during its development, an abstracted view of the IP's operation would consistently be available independent of whether it's being verified at block level or whether it's part of a much larger system.ApproachAfter experimenting with this a bit, I've concluded that the process of embedding debug and analysis instrumentation within an IP is actually pretty straightforward. The key goals guiding the approach are:Adding instrumentation must impose no overhead when the design is synthesized.&nbsp;Exposing debug and analysis information must be optional. We don't want to slow down simulation unnecessarily if we're not even taking advantage of the debug informationWhen adding embedded debug and analysis instrumentation to an IP, our first step is to create a 'socket' within the IP to which we can route the lower-level signals from which we'll construct the higher-level view of the IP's operation. From a design RTL perspective, this socket is an empty module whose ports are all inputs. We instance this 'debug-socket' module in the design and connect the signals of interest to it.Because the module contains no implementation and only accepts inputs, synthesis tools very efficiently optimize it out. This means that having the debug socket imposes no overhead on the synthesized result.Of course, we need to plug something into the debug socket. In the example we're about to see, what we put in the socket is a Python-based bus functional model. The same thing could, of course, be done with a SystemVerilog/UVM agent as well.Example - DMA EngineLet's look at a simple example of adding instrumentation to an existing IP. Over the years, I've frequently used the&nbsp;wb_dma core from opencores.org&nbsp;as a learning vehicle, and when creating examples. I created my first OVM testbench around the wb_dma core, learned how to migrate to UVM with it, and have even used it in SoC-level examples.&nbsp;DMA Block DiagramThe wb_dma IP supports up to 31 DMA channels internally that all communicate with the outside world via two initiator interfaces and are controlled by a register interface. It isn't overly complex, but determining what the DMA engine is attempting to do by observing traffic on the interfaces is a real challenge!When debugging a potential issue with the DMA, the key pieces of information to have are:When is a channel active? In other words, when does it have pending transfers to perform?When a channel is active, what is it's configuration? In other words, source/destination address, transfer size, etc.When is a channel actually performing transfers?While there may be additional things we'd like to know, this is a good start.The waveform trace above shows the abstracted view of operation produced for the DMA engine. Note the groups of traces that each describe what one channel is doing. The dst, src, and sz&nbsp;traces describe how an active channel is configured. If the channel is inactive, these traces are blanked out. The active&nbsp;signal is high when the channel is actually performing transfers. Looking at the duty cycle of the active&nbsp;signals across simultaneously-active channels gives us a good sense for whether a given channel is being given sufficient access to the initiator interfaces.&nbsp;Let's dig into the details a bit more on how this is implemented.DMA Debug/Analysis SocketWe first need to establish a debug/analysis "socket" -- an empty module -- that has access to all the signals we need. In the fwperiph-dma&nbsp;IP (a derivative of the original wb_dma&nbsp;project), this socket is implemented by the fwperiph_dma_debug module.&nbsp;And, that's all we need. The debug/analysis socket has access to:Register writes (adr, dat_w, we)Information on which channel is active (ch_sel, dma_busy)Information on when a transfer completes (dma_done_all)Note that, within the module, we have an `ifdef block allowing us to instance a module. This is the mechanism via which we insert the actual debug BFM into the design. Ideally, we would use the SystemVerilog bind&nbsp;construct, but this IP is designed to support a pure-Verilog flow. The `ifdef block accomplishes roughly the same thing as a type bind.Debug/Analysis BFMThe debug/analysis BFM has two components. One is a Verilog module that translates from the low-level signals up to operations such as "write channel 2 CSR" and "transfer on channel 3 complete". This module is about 250 lines of code, much of it of low complexity.&nbsp;The other component of the BFM is the Python class&nbsp;that tracks the higher-level view of what channels are active, how they are configured, and ensures that the debug information exposed in signal traces is updated. The Python BFM can also provide callbacks to enable higher-level analysis in Python. The Python BFM is around 150 lines of code.&nbsp;So, in total we have ~400 lines of code dedicated to debug and analysis -- a similar amount and style to what might be present in a block-level verification environment. The difference, here, is that this same code is reusable when we move to SoC level.&nbsp;ResultsThus far, I've mostly used the waveform-centric view provided by the DMA-controller integrated debug. Visual inspection isn't the most-efficient way to do analysis, but I've already had a couple of 'ah-ha' moments while developing some cocotb-based tests for the DMA controller.&nbsp;I was developing a full-traffic test that was intended to keep all DMA channels busy for most of the time when I saw the pattern in the image above. Notice that a transfer starts on each channel (left-hand side), and no other transfers start until all the previously-started transfers are complete (center-screen). Something similar happens on the right-hand side of the trace. Seeing this pattern graphically alerted me that my test was unintentionally waiting for all transfers to complete before starting the next batch, and thus artificially throttling activity on the DMA engine.With the test issue corrected, the image above shows expected behavior where new transfers start while other channels are still busy.Looking ForwardI've found the notion of IP-integrated debug and analysis instrumentation very intriguing, and early experience indicates that it's useful in practice. It's certainly true that not all IPs benefit from exposing this type of information, but my feeling is that many that contain complex, potentially-parallel, operations exposed via simple interfaces will. Examples, such as DMA engines, processor cores, and PCIe/USB/Ethernet controllers come to mind. And, think how nice it would be to have IP with this capability built-in!In this blog post, we've looked at the information exposed via the waveform trace. This is great to debug the IP's behavior -- while it's being verified on its own or during SoC bring-up. At the SoC level, the higher-level information exposed by at the Python level may be even more important. As we move to SoC level, we become increasingly interested in validation -- specifically, confirming that we have configured the various IPs in the design to support the intended use, but not over-configured them and, thus, incurred excess implementation costs. My feeling is that the information exposed at the Python level can help to derive performance metrics to help answer these questions.This has been a fun detour, and I plan to continue exploring it in the future -- especially, how it can enable higher-level analysis in Python. But, now it's time to look at how we can bring the embedded-software and hardware (Python)&nbsp; portions of our SoC testbench closer together. Look for that in the new few weeks.Referenceswb_dma IP (original Wishbone DMA IP) --&nbsp;https://opencores.org/projects/wb_dmafwperiph-dma IP (Modified DMA IP) --&nbsp;https://github.com/Featherweight-IP/fwperiph-dmaDisclaimerThe views and opinions expressed above are solely those of the author and do not represent those of my employer or any other party.]]></summary></entry></feed>
<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://bitsbytesgates.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://bitsbytesgates.com/" rel="alternate" type="text/html" /><updated>2023-03-19T01:52:37+00:00</updated><id>https://bitsbytesgates.com/feed.xml</id><title type="html">Bits, Bytes, and Gates</title><subtitle>There&apos;s oh so much fun to be had. At the leading edge,  at the bleeding edge, at the confluence of bits, bytes, and gates.</subtitle><entry><title type="html">Relating Actions with Dataflow</title><link href="https://bitsbytesgates.com/pss/2023/03/18/RelatingActionsWithDataflow.html" rel="alternate" type="text/html" title="Relating Actions with Dataflow" /><published>2023-03-18T00:00:00+00:00</published><updated>2023-03-18T00:00:00+00:00</updated><id>https://bitsbytesgates.com/pss/2023/03/18/RelatingActionsWithDataflow</id><content type="html" xml:base="https://bitsbytesgates.com/pss/2023/03/18/RelatingActionsWithDataflow.html"><![CDATA[<p align="center">
<img src="https://bitsbytesgates.com/imgs/2023/03/RelatingActionsWithDataflow_splash.png" /> 
</p>

<p>Modularity and reuse are key concerns when it comes to programming
languages. While languages without many modularity and reuse features may be
quick and easy to write – think shell scripts – they ultimately fail to scale. 
PSS provides a wealth of mechanisms for structuring test content for reuse
that are familiar to users of object-oriented languages. That said, PSS provides
unique approaches to modularity and reuse as well. The PSS <code class="language-plaintext highlighter-rouge">modeling layer</code> 
is strongly declarative,
and this has significant implications on the approach that PSS takes to 
provide reuse features for composing declarative behaviors.</p>

<p>In this post, we will start to look at declarative data relationships via the
PSS <code class="language-plaintext highlighter-rouge">buffer</code> construct.</p>

<h1 id="rewinding-a-bit">Rewinding a Bit…</h1>
<p>Recall that, in the last post, we were creating very simple multi-core
read/write tests, such as what is shown in the diagram below:</p>

<div class="mermaid" align="center">
graph TD;
    A[Core0\nWrite]--&gt;B[Core1\nCopy];
    B--&gt;C[Core0\nCheck];
</div>

<p>In order for our test to make sense, we needed a few relationships to hold:</p>
<ul>
  <li>The source address for our <em>Copy</em> address needed to be the same as the
destination address of the <em>Write</em></li>
  <li>The source address for the <em>Check</em> action needed to be same as the
destination address of the <em>Copy</em></li>
  <li>The source and destination addresses of the <em>Copy</em> needed to be 
different. We didn’t want to clobber our previously-written data until
we’ve had a chance to check it, after all.</li>
</ul>

<div class="language-pss highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">component</span> <span class="nc">memtest_c</span> <span class="p">{</span>

    <span class="c1">// ...</span>

    <span class="kd">action</span> <span class="nc">WriteCopyCheck</span> <span class="p">{</span>
        <span class="n">Write</span>             <span class="n">write</span><span class="p">;</span>
        <span class="n">Copy</span>              <span class="n">copy</span><span class="p">;</span>
        <span class="n">Check</span>             <span class="n">check</span><span class="p">;</span>

        <span class="k">activity</span> <span class="p">{</span>
            <span class="n">write</span><span class="p">;</span>
            <span class="n">copy</span><span class="p">;</span>
            <span class="n">check</span><span class="p">;</span>
        <span class="p">}</span>

        <span class="k">constraint</span> <span class="p">{</span>
            <span class="c1">// Copy reads from same location that Write populated</span>
            <span class="n">copy</span><span class="o">.</span><span class="n">src</span> <span class="o">==</span> <span class="n">write</span><span class="o">.</span><span class="n">offset</span><span class="p">;</span> 
            <span class="c1">// Check reads from the same location that Copy populated</span>
            <span class="n">copy</span><span class="o">.</span><span class="n">dst</span> <span class="o">==</span> <span class="n">check</span><span class="o">.</span><span class="n">offset</span><span class="p">;</span>
            <span class="c1">// All actions write the same number of words</span>
            <span class="n">copy</span><span class="o">.</span><span class="n">words</span> <span class="o">==</span> <span class="n">write</span><span class="o">.</span><span class="n">words</span><span class="p">;</span>
            <span class="n">copy</span><span class="o">.</span><span class="n">words</span> <span class="o">==</span> <span class="n">check</span><span class="o">.</span><span class="n">words</span><span class="p">;</span>

            <span class="c1">// Ensure that src/dst regions do not overlap</span>
            <span class="p">(</span><span class="n">copy</span><span class="o">.</span><span class="n">src</span><span class="o">+</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">copy</span><span class="o">.</span><span class="n">words</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">copy</span><span class="o">.</span><span class="n">dst</span><span class="p">)</span> <span class="o">||</span>
            <span class="p">(</span><span class="n">copy</span><span class="o">.</span><span class="n">src</span> <span class="o">&gt;</span> <span class="n">copy</span><span class="o">.</span><span class="n">dst</span><span class="o">+</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">copy</span><span class="o">.</span><span class="n">words</span><span class="p">));</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="c1">// ...</span>
<span class="p">}</span>
</code></pre></div></div>

<p>In the last post, we took a bit of a shortcut and modeled all of these 
relationships as data constraints in a <em>compound action</em>. While the code above
is perfectly legal and valid in PSS (hopefully you had a chance to try out
the example code with a PSS tool), it doesn’t lend itself to reuse. The specific
problem is that the <em>Check</em> action has some required data relationships that are not 
expressed as part of the action. And, the fact that that we’ve built these 
actions without expressing how they can be related by data means that users
will need to dig into the code to understand the internal variables and 
constraints – clearly not what we expect from modular code.</p>

<h1 id="from-data-to-temporal-declarative-relationships">From Data to Temporal Declarative Relationships</h1>

<p>If you’re coming from a SystemVerilog background, data constraints are most
likely the declarative programming feature that you’re most familiar with. 
SystemVerilog supports declarative descriptions with respect to data, but
not with respect to time (temporally declarative).</p>

<p>What does this mean?</p>

<div class="language-verilog highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">class</span> <span class="n">my_vseq</span> <span class="k">extends</span> <span class="n">uvm_sequence</span><span class="p">;</span>
    <span class="k">task</span> <span class="n">body</span><span class="p">();</span>
        <span class="n">my_subseq</span> <span class="n">seq1</span> <span class="o">=</span> <span class="n">my_subseq</span><span class="o">::</span><span class="n">type_id</span><span class="o">::</span><span class="n">create</span><span class="p">();</span>
        <span class="n">my_subseq</span> <span class="n">seq2</span> <span class="o">=</span> <span class="n">my_subseq</span><span class="o">::</span><span class="n">type_id</span><span class="o">::</span><span class="n">create</span><span class="p">();</span>

        <span class="n">seq1</span><span class="p">.</span><span class="n">start</span><span class="p">(</span><span class="n">m_subsqr</span><span class="p">);</span>
        <span class="n">seq2</span><span class="p">.</span><span class="n">start</span><span class="p">(</span><span class="n">m_subsqr</span><span class="p">);</span>
    <span class="k">endtask</span>
<span class="k">endclass</span>
</code></pre></div></div>

<p>The UVM sequence shows two sub-sequences being run sequentially. SystemVerilog
doesn’t provide any features that allow us to directly relate these two 
sequences declaratively (ie using constraints) while retaining their temporal
relationship. If we need a data relationship to hold across  <code class="language-plaintext highlighter-rouge">seq1</code> and 
<code class="language-plaintext highlighter-rouge">seq2</code>, then we need to group our two classes together in a larger class,
express the data relationship as a constraint in the containing class, 
solve the two sequences together, then deal with selectively executing each
of the sub-sequences in the desired temporal relationship.</p>

<p>It’s certainly not impossible, but can force us into some awkward design patterns 
of collecting lots of otherwise-unrelated classes such that they can be solved 
together before separating them to run over time.</p>

<h1 id="toward-a-declarative-api">Toward a Declarative API</h1>
<p>The cross-action constraints that we used in the previous post are one 
temporally-declarative feature that PSS provides. PSS allows us to express how 
action execution is 
related temporally, add constraints on top, and let the PSS tool worry about how to
group data and constraints such that both the data and temporal relationships
hold over time.</p>

<p>But, PSS goes beyond that as well. PSS provides specific data types for 
expressing the way that data that is shared or passed between temporally-related 
actions, and specific ways for actions to note when they input (require) 
data from other actions and when they output (produce) data for other actions. 
We’ll see more details on how this I/O <code class="language-plaintext highlighter-rouge">contract</code> for actions helps in the future. 
For now, it’s a great feature to assist in making PSS descriptions more modular 
and reusable.</p>

<h1 id="updating-the-memory-test-actions">Updating the Memory-Test Actions</h1>

<p>All of the data relationships in our memory test are between 
sequentially-executing actions. PSS provides the <code class="language-plaintext highlighter-rouge">buffer</code> data type to express
passing data sequentially between actions. The semantics of a <code class="language-plaintext highlighter-rouge">buffer</code> object
match our intuition based on the dictionary definition: it’s a place to store
data produced by one action before being consumed by some other action.</p>

<p>A <code class="language-plaintext highlighter-rouge">buffer</code> is a built-in data type in PSS that is a compound data structure.
In other words, it’s like a <code class="language-plaintext highlighter-rouge">struct</code> in C/C++ or Rust, and like a class 
(but without the methods) in some other languages.</p>

<p>In our application, there are two pieces of data that the actions need agree 
on: address offset and number of words being copied.</p>

<div class="language-pss highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">buffer</span> <span class="nc">mem_b</span> <span class="p">{</span>
    <span class="kd">rand</span> <span class="kt">bit</span><span class="p">[</span><span class="mi">32</span><span class="p">]</span> <span class="k">in</span> <span class="p">[</span><span class="mi">0</span><span class="o">..</span><span class="mh">0xFFFFFF</span><span class="p">]</span> <span class="n">offset</span><span class="p">;</span>
    <span class="kd">rand</span> <span class="kt">bit</span><span class="p">[</span><span class="mi">32</span><span class="p">]</span> <span class="k">in</span> <span class="p">[</span><span class="mi">1</span><span class="o">..</span><span class="mi">256</span><span class="p">]</span>      <span class="n">words</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>We declare these fields inside a data type of kind <code class="language-plaintext highlighter-rouge">buffer</code> to declare our
flow object. Note that these fields are declared <code class="language-plaintext highlighter-rouge">rand</code> because the represent
relationships between the producing and consuming actions, and because 
we intend to constrain them. We will use that <code class="language-plaintext highlighter-rouge">mem_b</code> type 
across our <code class="language-plaintext highlighter-rouge">Write</code>, <code class="language-plaintext highlighter-rouge">Copy</code>, and <code class="language-plaintext highlighter-rouge">Check</code> actions to represent data-flow
relationships. When we update our <em>Write</em>
action to use the buffer, we’ll replace the local <em>offset</em> and <em>words</em> 
fields with references into the buffer field.</p>

<p>Now, let’s compare the old and new versions of the <em>Write</em> action to see
the difference.</p>

<p><strong>Old</strong></p>

<figure class="highlight"><pre><code class="language-pss" data-lang="pss"><span class="kd">action</span> <span class="nc">Write</span> <span class="p">{</span>
  <span class="kd">rand</span> <span class="n">executor_claim_s</span><span class="o">&lt;</span><span class="n">core_s</span><span class="o">&gt;</span> <span class="n">core</span><span class="p">;</span>
  <span class="kd">rand</span> <span class="kt">bit</span><span class="p">[</span><span class="mi">64</span><span class="p">]</span> <span class="k">in</span> <span class="p">[</span><span class="mi">0</span><span class="o">..</span><span class="mh">0xFFFFFF</span><span class="p">]</span> <span class="n">offset</span><span class="p">;</span>
  <span class="kd">rand</span> <span class="kt">bit</span><span class="p">[</span><span class="mi">32</span><span class="p">]</span> <span class="k">in</span> <span class="p">[</span><span class="mi">1</span><span class="o">..</span><span class="mi">256</span><span class="p">]</span>      <span class="n">words</span><span class="p">;</span>

  <span class="k">exec</span> <span class="k">body</span> <span class="p">{</span>
    <span class="k">repeat</span> <span class="p">(</span><span class="n">i</span> <span class="p">:</span> <span class="n">words</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">write32</span><span class="p">(</span>
        <span class="n">make_handle_from_handle</span><span class="p">(</span><span class="n">comp</span><span class="o">.</span><span class="n">base_addr</span><span class="p">,</span> <span class="mi">4</span><span class="o">*</span><span class="p">(</span><span class="n">offset</span><span class="o">+</span><span class="n">i</span><span class="p">)),</span> 
          <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span></code></pre></figure>

<p><strong>New</strong></p>

<figure class="highlight"><pre><code class="language-pss" data-lang="pss"><span class="kd">action</span> <span class="nc">Write</span> <span class="p">{</span>
  <span class="k">output</span> <span class="n">mem_b</span> <span class="n">dat_o</span><span class="p">;</span>
  <span class="kd">rand</span> <span class="n">executor_claim_s</span><span class="o">&lt;</span><span class="n">core_s</span><span class="o">&gt;</span> <span class="n">core</span><span class="p">;</span>

  <span class="k">exec</span> <span class="k">body</span> <span class="p">{</span>
    <span class="k">repeat</span> <span class="p">(</span><span class="n">i</span> <span class="p">:</span> <span class="n">dat_o</span><span class="o">.</span><span class="n">words</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">write32</span><span class="p">(</span>
        <span class="n">make_handle_from_handle</span><span class="p">(</span><span class="n">comp</span><span class="o">.</span><span class="n">base_addr</span><span class="p">,</span> <span class="mi">4</span><span class="o">*</span><span class="p">(</span><span class="n">dat_o</span><span class="o">.</span><span class="n">offset</span><span class="o">+</span><span class="n">i</span><span class="p">)),</span> 
          <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span></code></pre></figure>

<p>Note that we’ve added an output to the action, and replaced use of the local
variables <em>offset</em> and <em>words</em> with references to the fields of the buffer
object output. While remaining functionally the same as before, our <em>Write</em>
action is now much more specific about its participation in the scenario.</p>

<p>When we depict PSS elements graphically, we show an action’s buffer inputs and 
outputs as shown below. Note that the output buffer is shown sequentially
<em>after</em> the <em>Write</em> action, since it is only available after the <em>Write</em> action
is complete.</p>

<div class="mermaid" align="center">
flowchart TD;
    write(Write)--&gt;mem_b([mem_b])
</div>

<p>Okay, let’s update the <em>Copy</em> and <em>Check</em> actions as well. Note that we
have moved the constraints that ensure that the source and destination 
areas do not overlap into the action. This, in addition to using flow objects
to relate the actions, helps to keep things modular and encapsulated.</p>

<figure class="highlight"><pre><code class="language-pss" data-lang="pss"><span class="kd">action</span> <span class="nc">Copy</span> <span class="p">{</span>
  <span class="k">input</span> <span class="n">mem_b</span>  <span class="n">dat_i</span><span class="p">;</span>
  <span class="k">output</span> <span class="n">mem_b</span> <span class="n">dat_o</span><span class="p">;</span>
  <span class="kd">rand</span> <span class="n">executor_claim_s</span><span class="o">&lt;</span><span class="n">core_s</span><span class="o">&gt;</span> <span class="n">core</span><span class="p">;</span>

  <span class="c1">// Ensure we copy the same number of words</span>
  <span class="k">constraint</span> <span class="n">dat_i</span><span class="o">.</span><span class="n">words</span> <span class="o">==</span> <span class="n">dat_o</span><span class="o">.</span><span class="n">words</span><span class="p">;</span>

  <span class="c1">// Ensure that src/dst regions do not overlap</span>
  <span class="k">constraint</span> <span class="p">(</span><span class="n">dat_i</span><span class="o">.</span><span class="n">offset</span><span class="o">+</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">dat_i</span><span class="o">.</span><span class="n">words</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">dat_o</span><span class="o">.</span><span class="n">offset</span><span class="p">)</span> <span class="o">||</span>
            <span class="p">(</span><span class="n">dat_i</span><span class="o">.</span><span class="n">offset</span> <span class="o">&gt;</span> <span class="n">dat_o</span><span class="o">.</span><span class="n">offset</span><span class="o">+</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">dat_i</span><span class="o">.</span><span class="n">words</span><span class="p">));</span>

  <span class="k">exec</span> <span class="k">body</span> <span class="p">{</span>
    <span class="kt">bit</span><span class="p">[</span><span class="mi">32</span><span class="p">]</span> <span class="n">tmp</span><span class="p">;</span>
    <span class="k">repeat</span> <span class="p">(</span><span class="n">i</span> <span class="p">:</span> <span class="n">dat_o</span><span class="o">.</span><span class="n">words</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">tmp</span> <span class="o">=</span> <span class="n">read32</span><span class="p">(</span>
        <span class="n">make_handle_from_handle</span><span class="p">(</span><span class="n">comp</span><span class="o">.</span><span class="n">base_addr</span><span class="p">,</span>     
          <span class="mi">4</span><span class="o">*</span><span class="p">(</span><span class="n">dat_i</span><span class="o">.</span><span class="n">offset</span><span class="o">+</span><span class="n">i</span><span class="p">)));</span>
      <span class="n">write32</span><span class="p">(</span>
        <span class="n">make_handle_from_handle</span><span class="p">(</span><span class="n">comp</span><span class="o">.</span><span class="n">base_addr</span><span class="p">,</span> 
          <span class="mi">4</span><span class="o">*</span><span class="p">(</span><span class="n">dat_o</span><span class="o">.</span><span class="n">offset</span><span class="o">+</span><span class="n">i</span><span class="p">)),</span>
        <span class="n">tmp</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span></code></pre></figure>

<p>Our updated <em>Copy</em> action will look like this when we show it in a 
diagram:</p>

<div class="mermaid" align="center">
flowchart TD;
    mem_b_i([mem_b]) --&gt; copy(Copy) --&gt; mem_b_o([mem_b])
</div>

<figure class="highlight"><pre><code class="language-pss" data-lang="pss"><span class="kd">action</span> <span class="nc">Check</span> <span class="p">{</span>
  <span class="k">input</span> <span class="n">mem_b</span> <span class="n">dat_i</span><span class="p">;</span>
  <span class="kd">rand</span> <span class="n">executor_claim_s</span><span class="o">&lt;</span><span class="n">core_s</span><span class="o">&gt;</span> <span class="n">core</span><span class="p">;</span>

  <span class="k">exec</span> <span class="k">body</span> <span class="p">{</span>
    <span class="kt">bit</span><span class="p">[</span><span class="mi">32</span><span class="p">]</span> <span class="n">tmp</span><span class="p">;</span>
    <span class="k">repeat</span> <span class="p">(</span><span class="n">i</span> <span class="p">:</span> <span class="n">dat_i</span><span class="o">.</span><span class="n">words</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">tmp</span> <span class="o">=</span> <span class="n">read32</span><span class="p">(</span>
        <span class="n">make_handle_from_handle</span><span class="p">(</span><span class="n">comp</span><span class="o">.</span><span class="n">base_addr</span><span class="p">,</span> 
          <span class="mi">4</span><span class="o">*</span><span class="p">(</span><span class="n">dat_i</span><span class="o">.</span><span class="n">offset</span><span class="o">+</span><span class="n">i</span><span class="p">)));</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">tmp</span> <span class="o">!=</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">error</span><span class="p">(</span><span class="s">"0x%08x: expect %d ; receive %d"</span><span class="p">,</span> 
          <span class="mi">4</span><span class="o">*</span><span class="p">(</span><span class="n">dat_i</span><span class="o">.</span><span class="n">offset</span><span class="o">+</span><span class="n">i</span><span class="p">),</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">tmp</span><span class="p">);</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span></code></pre></figure>

<h1 id="updating-the-memory-test-scenario">Updating the Memory-Test Scenario</h1>
<p>Now that our actions are updated to capture the data they require and produce,
we can turn our attention to assembling a scenario. Recall that our original
scenario looked like this, with constraints enforcing all relationships:</p>

<figure class="highlight"><pre><code class="language-pss" data-lang="pss"><span class="kd">action</span> <span class="nc">WriteCopyCheck</span> <span class="p">{</span>
  <span class="n">Write</span>             <span class="n">write</span><span class="p">;</span>
  <span class="n">Copy</span>              <span class="n">copy</span><span class="p">;</span>
  <span class="n">Check</span>             <span class="n">check</span><span class="p">;</span>

  <span class="k">activity</span> <span class="p">{</span>
    <span class="n">write</span><span class="p">;</span>
    <span class="n">copy</span><span class="p">;</span>
    <span class="n">check</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="k">constraint</span> <span class="p">{</span>
    <span class="c1">// Copy reads from same location that Write populated</span>
    <span class="n">copy</span><span class="o">.</span><span class="n">src</span> <span class="o">==</span> <span class="n">write</span><span class="o">.</span><span class="n">offset</span><span class="p">;</span> 
    <span class="c1">// Check reads from the same location that Copy populated</span>
    <span class="n">copy</span><span class="o">.</span><span class="n">dst</span> <span class="o">==</span> <span class="n">check</span><span class="o">.</span><span class="n">offset</span><span class="p">;</span>
    <span class="c1">// All actions write the same number of words</span>
    <span class="n">copy</span><span class="o">.</span><span class="n">words</span> <span class="o">==</span> <span class="n">write</span><span class="o">.</span><span class="n">words</span><span class="p">;</span>
    <span class="n">copy</span><span class="o">.</span><span class="n">words</span> <span class="o">==</span> <span class="n">check</span><span class="o">.</span><span class="n">words</span><span class="p">;</span>

    <span class="c1">// Ensure that src/dst regions do not overlap</span>
    <span class="p">(</span><span class="n">copy</span><span class="o">.</span><span class="n">src</span><span class="o">+</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">copy</span><span class="o">.</span><span class="n">words</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">copy</span><span class="o">.</span><span class="n">dst</span><span class="p">)</span> <span class="o">||</span>
     <span class="p">(</span><span class="n">copy</span><span class="o">.</span><span class="n">src</span> <span class="o">&gt;</span> <span class="n">copy</span><span class="o">.</span><span class="n">dst</span><span class="o">+</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">copy</span><span class="o">.</span><span class="n">words</span><span class="p">));</span>
  <span class="p">}</span>
<span class="p">}</span></code></pre></figure>

<p>Instead of using constraints, we will connect the input and
output buffers on the action together. How to do we form those
connections? The <code class="language-plaintext highlighter-rouge">bind</code> statement.</p>

<h2 id="the-activity-bind-statement">The Activity Bind Statement</h2>
<p>The activity <code class="language-plaintext highlighter-rouge">bind</code> statement is used to connect action I/O
ports together. In its simplest form, a <code class="language-plaintext highlighter-rouge">bind</code> statement 
connects a single input and output. For example:</p>

<figure class="highlight"><pre><code class="language-pss" data-lang="pss"><span class="kd">action</span> <span class="nc">WriteCopyCheck</span> <span class="p">{</span>
  <span class="n">Write</span>             <span class="n">write</span><span class="p">;</span>
  <span class="n">Copy</span>              <span class="n">copy</span><span class="p">;</span>
  <span class="n">Check</span>             <span class="n">check</span><span class="p">;</span>

  <span class="k">activity</span> <span class="p">{</span>
    <span class="n">write</span><span class="p">;</span>
    <span class="n">copy</span><span class="p">;</span>
    <span class="n">check</span><span class="p">;</span>
    <span class="k">bind</span> <span class="n">write</span><span class="o">.</span><span class="n">dat_o</span> <span class="n">copy</span><span class="o">.</span><span class="n">dat_i</span><span class="p">;</span>
    <span class="k">bind</span> <span class="n">copy</span><span class="o">.</span><span class="n">dat_o</span> <span class="n">check</span><span class="o">.</span><span class="n">dat_i</span><span class="p">;</span>
  <span class="p">}</span>

<span class="p">}</span></code></pre></figure>

<p>In this case, we are specifying that the output of the <code class="language-plaintext highlighter-rouge">Write</code>
action and input of the <code class="language-plaintext highlighter-rouge">Copy</code> action are connected, and the
output of the <code class="language-plaintext highlighter-rouge">Copy</code> action and input of the <code class="language-plaintext highlighter-rouge">Check</code> action 
are connected.</p>

<p>We might visualize this as follows.</p>

<div class="mermaid" align="center">
flowchart TB;

  write-.-&gt;mem_b_1
  mem_b_1-.-&gt;copy
  copy-.-&gt;mem_b_2
  mem_b_2-.-&gt;check

  subgraph Dataflow
    mem_b_1([mem_b])
    mem_b_2([mem_b])
  end
  subgraph Procedure
    write(Write)--&gt;copy(Copy)--&gt;check(Check)
  end
</div>

<p>We have two views of the scenario. In the <em>Procedure</em> portion we have 
the temporal relationship between actions (write, copy, check). In the
<em>Dataflow</em> portion, we can see the data objects relating various 
actions.</p>

<h1 id="extending-the-scenario">Extending the Scenario</h1>
<p>As mentioned in the beginning of the post, the <code class="language-plaintext highlighter-rouge">buffer</code> construct
is a feature that enables encapsulation and reuse.
Let’s leverage that reusability to extend our scenario to see how 
this works in practice. Let’s 
say that we want to perform two copies back to back instead of a
single one. All we need to do is add in the second <code class="language-plaintext highlighter-rouge">copy</code> action
and connect it into the scenario with <code class="language-plaintext highlighter-rouge">binds</code>.</p>

<figure class="highlight"><pre><code class="language-pss" data-lang="pss"><span class="kd">action</span> <span class="nc">Write2xCopyCheck</span> <span class="p">{</span>
  <span class="n">Write</span>             <span class="n">write</span><span class="p">;</span>
  <span class="n">Copy</span>              <span class="n">copy1</span><span class="p">;</span>
  <span class="n">Copy</span>              <span class="n">copy2</span><span class="p">;</span>
  <span class="n">Check</span>             <span class="n">check</span><span class="p">;</span>

  <span class="k">activity</span> <span class="p">{</span>
    <span class="n">write</span><span class="p">;</span>
    <span class="n">copy1</span><span class="p">;</span>
    <span class="n">copy2</span><span class="p">;</span>
    <span class="n">check</span><span class="p">;</span>
    <span class="k">bind</span> <span class="n">write</span><span class="o">.</span><span class="n">dat_o</span> <span class="n">copy1</span><span class="o">.</span><span class="n">dat_i</span><span class="p">;</span>
    <span class="k">bind</span> <span class="n">copy1</span><span class="o">.</span><span class="n">dat_o</span> <span class="n">copy2</span><span class="o">.</span><span class="n">dat_i</span><span class="p">;</span>
    <span class="k">bind</span> <span class="n">copy2</span><span class="o">.</span><span class="n">dat_o</span> <span class="n">check</span><span class="o">.</span><span class="n">dat_i</span><span class="p">;</span>
  <span class="p">}</span>
<span class="p">}</span></code></pre></figure>

<div class="mermaid" align="center">
flowchart LR

  write-.-&gt;mem_b_1
  mem_b_1-.-&gt;copy1
  copy1-.-&gt;mem_b_2
  mem_b_2-.-&gt;copy2
  copy2--&gt;mem_b_3
  mem_b_3-.-&gt;check

  subgraph Dataflow
    mem_b_1([mem_b])
    mem_b_2([mem_b])
    mem_b_3([mem_b])
  end
  subgraph Procedure
    write(Write)--&gt;copy1(Copy)--&gt;copy2(Copy)--&gt;check(Check)
  end
</div>

<p>First off, there is a reduction in the number of lines of code required
to setup this scenario compared to what would have been required if we
used plain data constraints. Secondly, input/output ports on actions express the 
<code class="language-plaintext highlighter-rouge">interface</code> of an action to the outside world. It’s a way for a 
library developer (or, just my colleague who wrote some actions) to 
express where I should focus as an end user of the action.</p>

<h1 id="flow-object-pools-and-binding">Flow-Object Pools and Binding</h1>
<p>There is one final thing to be aware of with PSS flow objects, and that
is the <code class="language-plaintext highlighter-rouge">pool</code> construct. We’ll largely gloss over it until we hit
cases where we really need to use pools. For now, it’s important to 
understand that actions need to be connected to the same pool in 
order to be connected via a flow object like a buffer. You’ll often
see code like what is shown below to create a pool for a flow object
type and connect all action I/O references of that flow-object 
type to the pool.</p>

<figure class="highlight"><pre><code class="language-pss" data-lang="pss"><span class="kd">buffer</span> <span class="nc">mem_b</span> <span class="p">{</span>
    <span class="kd">rand</span> <span class="kt">bit</span><span class="p">[</span><span class="mi">32</span><span class="p">]</span> <span class="k">in</span> <span class="p">[</span><span class="mi">0</span><span class="o">..</span><span class="mh">0xFFFFFF</span><span class="p">]</span> <span class="n">offset</span><span class="p">;</span>
    <span class="kd">rand</span> <span class="kt">bit</span><span class="p">[</span><span class="mi">32</span><span class="p">]</span> <span class="k">in</span> <span class="p">[</span><span class="mi">1</span><span class="o">..</span><span class="mi">256</span><span class="p">]</span>      <span class="n">words</span><span class="p">;</span>
<span class="p">}</span>

<span class="kd">component</span> <span class="nc">memtest_c</span> <span class="p">{</span>
    <span class="k">pool</span> <span class="n">mem_b</span>      <span class="n">mem_b_p</span><span class="p">;</span>
    <span class="k">bind</span> <span class="n">mem_b_p</span> <span class="o">*</span><span class="p">;</span>

    <span class="c1">// ...</span>
<span class="p">}</span></code></pre></figure>

<h1 id="wrapping-up-and-looking-forward">Wrapping up and Looking Forward</h1>
<p>In this post, we looked at the <code class="language-plaintext highlighter-rouge">buffer</code> declarative data-flow construct. 
We’ve seen how this can help to make our actions more reusable and 
better encapsulated. The <code class="language-plaintext highlighter-rouge">buffer</code> construct provides a way to relate
actions via sequential data transfer. As you might guess, PSS also 
provides similar constructs for enabling actions to be related by
data in other ways. We’ll look at those mechanisms in future posts.</p>

<p>For now, feel free to look at the full code listing and run it 
through your favorite PSS processing tool. Try adding new constraints
on sub-actions within the scenario and try further-expanding the scenario.</p>

<p>If you’re interested in reading more about buffers, pools, and other flow
objects, have a look at section 5.1 in the LRM referenced below ([1])</p>

<p>I’m a big proponent of using real-world examples to introduce concepts in a
practical context. In the next post, I’ll introduce an example that we will
use as a vehicle to introduce the next series of PSS modeling topics.</p>

<h2 id="references">References</h2>
<ul>
  <li>[1] <a href="https://www.accellera.org/downloads/standards/portable-stimulus">PSS LRM</a></li>
  <li>[2] <a href="https://bitsbytesgates.com/code_html/2023/03/memtest_buffer.html">MemTest PSS Code (Viewing)</a></li>
  <li>[3] <a href="https://bitsbytesgates.com/code/2023/03/memtest_buffer.pss">MemTest PSS Code (Raw Text)</a></li>
</ul>]]></content><author><name></name></author><category term="PSS" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Declarative Programming and Multi-Core Tests</title><link href="https://bitsbytesgates.com/pss/2023/03/11/DeclarativeMultiCoreTests.html" rel="alternate" type="text/html" title="Declarative Programming and Multi-Core Tests" /><published>2023-03-11T00:00:00+00:00</published><updated>2023-03-11T00:00:00+00:00</updated><id>https://bitsbytesgates.com/pss/2023/03/11/DeclarativeMultiCoreTests</id><content type="html" xml:base="https://bitsbytesgates.com/pss/2023/03/11/DeclarativeMultiCoreTests.html"><![CDATA[<p align="center">
<img src="https://bitsbytesgates.com/imgs/2023/03/PSSMultiCoreTests_splash.png" /> 
</p>

<p>As humans, we often pride ourselves on our ability to multi-task. Not 
only can we participate in a meeting, we can we simultaneously prepare the 
slides for the next meeting. Sadly, science has some bad news about 
our perception to multitask vs our actual ability. In reality, like a 
single-core processor, our mind is rapidly context switching between tasks
to provide an illusion of simultaneous focus on multiple tasks. Re-establishing
sole focus on a task can take 20 minutes and, during that time, productivity 
suffers by as much as 40%. Suffice it to say that we’re really working at 
half speed when constantly multitasking.</p>

<p>What does multi-tasking have to do with PSS and bare-metal test creation? Well,
there is another task that I’ve observed humans consistently find challenging:
parallel and, especially, multi-core programming. I think there’s actually 
a connection between this and the challenges our brain has in multi-tasking. 
With a sequential program, I can reason step-by-step as to what happens. 
I can do the same for some portions of a parallel program. But, whenever 
the parallel threads interact, I need to reason about their possible 
relationships at that point in time. What happens
under each possible ordering of threads reaching the synchronization point? 
It’s this last point that, I think, really stresses our multi-tasking ability.
Not only do we need to envision what is happening in the context of one
thread, but need to simultaneously envision the set of possible actions the
other threads may be taking.</p>

<h2 id="specific-bare-metal-multi-core-test-challenges">Specific Bare-Metal Multi-Core Test Challenges</h2>

<p>Creating bare-metal, multi-core tests poses challenges beyond just the core
challenges of managing parallel behavior described above. Keep in mind that,
because we don’t have an OS to manage the processor cores, our test will 
need to be partitioned into per-core test programs that synchronize with
each other.</p>

<div class="mermaid" align="center">
graph TD;
    A[Core0\nWrite]--&gt;B[Core1\nCopy];
    B--&gt;C[Core0\nCheck];
</div>

<p>The diagram above shows the test flow of a bring-up test that, on the surface,
is quite simple:</p>
<ul>
  <li>Write some memory from Core0</li>
  <li>Read that memory from Core1, and write some data elsewhere (copy)</li>
  <li>Read the memory written by Core1 from Core0 and check that it’s correct</li>
</ul>

<p>From this simple test flow, we will need to create two core-specific tests that
coordinate to achieve the desired activity. The diagram below shows what 
our test core-specific tests need to do:</p>

<div class="mermaid" align="center">
flowchart TB
    c01-. notify .-&gt;c10
    c11-. notify .-&gt;c02
    subgraph Core1
    c1s[["Core1 Start"]]
    c1s--&gt;c10
    c10(["Wait: Core0::Write"])
    c10--&gt;d
    d[Core1\nCopy]
    d--&gt;c11
    c11(["Notify: Copy Complete"])
    c11--&gt;c1e
    c1e[["Core1 End"]]
    end
    subgraph Core0
    c0s[["Core0 Start"]]
    c0s--&gt;a
    a[Core0\nWrite]
    a--&gt;c01
    c01(["Notify: Write Complete"])
    c01--&gt;c02
    c02(["Wait: Core1::Copy Complete"])
    c02--&gt;e
    e[Core0\nCheck]
    e--&gt;c0e
    c0e[["Core0 End"]]
    end
</div>

<ul>
  <li>Core 0
    <ul>
      <li>Wakes up and writes to the specified memory location</li>
      <li>Notifies Core 1 that the write is complete</li>
      <li>Waits for Core 1 to notify that the read/write is complete</li>
      <li>Read the data from the specified location</li>
    </ul>
  </li>
  <li>Core 1
    <ul>
      <li>Wakes up and waits for Core 0 to write to the specified memory location</li>
      <li>Reads the specified location and writes to another</li>
      <li>Notifies Core 0</li>
    </ul>
  </li>
</ul>

<p>In this case, we are working with three operations distributed across two 
cores. As we introduce more behaviors spread across more processor cores,
the individual tests only become more complex.</p>

<h2 id="pss-and-declarative-descriptions">PSS and Declarative Descriptions</h2>

<p>You may have heard PSS described as a <em>declarative language</em> and wondered
what that really meant in practice. PSS being a declarative-first language
is very important in enabling some of the capabilities of PSS. That said,
the definition of ‘declarative language’ is a bit flexible.</p>

<blockquote>
  <p>In computer science, declarative programming is a programming paradigm — a 
style of building the structure and elements of computer programs — that 
expresses the logic of a computation without describing its control flow.</p>
</blockquote>

<p>Put another way:</p>
<blockquote>
  <p>Declarative programming is a non-imperative style of programming in which 
programs describe their desired results without explicitly listing 
commands or steps that must be performed.</p>
</blockquote>

<p>The PSS <code class="language-plaintext highlighter-rouge">modeling layer</code> that we looked at in the last post is heavily 
constraint-based, exposing the declarative programming basis for that portion 
of the language. We also see the declarative
basis in how PSS approaches implementing multi-core test programs by focusing
the test writer on capturing the desired <code class="language-plaintext highlighter-rouge">intent</code> of the test with respect to
parallel execution rather than capturing the implementation of synchronization
across threads.</p>

<p>As we see more of the PSS language and its applications, the implications and
results of PSS having a declarative <code class="language-plaintext highlighter-rouge">modeling layer</code> will become clearer. For 
now, focus on two things:</p>
<ul>
  <li>We need to think in relationships and rules instead of computation steps.</li>
  <li>We can focus much more on what we <em>want</em> to happen in our tests instead of
on <em>how</em> we’re going to get our tests to do that.</li>
</ul>

<p>When it comes to our multi-core test, this means that we are free to focus
on what interesting patterns of memory access we want to form, and leave 
the task of creating self-synchronizing per-core test programs to our
PSS tool.</p>

<h2 id="pss-and-the-multi-core-memory-test">PSS and the Multi-Core Memory Test</h2>

<p>Let’s revisit our multi-core memory test – this time showing the PSS 
approach. First, a warning. We’re pretty early in looking through all the 
constructs supported by the PSS language, so don’t worry if you don’t 
recognize or understand the details of every language construct.</p>

<div class="mermaid" align="center">
graph TD;
    A[Core0\nWrite]--&gt;B[Core1\nCopy];
    B--&gt;C[Core0\nCheck];
</div>

<h3 id="modeling-the-leaf-level-actions">Modeling the Leaf-Level Actions</h3>

<p>First, let’s create Write, Copy, and Check actions. Let’s keep our data
generation and checking simple by using an incrementing data pattern.</p>

<div class="language-pss highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">component</span> <span class="nc">memtest_c</span> <span class="p">{</span>
    <span class="n">addr_handle_t</span>       <span class="n">base_addr</span><span class="p">;</span>

    <span class="kd">action</span> <span class="nc">Write</span> <span class="p">{</span>
        <span class="kd">rand</span> <span class="n">executor_claim_s</span><span class="o">&lt;</span><span class="n">core_s</span><span class="o">&gt;</span> <span class="n">core</span><span class="p">;</span>
        <span class="kd">rand</span> <span class="kt">bit</span><span class="p">[</span><span class="mi">64</span><span class="p">]</span> <span class="k">in</span> <span class="p">[</span><span class="mi">0</span><span class="o">..</span><span class="mh">0xFFFFFF</span><span class="p">]</span> <span class="n">offset</span><span class="p">;</span>
        <span class="kd">rand</span> <span class="kt">bit</span><span class="p">[</span><span class="mi">32</span><span class="p">]</span> <span class="k">in</span> <span class="p">[</span><span class="mi">1</span><span class="o">..</span><span class="mi">256</span><span class="p">]</span>      <span class="n">words</span><span class="p">;</span>

        <span class="k">exec</span> <span class="k">body</span> <span class="p">{</span>
            <span class="k">repeat</span> <span class="p">(</span><span class="n">i</span> <span class="p">:</span> <span class="n">words</span><span class="p">)</span> <span class="p">{</span>
                <span class="n">write32</span><span class="p">(</span>
                    <span class="n">make_handle_from_handle</span><span class="p">(</span><span class="n">comp</span><span class="o">.</span><span class="n">base_addr</span><span class="p">,</span> <span class="mi">4</span><span class="o">*</span><span class="p">(</span><span class="n">offset</span><span class="o">+</span><span class="n">i</span><span class="p">)),</span> 
                    <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">);</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="c1">// ...</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Okay, what do we have going on here? First off, let’s talk about two new
data types: addr_handle_t and executor_claim. PSS defines the addr_handle_t
type to represent an abstract reference to memory on the target system. 
You can think of this as a sort of pointer if you’re a C programmer. Unlike
in C, though, we don’t directly manipulate an addr_handle_t variable.</p>

<p>PSS defines the <em>executor_claim_s</em> type to allow an action to specify 
on which core it runs. The term <em>executor</em> encompasses both processor
cores and elements of the testbench that execute behavior. As a user, we need
to be able to direct actions to run on specific cores using relevant 
characteristics of the cores. The executor <em>claim</em> data structure is
templated with a data type that allows us to specify that relevant data.</p>

<p>Finally, we have two random variables to specify a offset and size for 
the write, and an exec block that specifies the behavior to run
when the action executes. In this case, call the <em>write32</em> PSS 
built-in method to write an incrementing value to a memory location.</p>

<div class="language-pss highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">component</span> <span class="nc">memtest_c</span> <span class="p">{</span>
    <span class="c1">// ...</span>

    <span class="kd">action</span> <span class="nc">Copy</span> <span class="p">{</span>
        <span class="kd">rand</span> <span class="n">executor_claim_s</span><span class="o">&lt;</span><span class="n">core_s</span><span class="o">&gt;</span> <span class="n">core</span><span class="p">;</span>
        <span class="kd">rand</span> <span class="kt">bit</span><span class="p">[</span><span class="mi">64</span><span class="p">]</span> <span class="k">in</span> <span class="p">[</span><span class="mi">0</span><span class="o">..</span><span class="mh">0xFFFFFF</span><span class="p">]</span> <span class="n">src</span><span class="p">;</span>
        <span class="kd">rand</span> <span class="kt">bit</span><span class="p">[</span><span class="mi">64</span><span class="p">]</span> <span class="k">in</span> <span class="p">[</span><span class="mi">0</span><span class="o">..</span><span class="mh">0xFFFFFF</span><span class="p">]</span> <span class="n">dst</span><span class="p">;</span>
        <span class="kd">rand</span> <span class="kt">bit</span><span class="p">[</span><span class="mi">32</span><span class="p">]</span> <span class="k">in</span> <span class="p">[</span><span class="mi">1</span><span class="o">..</span><span class="mi">256</span><span class="p">]</span>      <span class="n">words</span><span class="p">;</span>

        <span class="k">exec</span> <span class="k">body</span> <span class="p">{</span>
            <span class="kt">bit</span><span class="p">[</span><span class="mi">32</span><span class="p">]</span> <span class="n">tmp</span><span class="p">;</span>
            <span class="k">repeat</span> <span class="p">(</span><span class="n">i</span> <span class="p">:</span> <span class="n">words</span><span class="p">)</span> <span class="p">{</span>
                <span class="n">tmp</span> <span class="o">=</span> <span class="n">read32</span><span class="p">(</span>
                    <span class="n">make_handle_from_handle</span><span class="p">(</span><span class="n">comp</span><span class="o">.</span><span class="n">base_addr</span><span class="p">,</span>     
                        <span class="mi">4</span><span class="o">*</span><span class="p">(</span><span class="n">src</span><span class="o">+</span><span class="n">i</span><span class="p">)));</span>
                <span class="n">write32</span><span class="p">(</span>
                    <span class="n">make_handle_from_handle</span><span class="p">(</span><span class="n">comp</span><span class="o">.</span><span class="n">base_addr</span><span class="p">,</span> 
                        <span class="mi">4</span><span class="o">*</span><span class="p">(</span><span class="n">dst</span><span class="o">+</span><span class="n">i</span><span class="p">)),</span> 
                    <span class="n">tmp</span><span class="p">);</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="c1">// ...</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Okay, many familiar things with the Copy action. Here, we’re also selecting
a core to run on (<code class="language-plaintext highlighter-rouge">core</code>) and have a random <code class="language-plaintext highlighter-rouge">src</code> and <code class="language-plaintext highlighter-rouge">dst</code> offset that 
point to different areas in memory.</p>

<p>In this case, our <code class="language-plaintext highlighter-rouge">exec body</code> block reads from the source area and writes
to the destination area.</p>

<div class="language-pss highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">component</span> <span class="nc">memtest_c</span> <span class="p">{</span>

    <span class="c1">// ...</span>

    <span class="kd">action</span> <span class="nc">Check</span> <span class="p">{</span>
        <span class="kd">rand</span> <span class="n">executor_claim_s</span><span class="o">&lt;</span><span class="n">core_s</span><span class="o">&gt;</span> <span class="n">core</span><span class="p">;</span>
        <span class="kd">rand</span> <span class="kt">bit</span><span class="p">[</span><span class="mi">64</span><span class="p">]</span> <span class="k">in</span> <span class="p">[</span><span class="mi">0</span><span class="o">..</span><span class="mh">0xFFFFFF</span><span class="p">]</span> <span class="n">offset</span><span class="p">;</span>
        <span class="kd">rand</span> <span class="kt">bit</span><span class="p">[</span><span class="mi">32</span><span class="p">]</span> <span class="k">in</span> <span class="p">[</span><span class="mi">1</span><span class="o">..</span><span class="mi">256</span><span class="p">]</span>      <span class="n">words</span><span class="p">;</span>

        <span class="k">exec</span> <span class="k">body</span> <span class="p">{</span>
            <span class="kt">bit</span><span class="p">[</span><span class="mi">32</span><span class="p">]</span> <span class="n">tmp</span><span class="p">;</span>
            <span class="k">repeat</span> <span class="p">(</span><span class="n">i</span> <span class="p">:</span> <span class="n">words</span><span class="p">)</span> <span class="p">{</span>
                <span class="n">tmp</span> <span class="o">=</span> <span class="n">read32</span><span class="p">(</span>
                    <span class="n">make_handle_from_handle</span><span class="p">(</span><span class="n">comp</span><span class="o">.</span><span class="n">base_addr</span><span class="p">,</span> 
                        <span class="mi">4</span><span class="o">*</span><span class="p">(</span><span class="n">offset</span><span class="o">+</span><span class="n">i</span><span class="p">)));</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">tmp</span> <span class="o">!=</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
                    <span class="n">error</span><span class="p">(</span><span class="s">"0x%08x: expect %d ; receive %d"</span><span class="p">,</span> 
                        <span class="mi">4</span><span class="o">*</span><span class="p">(</span><span class="n">offset</span><span class="o">+</span><span class="n">i</span><span class="p">),</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">tmp</span><span class="p">);</span>
                <span class="p">}</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="c1">// ...</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Finally, our <em>Check</em> action reads back words from a region of memory and expects
to find an incrementing pattern of data.</p>

<h3 id="a-convenience-action">A Convenience Action</h3>
<p>In order to perform a write, copy, check operation with these three actions, we
need to ensure that some relationships hold. Let’s create a convenience action
where we can place those constraints.</p>

<p>As a side note, PSS provides much richer mechanisms for managing memory and
coordination between actions. But let’s keep things simple for now.</p>

<div class="language-pss highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">component</span> <span class="nc">memtest_c</span> <span class="p">{</span>

    <span class="c1">// ...</span>

    <span class="kd">action</span> <span class="nc">WriteCopyCheck</span> <span class="p">{</span>
        <span class="n">Write</span>             <span class="n">write</span><span class="p">;</span>
        <span class="n">Copy</span>              <span class="n">copy</span><span class="p">;</span>
        <span class="n">Check</span>             <span class="n">check</span><span class="p">;</span>

        <span class="k">activity</span> <span class="p">{</span>
            <span class="n">write</span><span class="p">;</span>
            <span class="n">copy</span><span class="p">;</span>
            <span class="n">check</span><span class="p">;</span>
        <span class="p">}</span>

        <span class="k">constraint</span> <span class="p">{</span>
            <span class="c1">// Copy reads from same location that Write populated</span>
            <span class="n">copy</span><span class="o">.</span><span class="n">src</span> <span class="o">==</span> <span class="n">write</span><span class="o">.</span><span class="n">offset</span><span class="p">;</span> 
            <span class="c1">// Check reads from the same location that Copy populated</span>
            <span class="n">copy</span><span class="o">.</span><span class="n">dst</span> <span class="o">==</span> <span class="n">check</span><span class="o">.</span><span class="n">offset</span><span class="p">;</span>
            <span class="c1">// All actions write the same number of words</span>
            <span class="n">copy</span><span class="o">.</span><span class="n">words</span> <span class="o">==</span> <span class="n">write</span><span class="o">.</span><span class="n">words</span><span class="p">;</span>
            <span class="n">copy</span><span class="o">.</span><span class="n">words</span> <span class="o">==</span> <span class="n">check</span><span class="o">.</span><span class="n">words</span><span class="p">;</span>

            <span class="c1">// Ensure that src/dst regions do not overlap</span>
            <span class="p">(</span><span class="n">copy</span><span class="o">.</span><span class="n">src</span><span class="o">+</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">copy</span><span class="o">.</span><span class="n">words</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">copy</span><span class="o">.</span><span class="n">dst</span><span class="p">)</span> <span class="o">||</span>
            <span class="p">(</span><span class="n">copy</span><span class="o">.</span><span class="n">src</span> <span class="o">&gt;</span> <span class="n">copy</span><span class="o">.</span><span class="n">dst</span><span class="o">+</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">copy</span><span class="o">.</span><span class="n">words</span><span class="p">));</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="c1">// ...</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Our <code class="language-plaintext highlighter-rouge">WriteCopyCheck</code> action provides us a simple and reusable 
write/copy/check operation that we can use and customize without
worrying about the constraints inside.</p>

<h3 id="modeling-the-cores-and-memory">Modeling the Cores and Memory</h3>

<p>Let’s come back to how we describe cores. The leaf-level actions use a data
structure named <code class="language-plaintext highlighter-rouge">core_s</code> to describe information about the processor cores.
We can choose to put any type of data in this data structure to describe
key attributes about the processor cores in our system. For now, let’s 
just give each core a numeric ID.</p>

<div class="language-pss highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">struct</span> <span class="nc">core_s</span> <span class="p">:</span> <span class="n">executor_trait_s</span> <span class="p">{</span>
    <span class="kd">rand</span> <span class="kt">bit</span><span class="p">[</span><span class="mi">8</span><span class="p">]</span>     <span class="n">id</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>PSS defines two built-in component types to represent an individual executor
and a group of executors. Assuming we have 4 cores, let’s define a 
corresponding set of executors and group them.</p>

<div class="language-pss highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">struct</span> <span class="nc">core_s</span> <span class="p">:</span> <span class="n">executor_trait_s</span> <span class="p">{</span>
    <span class="kd">rand</span> <span class="kt">bit</span><span class="p">[</span><span class="mi">8</span><span class="p">]</span>     <span class="n">id</span><span class="p">;</span>
<span class="p">}</span>

<span class="kd">component</span> <span class="nc">pss_top</span> <span class="p">{</span>
    <span class="n">executor_c</span><span class="o">&lt;</span><span class="n">core_s</span><span class="o">&gt;</span>         <span class="n">core</span><span class="p">[</span><span class="mi">4</span><span class="p">];</span>
    <span class="n">executor_group_c</span><span class="o">&lt;</span><span class="n">core_s</span><span class="o">&gt;</span>   <span class="n">cores</span><span class="p">;</span>
    <span class="n">transparent_addr_space_c</span><span class="o">&lt;&gt;</span> <span class="n">aspace</span><span class="p">;</span>
    <span class="n">memtest_c</span>                  <span class="n">memtest</span><span class="p">;</span>

    <span class="k">exec</span> <span class="k">init</span> <span class="p">{</span>
        <span class="k">foreach</span> <span class="p">(</span><span class="n">core</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="p">{</span>
            <span class="n">core</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">trait</span><span class="o">.</span><span class="n">id</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>
            <span class="n">cores</span><span class="o">.</span><span class="n">add_executor</span><span class="p">(</span><span class="n">core</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
        <span class="p">}</span>

        <span class="c1">// Define a memory region</span>
        <span class="n">transparent_addr_region_s</span><span class="o">&lt;&gt;</span> <span class="n">region</span><span class="p">;</span>
        <span class="n">region</span><span class="o">.</span><span class="n">addr</span> <span class="o">=</span> <span class="mh">0x8000_0000</span><span class="p">;</span>
        <span class="n">region</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="mh">0x1000_0000</span><span class="p">;</span>
        <span class="n">memtest</span><span class="o">.</span><span class="n">base_addr</span> <span class="o">=</span> <span class="n">aspace</span><span class="o">.</span><span class="n">add_region</span><span class="p">(</span><span class="n">region</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>In the snippet above, we’ve declared an executor for each core and specified 
its unique <code class="language-plaintext highlighter-rouge">id</code>. Each individual core is added to the <code class="language-plaintext highlighter-rouge">cores</code> group of 
executors. As you might guess, this entire scheme is designed to handle 
much more complex associations of cores and groups of cores.</p>

<p>We also declare an address space that contains a region of memory. The memory
region starts at 0x8000_0000 and is 0x1000_0000 in size. The <code class="language-plaintext highlighter-rouge">add_region</code> 
call shown in the exec block returns an address handle, which we assign
to the <code class="language-plaintext highlighter-rouge">base_addr</code> field in the memtest component. Our Write/Copy/Check 
actions will be able to access the memory region via this handle.</p>

<h3 id="creating-tests">Creating Tests</h3>

<p>Let’s start by writing a test that is identical to the test flowchart that
we’ve been looking at:</p>

<div class="mermaid" align="center">
graph TD;
    A[Core0\nWrite]--&gt;B[Core1\nCopy];
    B--&gt;C[Core0\nCheck];
</div>

<div class="language-pss highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">component</span> <span class="nc">pss_top</span> <span class="p">{</span>
    <span class="c1">// ...</span>

    <span class="kd">action</span> <span class="nc">Copy_0_1_0</span> <span class="p">{</span>
        <span class="k">activity</span> <span class="p">{</span>
            <span class="k">do</span> <span class="nn">memtest_c</span><span class="p">::</span><span class="n">WriteCopyCheck</span> <span class="k">with</span> <span class="p">{</span>
                <span class="n">write</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">trait</span><span class="o">.</span><span class="n">id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">;</span>
                <span class="n">copy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">trait</span><span class="o">.</span><span class="n">id</span> <span class="o">==</span> <span class="mi">1</span><span class="p">;</span>
                <span class="n">check</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">trait</span><span class="o">.</span><span class="n">id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">;</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Simply by adding a few extra constraints (rules) on top of our convenience
Write/Copy/Check action, we can achieve exactly the scenario of writing
data from Core 0, Copying it using Core 1, then checking the result from Core 0.
Notice that, in this case, the size of data being created and copied is random.</p>

<div class="language-pss highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">component</span> <span class="nc">pss_top</span> <span class="p">{</span>
    <span class="c1">// ...</span>

    <span class="kd">action</span> <span class="nc">Copy_same_core</span> <span class="p">{</span>
        <span class="k">activity</span> <span class="p">{</span>
            <span class="k">do</span> <span class="nn">memtest_c</span><span class="p">::</span><span class="n">WriteCopyCheck</span> <span class="k">with</span> <span class="p">{</span>
                <span class="n">write</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">trait</span><span class="o">.</span><span class="n">id</span> <span class="o">==</span> <span class="n">copy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">trait</span><span class="o">.</span><span class="n">id</span><span class="p">;</span>
                <span class="n">copy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">trait</span><span class="o">.</span><span class="n">id</span> <span class="o">==</span> <span class="n">check</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">trait</span><span class="o">.</span><span class="n">id</span><span class="p">;</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>With a slightly different set of rules, we can say that we want the same core
to perform the write, copy, and check. The actual core (0..3) will be 
randomly selected</p>

<div class="language-pss highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">component</span> <span class="nc">pss_top</span> <span class="p">{</span>
    <span class="c1">// ...</span>

    <span class="kd">action</span> <span class="nc">Copy_check_diff_core</span> <span class="p">{</span>
        <span class="k">activity</span> <span class="p">{</span>
            <span class="k">do</span> <span class="nn">memtest_c</span><span class="p">::</span><span class="n">WriteCopyCheck</span> <span class="k">with</span> <span class="p">{</span>
                <span class="n">write</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">trait</span><span class="o">.</span><span class="n">id</span> <span class="o">!=</span> <span class="n">check</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">trait</span><span class="o">.</span><span class="n">id</span><span class="p">;</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Changing the rules again, we can require the core writing data to be different
from the core checking data. The one copying data is left completely random.</p>

<h2 id="conclusion">Conclusion</h2>
<p>While the human mind may not lend itself to efficiently reasoning about concurrency,
automation can go a long way to simplifying the creation of multi-core tests. In this 
post, we’ve seen a few ways in which the declarative modeling approach that PSS defines
helps in focusing the user on capturing <em>what</em> to test, and delegating the task of
making happen – the <em>how</em> – to the PSS test-synthesis tool. This lets us quickly
change the rules of the test to focus in on specific scenarios – how much data 
is being transferred, for example – and the generated test follows. This results in 
a huge boost in test-creation productivity vs manually coding (or copy/paste/modifying)
individual tests.</p>

<p>If you have access to a PSS processing tool, please try out the example code
(link below). You should be able to observe the multi-core synchronization code 
that the PSS tool emits to explicitly schedule and synchronize behavior across
multiple processor cores.</p>

<p>In the next post, we’ll start to learn about the features PSS provides to help 
actions communicate in a reusable and modular fashion.</p>

<h3 id="references">References</h3>
<ul>
  <li><a href="https://www.npr.org/2008/10/02/95256794/think-youre-multitasking-think-again">https://www.npr.org/2008/10/02/95256794/think-youre-multitasking-think-again</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Declarative_programming">https://en.wikipedia.org/wiki/Declarative_programming</a></li>
  <li><a href="https://bitsbytesgates.com/code_html/2023/03/memtest.html">MemTest PSS Code (Viewing)</a></li>
  <li><a href="https://bitsbytesgates.com/code/2023/03/memtest.pss">MemTest PSS Code (Raw Text)</a></li>
</ul>]]></content><author><name></name></author><category term="PSS" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">PSS Fundamentals: Actions, Components, and Test Generation</title><link href="https://bitsbytesgates.com/pss/2023/03/03/ActionsComponents_and_TestGeneration.html" rel="alternate" type="text/html" title="PSS Fundamentals: Actions, Components, and Test Generation" /><published>2023-03-03T00:00:00+00:00</published><updated>2023-03-03T00:00:00+00:00</updated><id>https://bitsbytesgates.com/pss/2023/03/03/ActionsComponents_and_TestGeneration</id><content type="html" xml:base="https://bitsbytesgates.com/pss/2023/03/03/ActionsComponents_and_TestGeneration.html"><![CDATA[<p align="center">
<img src="https://bitsbytesgates.com/imgs/2023/PSSFundamentals_ActionsComponents_splash.png" /> 
</p>

<p>Complex engineering endeavors require complex calculations. It’s open to 
debate as to when the first engineering project that required complex 
calculations occurred. What we do know is that those 
calculations would have been done by hand. And this 
state largely remained until the broad availability of the electronic 
calculator in the 1950s and 1960s, even as engineering projects 
and the technology we used to accomplish them become more ambitious.</p>

<p align="center">
<img src="https://bitsbytesgates.com/imgs/2023/US-Veterans-Bureau-Computing-Division-1924.jpg" height="480" />
<em><figcaption>Computing Division of the US Veterans Bureau, 1924. Image Courtesy of the Computer History Museum.</figcaption></em>
</p>

<p>But this doesn’t mean that “computers” (those individuals performing
computations with pen and ink or simple mechanical machines) were left 
to compute everything from the ground up. Fortunately, books were published 
containing mathematical tables that provided the pre-computed results of 
standard trigonometric 
functions for various input values. The data in these books of 
mathematical tables would, of course, have been produced
laboriously by some other “computer” or “computers” working with pen 
and paper. But, they were invaluable at increasing the speed with 
which complex calculations could be completed by hand.</p>

<p align="center">
<img src="https://bitsbytesgates.com/imgs/2023/LogTrigTablesBook.jpg" height="320" />
<em><figcaption>Logarithmic and Trigonometric Tables Book. Image courtesy of the Smithsonian.</figcaption></em>
</p>

<p>What does this have to do with Portable Test and Stimulus (PSS)? 
PSS is specifically designed to enable PSS processing tools to 
pre-compute the result of complex test scenario relationships in order
to make the best use of instructions running at a few Hertz on a simulated 
RTL model of the design processor. But, before we get to how PSS creates tests, 
we need to learn about two fundamental PSS concepts: Actions and Components</p>

<h2 id="actions-and-layered-test-modeling">Actions and Layered Test Modeling</h2>
<p>Our next key topic is the Action. As it so happens, several languages
and description formats use the notion of <em>Action</em>. After all, it 
is a very intuitive term to capture an element that is all about
describing and encapsulating behavior. Before digging into actions,
it is helpful to understand how PSS divides a description of test 
behavior into two portions.</p>

<p align="center">
<img src="https://bitsbytesgates.com/imgs/2023/ModelingRealizationLayers.png" />
<em><figcaption>PSS Modeling and Realization Layers.</figcaption></em>
</p>

<p>The upper portion, called the Modeling Layer, contains the constraintt-driven 
features for modeling test scenarios, and is responsible for capturing the 
space of interesting and useful behaviors.</p>

<p>In contrast, the realization layer is more-or-less 
intended to carry out the instructions of the modeling layer, and 
contains familiar procedural statements such as appear in other 
programming languages like C, Java, and Python. The realization layer
is also where we’ll find constructs for reading and writing registers
and memory. While the realization layer makes local decisions, often
related to handshaking with the device it controls, the modeling layer
is intended to make the big-picture decisions about how the system
is exercised.</p>

<h3 id="connecting-modeling-and-realization-layers">Connecting Modeling and Realization Layers</h3>
<p>As the diagram suggests, <em>Actions</em> bridge the boundary between modeling and 
realization layer because they can contain both modeling and realization 
aspects. In all cases, actions group the data, constraints, and implementation 
for a given behavior.</p>

<div class="language-pss highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">action</span> <span class="nc">check_reg_reset_vals</span> <span class="p">{</span>
    <span class="kt">list</span><span class="o">&lt;</span><span class="kt">bit</span><span class="p">[</span><span class="mi">32</span><span class="p">]</span><span class="o">&gt;</span> <span class="n">reset_vals</span> <span class="o">=</span> <span class="p">{</span><span class="mh">0x0000_0000</span><span class="p">,</span> <span class="mh">0x0180_2FFF</span><span class="p">,</span> <span class="mh">0x8000_0000</span><span class="p">};</span>
    <span class="kd">rand</span> <span class="kt">bit</span><span class="p">[</span><span class="mi">8</span><span class="p">]</span> <span class="n">start</span><span class="p">;</span>

    <span class="k">constraint</span> <span class="n">start</span> <span class="k">in</span> <span class="p">[</span><span class="mi">0</span><span class="o">..</span><span class="n">reset_vals</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="o">-</span><span class="mi">1</span><span class="p">];</span>

    <span class="k">exec</span> <span class="k">body</span> <span class="p">{</span>
        <span class="kt">bit</span><span class="p">[</span><span class="mi">32</span><span class="p">]</span> <span class="n">off</span><span class="p">,</span> <span class="n">addr</span><span class="p">,</span> <span class="n">val</span><span class="p">;</span>
        <span class="k">repeat</span> <span class="p">(</span><span class="n">i</span> <span class="p">:</span> <span class="n">reset_vals</span><span class="o">.</span><span class="n">size</span><span class="p">())</span> <span class="p">{</span>
            <span class="n">off</span> <span class="o">=</span> <span class="p">((</span><span class="n">start</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span><span class="o">%</span><span class="n">reset_vals</span><span class="o">.</span><span class="n">size</span><span class="p">());</span>
            <span class="n">addr</span> <span class="o">=</span> <span class="mh">0x1000_0000</span> <span class="o">+</span> <span class="mi">4</span><span class="o">*</span><span class="n">off</span><span class="p">;</span>
            <span class="n">val</span> <span class="o">=</span> <span class="n">read32</span><span class="p">(</span><span class="n">addr</span><span class="p">);</span>

            <span class="k">if</span> <span class="p">(</span><span class="n">val</span> <span class="o">!=</span> <span class="n">reset_vals</span><span class="p">[</span><span class="n">off</span><span class="p">])</span> <span class="p">{</span>
                <span class="n">error</span><span class="p">(</span><span class="s">"Failed to read register at 0x%08x: read 0x%08x ; expected %08x"</span><span class="p">,</span>
                    <span class="n">addr</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">reset_vals</span><span class="p">[</span><span class="n">off</span><span class="p">]);</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The example above is an action that checks the reset value of registers in the
design. It contains a list of register expected values, and a random variable
that will specify the order in which the action checks the reset values – 
just to be sure that the order in which we access registers doesn’t change
behaviors. This is our modeling layer.</p>

<p>The realization layer of the example is contained in the <code class="language-plaintext highlighter-rouge">body</code> block. This
is the code that carries out the ‘higher-level’ decisions made by the 
modeling layer. In this case, we:</p>
<ul>
  <li>Loop over the set of registers we need to check</li>
  <li>Compute the offset and address of the selected register which are 
relative to a random starting point.</li>
  <li>Read it, check against the expected value, and report any mismatches.</li>
</ul>

<p>Even this tiny action begins to show some of the value of modeling scenarios
with PSS. Our code is similar to what we might write in C and, if anything, is
a bit more compact. Let’s come back to this example to see how PSS can help
us with generating test variants.</p>

<p>One other thing that we’ll come back to are activities. The action above is
called an <code class="language-plaintext highlighter-rouge">atomic</code> action. An atomic action is a leaf-level action that
is implemented in terms of procedural code. We can also implement the 
behavior of an action in terms of other actions. This so-called <code class="language-plaintext highlighter-rouge">activity</code>
provides us an expanded set of modeling features.</p>

<h2 id="components">Components</h2>
<p><code class="language-plaintext highlighter-rouge">Components</code> are the other key PSS fundamentals construct for this post. The 
requirement for <code class="language-plaintext highlighter-rouge">components</code> stems from the observation that actions in
a system <code class="language-plaintext highlighter-rouge">act on</code> a specific context. A DMA transfer action must <code class="language-plaintext highlighter-rouge">act on</code> 
a <em>specific</em> DMA controller, because there are likely to
be several in a system. A DMA transfer action needs to know things like
what the base address is for the DMA controller registers. The PSS <code class="language-plaintext highlighter-rouge">Component</code> 
construct fills this requirement for a persistent, static entity to model
physical entities, the resources they contain, and the operations that 
can be performed on them.</p>

<div class="language-pss highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">component</span> <span class="nc">dma_c</span> <span class="p">{</span>
    <span class="kd">action</span> <span class="nc">mem2mem_a</span> <span class="p">{</span>
        <span class="c1">// ...</span>
    <span class="p">}</span>

    <span class="c1">// ...</span>
<span class="p">}</span>

<span class="kd">component</span> <span class="nc">subsys_c</span> <span class="p">{</span>
    <span class="n">dma_c</span>           <span class="n">dma0</span><span class="p">;</span>
    <span class="n">dma_c</span>           <span class="n">dma1</span><span class="p">;</span>

    <span class="c1">// ...</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Components may be composed hierarchically, much as designs are. So, if your
subsystem design contains two instances of a DMA controller IP, your
PSS component that represents the subsystem from a test perspective will
as well.</p>

<p>Components is another topic that we will revisit in greater depth in a 
future post. For now, their fundamental attributes are:</p>
<ul>
  <li>The component tree remains constant for the lifetime of the test</li>
  <li>A component type groups the supported behaviors and resources required by 
those behaviors</li>
  <li>Each action execution is associated with a corresponding instance of
a component</li>
</ul>

<h2 id="test-creation-flow">Test Creation Flow</h2>
<p>PSS breaks the execution of a PSS model into two large phases:</p>
<ul>
  <li>Solve  – Constraints are solved, random variable values are assigned, etc</li>
  <li>Target – Behavior executes on the target platform</li>
</ul>

<p>The goal is to enable separating or combining these phases depending 
on the characteristics of the environment.</p>

<p align="center">
<img src="https://bitsbytesgates.com/imgs/2023/PSSTestGenFlow.png" />
<em><figcaption>PSS Pre-Gen Test Generation Flow.</figcaption></em>
</p>

<p>The figure above shows a typical <em>pre-generation</em> flow targeted at 
producing bare-metal test software for SoC integration verification.
In this case, simplifying computations performed on the target 
platform is a key goal, since the simulated RTL processor model runs
at a very low effective clock speed.</p>

<p>Looking back at our ‘register reset test’ action, the implementation
code might look like the following:</p>

<figure class="highlight"><pre><code class="language-c" data-lang="c"><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="kt">uint32_t</span> <span class="n">val</span><span class="p">;</span>
  <span class="kt">uint32_t</span> <span class="n">ret</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

  <span class="n">val</span> <span class="o">=</span> <span class="o">*</span><span class="p">((</span><span class="k">volatile</span> <span class="kt">uint32_t</span> <span class="o">*</span><span class="p">)</span><span class="mh">0x10000004</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">val</span> <span class="o">!=</span> <span class="mh">0x01802FFF</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">error</span><span class="p">(</span><span class="s">"Failed to read register at 0x%08x: read 0x%08x ; expected %08x"</span><span class="p">,</span>
         <span class="mh">0x10000004</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="mh">0x01802FFF</span><span class="p">);</span>
    <span class="n">ret</span> <span class="o">|=</span> <span class="mi">1</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="n">val</span> <span class="o">=</span> <span class="o">*</span><span class="p">((</span><span class="k">volatile</span> <span class="kt">uint32_t</span> <span class="o">*</span><span class="p">)</span><span class="mh">0x10000008</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">val</span> <span class="o">!=</span> <span class="mh">0x80000000</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">error</span><span class="p">(</span><span class="s">"Failed to read register at 0x%08x: read 0x%08x ; expected %08x"</span><span class="p">,</span>
        <span class="mh">0x10000008</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="mh">0x80000000</span><span class="p">);</span>
    <span class="n">ret</span> <span class="o">|=</span> <span class="mi">1</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="n">val</span> <span class="o">=</span> <span class="o">*</span><span class="p">((</span><span class="k">volatile</span> <span class="kt">uint32_t</span> <span class="o">*</span><span class="p">)</span><span class="mh">0x10000000</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">val</span> <span class="o">!=</span> <span class="mh">0x00000000</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">error</span><span class="p">(</span><span class="s">"Failed to read register at 0x%08x: read 0x%08x ; expected %08x"</span><span class="p">,</span>
        <span class="mh">0x10000000</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="mh">0x00000000</span><span class="p">);</span>
    <span class="n">ret</span> <span class="o">|=</span> <span class="mi">1</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">ret</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure>

<p>Note that, in this case, the PSS processing tool has pre-computed the random 
starting index, unrolled the loop, and pre-computed addresses in order to minimize 
instructions executed on the target platform. It could also have locally-computed
a random value between 0 and 2 for the starting index. Due to the abstraction
level at which the test behavior is defined, the PSS processing tool has many
implementations options that can be traded off against the requirements of the
implementation platform.</p>

<p>Much as mathematical tables helped human
computers to maximize the results they were able to produce by hand, the PSS
semantics that enable pre-computation of results help PSS-created tests maximize
test throughput on simulated hardware platforms.</p>

<h2 id="looking-forward">Looking Forward</h2>
<p>In this post, we have learned about two key PSS constructs: <em>actions</em> and <em>components</em>. 
<em>Actions</em> describe model-level behavior and connect that high-level behavior to 
test realization implementation via exec blocks. <em>Components</em> describe structure,
and group behaviors (actions) with the resources they require. From now on, every
example will be built from <em>Actions</em> and <em>Components</em>, and we will add new ways
that PSS enables actions and components to interact.</p>

<p>In the next post, we will begin to look in more detail at the declarative basis
of the PSS language. Being, first and foremost, declarative makes PSS a bit 
different as languages go, but also enables many of its most impressive capabilities.</p>]]></content><author><name></name></author><category term="PSS" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Automating Bare-Metal Tests with PSS</title><link href="https://bitsbytesgates.com/pss/2023/02/25/AutomatingBareMetalTestsWithPSS.html" rel="alternate" type="text/html" title="Automating Bare-Metal Tests with PSS" /><published>2023-02-25T00:00:00+00:00</published><updated>2023-02-25T00:00:00+00:00</updated><id>https://bitsbytesgates.com/pss/2023/02/25/AutomatingBareMetalTestsWithPSS</id><content type="html" xml:base="https://bitsbytesgates.com/pss/2023/02/25/AutomatingBareMetalTestsWithPSS.html"><![CDATA[<p align="center">
<img src="https://bitsbytesgates.com/imgs/2023/AutomatingBareMetalSoCTestsWithPSS_splash.png" /> 
</p>
<p>As a technologist, it’s tempting to focus on what is new 
(at least, new to me) – especially when choosing what to write about.
I’m periodically reminded that there is immense value in returning
to topics. Returning to a topic might raise awareness with a different
set of readers, but it’s also highly likely that I’ll learn something
new about the topic, or that my perspective on the topic will evolve 
in the process.</p>

<p>This post marks the beginning of just such a “back to basics” series
that focuses on the <a href="https://www.accellera.org/downloads/standards/portable-stimulus">Accellera Portable Test and Stimulus</a> 
(PSS) language. PSS is a specification language for modeling behavior 
to test. It specifically focuses on providing features that help 
in creating bare-metal software-driven system-level tests. Consequently,
this ‘Intro to PSS’ series will focus on the task of the bare-metal
software-driven test writer, and the value PSS provides.</p>

<h2 id="programming-languages-categories">Programming Languages Categories</h2>
<p>When it comes to programming languages, there are two big 
categories: general-purpose programming languages and domain-specific
programming languages. General-purpose programming languages such
as C/C++, Java, Python, and Rust are designed to be able to 
implement any algorithm or behavior. The “any…” part comes 
with a caveat, of course: any algorithm or behavior given sufficient
expertise and time.</p>

<p>There are classes of problems that have such
well-defined expert solutions that it’s considered wasteful
to actually apply large amounts of expert-programmer 
resource to create a bespoke solution each time they arise. This is where 
domain-specific languages (DSL) come into play. A domain-specific language 
provides a way to capture a problem in a way that a domain expert finds 
familiar, along with enough information to enable a synthesis tool to 
produce an optimal implementation in a general-purpose language.</p>

<h2 id="cases-where-dsls-shine">Cases where DSLs shine</h2>
<p>As you may have guessed, PSS is a domain-specific language. In this case,
one targeted at capturing system-level test behavior in terms 
familiar to a domain expert.
Let’s take a look at another very popular application for domain-specific
languages to better understand the value and tradeoffs that they provide.</p>

<p>An excellent example of an application of domain-specific languages 
is building lexical analyzers (lexers) and parsers.  People that 
define languages think in terms of language grammars – typically
captured in <a href="https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_form">Backus-Naur Form</a> 
(BNF). For example, here is a snippet of BNF grammar from the PSS 
language-reference manual.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>action_body_item ::=
 activity_declaration
 | override_declaration
 | constraint_declaration
 | action_field_declaration
</code></pre></div></div>

<p>Coding a parser by hand from this description involves a fair amount of 
analysis to, for example, identify the keywords that would cause a parser
to proceed down one branch vs another, and how many tokens of
‘lookahead’ are needed in each case to disambiguate choices. While these
are critical implementation decisions, they’re difficult for a human to
make by hand since they often involve ‘global’ thinking about the whole
of a sizable language grammar. In other words, not areas of thinking that
to which the untrained human mind lends itself.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>action_body_item:
 activity_declaration
 | override_declaration
 | constraint_declaration
 | action_field_declaration
 ;
</code></pre></div></div>

<p>Re-expressing the BNF in the terms of a domain-specific language is an
almost-trivial exercise for a domain expert in language design. The 
code snippet above is in <a href="https://www.antlr.org/">ANTLR4</a> format. There 
are a few small changes in format, and a few things that are conveyed 
typographically in a printed language grammar are conveyed differently
to support programmatic processing, but overall the DSL description
is easy to learn for a domain expert.</p>

<p>By capturing our language grammar in a domain-specific language format,
we’re able to make use of a parser/lexer builder tool to derive an 
efficient implementation of a language parser for this language. The 
parser-builder tool is able to easily and quickly make high-quality
global optimizations that would have been very time consuming and 
error-prone for a human to make. And, if we change the grammar at 
some point, we only need to re-run the the parser-builder tool to derive
a new (and still optimal) parser implementation.</p>

<p>Looking at a few domain-specific languages, they tend to shine when:</p>
<ul>
  <li>There exists a natural (innate or acquired) way for a domain expert to 
capture a domain problem.</li>
  <li>Deterministic, automated methods exist to derive an optimized 
implementation from the domain-specific description</li>
  <li>There is a significant difference between the best way to describe a 
problem and the best way to implement it in a general-purpose 
programming language</li>
  <li>Achieving a good implementation requires global and/or concurrent 
optimization.</li>
</ul>

<h2 id="where-does-bare-metal-testing-fit">Where does Bare-Metal Testing fit?</h2>
<p>With that in mind, let’s look back at our target application for PSS:
creation of bare-metal software-driven tests. In a typical 
system-development flow, development and verification of hardware and
software proceed on different paths up to a point. Of course, there 
can be some cross-over in the process, but the time when software
really starts to run on the hardware for which it is intended is
once the hardware system is assembled and verified.</p>

<p align="center">
<img src="https://bitsbytesgates.com/imgs/2023/Hw_Sw_VDiagram_640.png" /> 
</p>

<p>Once a stable hardware-like representation of the hardware system 
exists, the integration team can really get started on completing
software for the system. Note that I said “hardware-like”. In many
cases, this representation of the hardware system will be a hardware
emulator, or an FPGA prototype. The key is that the representation
has sufficient stability and performance to support 
software-development efforts.</p>

<p>Two key factors to a successful hardware/software integration process are 
maximizing the stability of the hardware platform and having an efficient
path to reproduce and produce minimized testcases for any bugs found
by the integration team. The quality, quantity, and flexibility of the
bare-metal tests used to verify the system-level hardware platform has
a huge impact on success here.</p>

<h2 id="what-are-bare-metal-tests">What are Bare-Metal Tests?</h2>
<p>In order to appreciate some of the key benefits that PSS has to offer 
in creating bare-metal software-driven tests, it’s useful to understand
a bit more about about the characteristics of bare-metal software tests.</p>

<p>As their name suggests, bare-metal software tests run directly on the
processor cores of a design. Unlike production software, they don’t 
run on top of an operating system or real-time operating system (RTOS),
and consequently don’t have access to services that operating systems
provide, such as:</p>
<ul>
  <li>Dynamic memory allocation</li>
  <li>Threads and processes</li>
  <li>Multi-processor scheduling infrastructure</li>
  <li>Device-driver infrastructure</li>
</ul>

<p>There are very good reasons that bare-metal tests opt to run directly
on the hardware and forego the use of an operating system.</p>

<p>Early software-driven integration testing is performed using a hardware
simulator to provide maximum debug visibility. Simulating a full system
at RTL is a slow process. Even simple operating systems run a 
not-insignificant amount of code prior to execution of the application,
all of which is running very very slowly on a simulated RTL model of 
a processor. Running bare-metal tests enables running more actual test
code.</p>

<p>The code that an operating system runs on start-up assumes that a the
hardware platform is stable. For example, for proper operation, it 
will require that memory accesses to different regions, and very possibly
atomic operations, are working. It will require that exceptions and 
interrupts are stable. Instability in the hardware platform is likely
to manifest itself in kernel panic, “blue screen of death”, or other 
generic error signal that is unlikely to point the developer to the
root cause with any accuracy. We can create much more focused tests
for ensuring stability in various aspects of the platform that can
produce much more accurate failure signatures, allowing the developer
to zero in on the root cause much more quickly.</p>

<p>Finally, writing bare-metal tests gives us much more fine-grained
control over the hardware. The goal of an OS is to produce an 
optimal balance between overall throughput, scheduling fairness,
and other factors such as power consumption. The bare-metal testers
goal is often to hit corner cases that such a ‘balanced’ approach
to running code doesn’t lend itself to.</p>

<h2 id="so-how-does-pss-help">So, how does PSS Help?</h2>
<p>So, we have good reasons for writing our early software-driven 
tests as bare-metal software. But this doesn’t really make them
any easier to write. Over the next few posts we will explore how
the PSS domain-specific language helps to boost test-writing
productivity, while still enabling us to derive lean and mean
bare-metal software-driven tests. We will see how PSS processing
tools bridge the abstraction gap between a PSS-level description
and good bare-metal implementation code. We will see how a PSS
description enables PSS processing tools to make good global
optimizations that are difficult and error-prone for humans. It
remains to be seen whether PSS is considered to provide a natural 
(innate or acquired) way to capture system-level tests. I hope 
you’ll have the information to assess this for yourself by the
end of this series.</p>]]></content><author><name></name></author><category term="PSS" /><summary type="html"><![CDATA[As a technologist, it’s tempting to focus on what is new (at least, new to me) – especially when choosing what to write about. I’m periodically reminded that there is immense value in returning to topics. Returning to a topic might raise awareness with a different set of readers, but it’s also highly likely that I’ll learn something new about the topic, or that my perspective on the topic will evolve in the process.]]></summary></entry><entry><title type="html">New Year, New Space</title><link href="https://bitsbytesgates.com/intro/2023/02/16/NewYearNewSpace.html" rel="alternate" type="text/html" title="New Year, New Space" /><published>2023-02-16T00:00:00+00:00</published><updated>2023-02-16T00:00:00+00:00</updated><id>https://bitsbytesgates.com/intro/2023/02/16/NewYearNewSpace</id><content type="html" xml:base="https://bitsbytesgates.com/intro/2023/02/16/NewYearNewSpace.html"><![CDATA[<p>We’ve more than gotten started on the new year. In fact, DVCon – reliably
and predictably held during the last few days of February and initial 
few of March is right around the corner. And, here I am just getting the
first post of the year out.</p>

<p>Getting a late start on blog posts this year isn’t for lack of interesting
ideas and projects to, though. Here are a few things you can look forward 
to in the coming Blog year.</p>

<h1 id="new-space">New Space</h1>
<p>First, about the new space… For many years, I hosted the bitsbytesgates 
blog on the blogspot.com platform.
It was (relatively) easy to type up posts, and simple was good. That said,
the more technical content I put out (code snippets and such), the more I 
started to hit the usability edges of the blogspot environment. So, like
so many of my peers, I decided it was time to take the plunge and move 
to a new platform, and that this was the year to do it.</p>

<p>Going forward, you can find the bitsbytesgates blog at <a href="https://bitsbytesgates.com">https://bitsbytesgates.com</a>.</p>

<h1 id="python-functional-verification">Python Functional Verification</h1>
<p>Using Python for functional verification continues to be an interest of
mine. Declarative descriptions have also been an interest of mine for some time. 
This year, I want to look more deeply at how some of those declarative
approaches to capturing aspects of verification environments can be deployed
in Python to make Python-based functional verification even more productive.</p>

<h1 id="constrained-random-generation-and-functional-coverage">Constrained-Random Generation and Functional Coverage</h1>
<p>If you’ve been following the blog for a while, you’re aware of some of the
Python-based projects that bring constrained-random generation and 
functional coverage capture and manipulation into Python. Using these
capabilities in Python continues to be key, but I’m looking at making 
some of these capabilities available to projects not implemented in 
Python.</p>

<h1 id="portable-test-and-stimulus-pss">Portable Test and Stimulus (PSS)</h1>
<p>I’ve been involved in the Accellera standards committee for 
Portable Test and Stimulus (PSS) since its inception, but haven’t written
much about it here … until this year. I’m very optimistic about the 
opportunity PSS has to substantially improve the way that we approach
system-level tests – and, especially, the creation of bare-metal software
test content. Look for more about PSS, starting with a ground-up tour, in
the coming year.</p>

<h1 id="conclusion">Conclusion</h1>
<p>With that, welcome to the new blog space, and to a new year of posts. And,
if you happen to be attending DVCon 2023 in San Jose, I hope we get a
chance to interact live and in person!</p>]]></content><author><name></name></author><category term="Intro" /><summary type="html"><![CDATA[We’ve more than gotten started on the new year. In fact, DVCon – reliably and predictably held during the last few days of February and initial few of March is right around the corner. And, here I am just getting the first post of the year out.]]></summary></entry><entry><title type="html">Simplifying Custom Template-Generated Content</title><link href="https://bitsbytesgates.com/2022/08/21/simplifying-custom-template-generated.html" rel="alternate" type="text/html" title="Simplifying Custom Template-Generated Content" /><published>2022-08-21T17:18:00+00:00</published><updated>2022-08-21T17:18:00+00:00</updated><id>https://bitsbytesgates.com/2022/08/21/simplifying-custom-template-generated</id><content type="html" xml:base="https://bitsbytesgates.com/2022/08/21/simplifying-custom-template-generated.html"><![CDATA[<p style="text-align: center;">&nbsp;</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjW6ygOoIw0lvUu7rGQcU-QVVwfR0SFtT6zCJtRzOilSOqHR8OwMnyDjG3C32-arWwoRRP0mKzGieAzgnyEPWk5fFsnzWNezMqZgDFNOUQy9MCRdodg9cWa3ghlBgKJgxJmXLOAXXCkMJI7tdm-fvXLQZIVvYFCXSXc7yHXz5NjDUt3SRbvhit25z52PQ/s540/splash.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="300" data-original-width="540" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjW6ygOoIw0lvUu7rGQcU-QVVwfR0SFtT6zCJtRzOilSOqHR8OwMnyDjG3C32-arWwoRRP0mKzGieAzgnyEPWk5fFsnzWNezMqZgDFNOUQy9MCRdodg9cWa3ghlBgKJgxJmXLOAXXCkMJI7tdm-fvXLQZIVvYFCXSXc7yHXz5NjDUt3SRbvhit25z52PQ/s16000/splash.png" /></a></div><br /><p></p><p>As a verification engineer, it's quite common to work with data and code that follow a regular pattern. Having an efficient way to create this repetitive code is a significant productivity boost. While there certainly are places in the code where 'your critical generation or checking algorithm' goes, much of the structure of an agent, a test environment, etc remain the same. The same goes for other parts of the flow, such as project meta-data, test lists, etc. There are two things that keep us from just making copies of a set of 'golden' files to create the basis for a new UVM agent, project, etc: some or all of the files need to have some data substituted or changed. For example, we want to substitute the name of the new UVM agent we're creating into most of the new SystemVerilog source code.</p><p>Custom code generators have been developed for some of these tasks. These often focus on providing a domain-specific way to capture input data, such as the structure of a UVM testbench or the layout of registers in a design. But there are many more opportunities to generate template-driven code that cannot justify the investment to create a focused solution.</p><p>A few years ago, I created the <a href="https://github.com/fvutils/vte/">Verification Template Engine (VTE)</a> to serve my needs for generating template-driven content. I developed VTE with three user-experience requirements in mind:</p><p></p><ul style="text-align: left;"><li>Creating a new template should be very easy, but have access to powerful generation features</li><li>Managing the available templates should be simple for a user.&nbsp;</li><li>The core tools should be generic, and make few or no assumptions about what is being generated</li></ul><div>VTE focuses on organizing and discovering template content, but leverages the <a href="https://palletsprojects.com/p/jinja/">Jinja2 template engine</a> to do the heavy lifting of template expansion. In some sense, you can think of VTE as providing a user interface to the Jinaj2 library.</div><div><br /></div><div>I've been using VTE since developing it, but am just getting back to create proper documentation, which you can find here: <a href="https://fvutils.github.io/vte/">https://fvutils.github.io/vte/</a>. As part of that work, I created a quickstart guide which is both in the documentation, and forms the remainder of this post.&nbsp;</div><div><br /></div><p></p><div><div><b>Installing VTE</b></div><div>The easiest way to install VTE is from PyPi.</div><div><br /></div><div><span style="font-family: courier;">% python3 -m pip install --user vte</span></div><div>Test that you can run VTE by running the command (vte) and/or invoking the module:</div><div><br /></div><div><span style="font-family: courier;">% vte --help</span></div><div><span style="font-family: courier;">% python3 -m vte --help</span></div><div><br /></div><div><b>Creating a Template</b></div><div>VTE discovers templates by searching directories on the VTE_TEMPLATE_PATH environment variable. VTE uses a marker file named .vte to identify the root of a template. All files and directories in and below a template directory are considered to be part of the template. The template identifier is composed from the directory names between the directory listed in VTE_TEMPLATE_PATH and the directory containing the .vte marker file.</div><div><br /></div><div>Let’s look at an example to illustrate the rules.</div><div><br /></div><div><span style="font-family: courier;">templates</span></div><div><span style="font-family: courier;">&nbsp; uvm</span></div><div><span style="font-family: courier;">&nbsp; &nbsp; agent</span></div><div><span style="font-family: courier;">&nbsp; &nbsp; &nbsp; .vte</span></div><div><span style="font-family: courier;">&nbsp; &nbsp; component</span></div><div><span style="font-family: courier;">&nbsp; &nbsp; &nbsp; .vte</span></div><div><span style="font-family: courier;">&nbsp; doc</span></div><div><span style="font-family: courier;">&nbsp; &nbsp; blog_post</span></div><div><span style="font-family: courier;">&nbsp; &nbsp; &nbsp; .vte</span></div><div><span style="font-family: courier;">&nbsp; &nbsp; readme</span></div><div><span style="font-family: courier;">&nbsp; &nbsp; &nbsp; .vte</span></div><div><br /></div><div>Let’s assume we add the templates directory to VTE_TEMPLATE_PATH. VTE will find four templates:</div><div><br /></div><div><span style="font-family: courier;">uvm.agent</span></div><div><span style="font-family: courier;">uvm.component</span></div><div><span style="font-family: courier;">doc.blog_post</span></div><div><span style="font-family: courier;">doc.readme</span></div><div><br /></div><div>All files in and below the directory containing the .vte marker will be rendered when the template is used.</div><div><br /></div><div><b>Creating the Template Structure</b></div><div>Let’s create a very simple template structure. Create the following directory structure:</div><div><br /></div><div><span style="font-family: courier;">templates</span></div><div><span style="font-family: courier;">&nbsp; doc</span></div><div><span style="font-family: courier;">&nbsp; &nbsp; readme</span></div><div><br /></div><div>Change directory to templates/doc/readme and run the quickstart command:</div><div><br /></div><div><span style="font-family: courier;">% vte quickstart</span></div><div><span style="font-family: courier;">Verification Template Engine Quickstart</span></div><div><span style="font-family: courier;">Template directory: templates/doc/readme</span></div><div><span style="font-family: courier;">Template Description []? Create a simple README</span></div><div><br /></div><div>This command will prompt for a description to use for the template. Enter a description and press ENTER. This will create the .vte marker file.</div><div><br /></div><div>View the .vte file. You’ll see that the initial version is quite simple. For now, this is all we need.</div><div><br /></div><div><span style="font-family: courier;">template:</span></div><div><span style="font-family: courier;">&nbsp; description: Create a simple README</span></div><div><span style="font-family: courier;">&nbsp; parameters: []</span></div><div><span style="font-family: courier;">#&nbsp; &nbsp;- name: param_name</span></div><div><span style="font-family: courier;">#&nbsp; &nbsp; &nbsp;description: param_desc</span></div><div><span style="font-family: courier;">#&nbsp; &nbsp; &nbsp;default: param_default</span></div><div><br /></div><div><b>Creating the Template File</b></div><div>Now, let’s create the template file that will be processed when we render the template. Our readme template only has one file: README.md.</div><div><br /></div><div>Create a file named README.md containing the following content in the templates/doc/readme directory:</div><div><br /></div><div><span style="font-family: courier;"># README for {{name}}</span></div><div><span style="font-family: courier;">TODO: put in some content of interest</span></div><div><br /></div><div>VTE supports defining and using multiple parameters, but defines one built-in parameter that must be supplied for all templates: name. Our template file references name using Jinja2 syntax for variable references.</div><div><br /></div><div>We have now created a simple template for creating README.md files.</div><div><br /></div><div><b>Rendering a Template</b></div><div>In order to render templates, VTE must first be able to discover them. Add the templates directory to the VTE_TEMPLATE_PATH environment variable.</div><div><br /></div><div><span style="font-family: courier;">% export VTE_TEMPLATE_PATH=&lt;path&gt;/templates # Bourne shell</span></div><div><span style="font-family: courier;">% setenv VTE_TEMPLATE_PATH &lt;path&gt;/templates # csh/tsh</span></div><div>Let’s test this out by running the vte list command:</div><div><br /></div><div><span style="font-family: courier;">% vte list</span></div><div><span style="font-family: courier;">doc.readme - Create a simple README</span></div><div><br /></div><div>If you see the doc.readme line above, VTE has successfully discovered the template.</div><div><br /></div><div>Now, let’s actually generate something. Let’s create a new directory parallel to the templates directory in which to try this out</div><div><br /></div><div><span style="font-family: courier;">% mkdir scratch</span></div><div><span style="font-family: courier;">% cd scratch</span></div><div><br /></div><div>Finally, let’s run the generate command:</div><div><br /></div><div><span style="font-family: courier;">% vte generate doc.readme my_project</span></div><div><span style="font-family: courier;">Note: processing template README.md</span></div><div><br /></div><div>VTE prints a line for each template file is processes. The output above confirms that is processed the template README.md file.</div><div><br /></div><div>Let’s have a look at the result. View the README.md file in the scratch directory.</div><div><br /></div><div><span style="font-family: courier;"># README for my_project</span></div><div><span style="font-family: courier;">TODO: put in some content of interest</span></div><div><br /></div><div>Node that the {{name}} reference was replaced by the name (my_project) that we specified.</div><div><br /></div><div>You have now created your first VTE template!</div><div><br /></div></div><p><b>Conclusion</b></p><p>As the tutorial above illustrates, creating a new template for use with VTE is no more effort than making a few name substitutions. If you use the template more than once, you will already have received a positive return on the effort invested. While templates can be simple, you have the full power of the <a href="https://palletsprojects.com/p/jinja/">Jinja2</a> template engine when you need to do something more complex. I encourage you to check out the <a href="https://fvutils.github.io/vte/">VTE documentation</a> and look for opportunities where using template-driven content generation can make your life easier and make you more productive.</p><p><br /></p><div style="text-align: center;">Copyright 2022 Matthew Ballance</div><div><p style="font-variant-east-asian: normal; font-variant-numeric: normal; line-height: 16px; margin-bottom: 0in;"><span style="color: #666666;"><span face="Trebuchet MS, Trebuchet, Verdana, sans-serif"><span style="font-size: 9pt;"><i><span style="background: rgb(255, 255, 255);">The views and opinions expressed above are solely those of the author and do not represent those of my employer or any other party.</span></i></span></span></span></p></div>]]></content><author><name>Matthew Ballance</name></author><category term="Jinja2" /><category term="Code Generation" /><category term="Templates" /><category term="Python" /><category term="Functional Verification" /><summary type="html"><![CDATA[&nbsp;As a verification engineer, it's quite common to work with data and code that follow a regular pattern. Having an efficient way to create this repetitive code is a significant productivity boost. While there certainly are places in the code where 'your critical generation or checking algorithm' goes, much of the structure of an agent, a test environment, etc remain the same. The same goes for other parts of the flow, such as project meta-data, test lists, etc. There are two things that keep us from just making copies of a set of 'golden' files to create the basis for a new UVM agent, project, etc: some or all of the files need to have some data substituted or changed. For example, we want to substitute the name of the new UVM agent we're creating into most of the new SystemVerilog source code.Custom code generators have been developed for some of these tasks. These often focus on providing a domain-specific way to capture input data, such as the structure of a UVM testbench or the layout of registers in a design. But there are many more opportunities to generate template-driven code that cannot justify the investment to create a focused solution.A few years ago, I created the Verification Template Engine (VTE) to serve my needs for generating template-driven content. I developed VTE with three user-experience requirements in mind:Creating a new template should be very easy, but have access to powerful generation featuresManaging the available templates should be simple for a user.&nbsp;The core tools should be generic, and make few or no assumptions about what is being generatedVTE focuses on organizing and discovering template content, but leverages the Jinja2 template engine to do the heavy lifting of template expansion. In some sense, you can think of VTE as providing a user interface to the Jinaj2 library.I've been using VTE since developing it, but am just getting back to create proper documentation, which you can find here: https://fvutils.github.io/vte/. As part of that work, I created a quickstart guide which is both in the documentation, and forms the remainder of this post.&nbsp;Installing VTEThe easiest way to install VTE is from PyPi.% python3 -m pip install --user vteTest that you can run VTE by running the command (vte) and/or invoking the module:% vte --help% python3 -m vte --helpCreating a TemplateVTE discovers templates by searching directories on the VTE_TEMPLATE_PATH environment variable. VTE uses a marker file named .vte to identify the root of a template. All files and directories in and below a template directory are considered to be part of the template. The template identifier is composed from the directory names between the directory listed in VTE_TEMPLATE_PATH and the directory containing the .vte marker file.Let’s look at an example to illustrate the rules.templates&nbsp; uvm&nbsp; &nbsp; agent&nbsp; &nbsp; &nbsp; .vte&nbsp; &nbsp; component&nbsp; &nbsp; &nbsp; .vte&nbsp; doc&nbsp; &nbsp; blog_post&nbsp; &nbsp; &nbsp; .vte&nbsp; &nbsp; readme&nbsp; &nbsp; &nbsp; .vteLet’s assume we add the templates directory to VTE_TEMPLATE_PATH. VTE will find four templates:uvm.agentuvm.componentdoc.blog_postdoc.readmeAll files in and below the directory containing the .vte marker will be rendered when the template is used.Creating the Template StructureLet’s create a very simple template structure. Create the following directory structure:templates&nbsp; doc&nbsp; &nbsp; readmeChange directory to templates/doc/readme and run the quickstart command:% vte quickstartVerification Template Engine QuickstartTemplate directory: templates/doc/readmeTemplate Description []? Create a simple READMEThis command will prompt for a description to use for the template. Enter a description and press ENTER. This will create the .vte marker file.View the .vte file. You’ll see that the initial version is quite simple. For now, this is all we need.template:&nbsp; description: Create a simple README&nbsp; parameters: []#&nbsp; &nbsp;- name: param_name#&nbsp; &nbsp; &nbsp;description: param_desc#&nbsp; &nbsp; &nbsp;default: param_defaultCreating the Template FileNow, let’s create the template file that will be processed when we render the template. Our readme template only has one file: README.md.Create a file named README.md containing the following content in the templates/doc/readme directory:# README for {{name}}TODO: put in some content of interestVTE supports defining and using multiple parameters, but defines one built-in parameter that must be supplied for all templates: name. Our template file references name using Jinja2 syntax for variable references.We have now created a simple template for creating README.md files.Rendering a TemplateIn order to render templates, VTE must first be able to discover them. Add the templates directory to the VTE_TEMPLATE_PATH environment variable.% export VTE_TEMPLATE_PATH=&lt;path&gt;/templates # Bourne shell% setenv VTE_TEMPLATE_PATH &lt;path&gt;/templates # csh/tshLet’s test this out by running the vte list command:% vte listdoc.readme - Create a simple READMEIf you see the doc.readme line above, VTE has successfully discovered the template.Now, let’s actually generate something. Let’s create a new directory parallel to the templates directory in which to try this out% mkdir scratch% cd scratchFinally, let’s run the generate command:% vte generate doc.readme my_projectNote: processing template README.mdVTE prints a line for each template file is processes. The output above confirms that is processed the template README.md file.Let’s have a look at the result. View the README.md file in the scratch directory.# README for my_projectTODO: put in some content of interestNode that the {{name}} reference was replaced by the name (my_project) that we specified.You have now created your first VTE template!ConclusionAs the tutorial above illustrates, creating a new template for use with VTE is no more effort than making a few name substitutions. If you use the template more than once, you will already have received a positive return on the effort invested. While templates can be simple, you have the full power of the Jinja2 template engine when you need to do something more complex. I encourage you to check out the VTE documentation and look for opportunities where using template-driven content generation can make your life easier and make you more productive.Copyright 2022 Matthew BallanceThe views and opinions expressed above are solely those of the author and do not represent those of my employer or any other party.]]></summary></entry><entry><title type="html">PyUCIS: Manipulating Coverage Data</title><link href="https://bitsbytesgates.com/2022/07/17/pyucis-manipulating-coverage-data.html" rel="alternate" type="text/html" title="PyUCIS: Manipulating Coverage Data" /><published>2022-07-17T20:37:00+00:00</published><updated>2022-07-17T20:37:00+00:00</updated><id>https://bitsbytesgates.com/2022/07/17/pyucis-manipulating-coverage-data</id><content type="html" xml:base="https://bitsbytesgates.com/2022/07/17/pyucis-manipulating-coverage-data.html"><![CDATA[<p style="text-align: center;">&nbsp;<a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhp1-_7rtBzwcTNYBtGBUsqaxugtgc8RIz3D1KHRhbTpqghM-oTZjXq_6-ngByPlwmSHZgnu6f5e8ptq6wLa5EZFkOuOHWmgYY0JLua-wLrlTw38FcWT_hMZuFzwOEnxYv1oEimFAAqnr4bcDEo2meEFEZQvH7YdDXMGUixfLzHC6KGfQ_iuXOgn48ogg/s540/splash.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="300" data-original-width="540" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhp1-_7rtBzwcTNYBtGBUsqaxugtgc8RIz3D1KHRhbTpqghM-oTZjXq_6-ngByPlwmSHZgnu6f5e8ptq6wLa5EZFkOuOHWmgYY0JLua-wLrlTw38FcWT_hMZuFzwOEnxYv1oEimFAAqnr4bcDEo2meEFEZQvH7YdDXMGUixfLzHC6KGfQ_iuXOgn48ogg/s16000/splash.png" /></a><br /><br /></p><p>In a prior post, we looked at how to inspect coverage as a text report and export coverage data using the PyVSC API, and view coverage graphically using the PyUCIS-Viewer. Recent enhancements have enabled the PyUCIS library to provide even more ways to manipulate coverage data. Over the next couple of posts, we’ll look at those enhancements.&nbsp;</p><p><b>New ‘ucis’ Command</b></p><p>PyUCIS is a library for working with the Accellera UCIS data model. It started as a library for other applications and libraries, such as PyVSC and the PyUCIS Viewer, to use to read and write data using the UCIS data model. Recent enhancements have added standalone functionality which can meaningfully be accessed from the command line.&nbsp;</p><p>You can find documentation for the ucis command and sub-commands in the <a href="https://pyucis.readthedocs.io/en/latest/commands.html">PyUCIS documentation</a>. Fundamentally, there are four key operations:</p><p></p><ul style="text-align: left;"><li>Convert coverage data from one format to another</li><li>Merge coverage data from multiple databases into a single database</li><li>Produce coverage reports in various formats</li><li>Obtain information about available coverage data and report formats</li></ul><p></p><p>These commands are just a starting point. They will be enhanced over time, and more commands may be added as well. If you have suggestions for new commands and/or new capabilities for existing commands, feel free to add an enhancement request on the <a href="https://github.com/fvutils/pyucis/issues">PyUCIS GitHub page</a>.</p><p><b>Plug-in Framework</b></p><p>PyUCIS has added a plug-in framework with support for database formats and report formats. The goal is to make commands operating on coverage data extensible extensible from the beginning, as well as to enable the set of supported coverage-data formats and report formats to be easily extended without changing PyUCIS.&nbsp; I’ll devote a future post to the plug-in framework. For now, the ucis command supports listing the available coverage-data and report plug-ins. For example:</p><p><span style="font-family: courier;">% ucis list-db-formats</span></p><p><span style="font-family: courier;">libucis - Reads coverage data via an implementation of the UCIS C API</span></p><p><span style="font-family: courier;">xml&nbsp; &nbsp; &nbsp;- Supports reading and writing UCIS XML interchange</span></p><p><span style="font-family: courier;">yaml&nbsp; &nbsp; - Reads coverage data from a YAML file</span></p><p><br /></p><p><b>New Input Format</b></p><p>One often-requested PyUCIS feature is the ability to merge coverage data from several input coverage databases into a single resulting coverage database. One of the first challenges I faced in implementing this functionality was how to write tests. The UCIS API is written with applications in mind. I’ve found it to be a pretty-verbose API when it comes to writing tests. Consequently, tests written directly in terms of the API aren’t particularly easy to follow from a code perspective.</p><p>I decided to define a YAML format to make it simpler to capture coverage data in an easy-to -read way. Initially, this was just for testing. However, it may also be a useful interchange format that is less verbose and complex (also, quite possibly, more simplistic) that the XML interchange format defined by the UCIS standard.</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdaeXpmEYuvyXmWxluRALy-WLMci6JOtkxwHv8pizdR3AZyMLLDcKkbEzSkaj6fT7b9U4yxc06CLKMX1xy6Cc_VM2Ntgk0A7ri3epneWGF8BKVgzafT4-ks3BwSeZVIqKeUZFdxnVAc9AIiIC9gS75isWG5JSdGHqlobAPQSLcodmUBOHPA0lTrC6n9Q/s250/yaml_coverage_spec.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="237" data-original-width="250" height="237" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdaeXpmEYuvyXmWxluRALy-WLMci6JOtkxwHv8pizdR3AZyMLLDcKkbEzSkaj6fT7b9U4yxc06CLKMX1xy6Cc_VM2Ntgk0A7ri3epneWGF8BKVgzafT4-ks3BwSeZVIqKeUZFdxnVAc9AIiIC9gS75isWG5JSdGHqlobAPQSLcodmUBOHPA0lTrC6n9Q/s1600/yaml_coverage_spec.png" width="250" /></a></div><div><br /></div><div>A simple coverage specification is shown above. This coverage data describes a covergroup type (my_cvg) with a single instance (i1). A single coverpoint (cp1) has two bins (b1, b2) of which one has a single hit and one has no hits. While this coverage specification was created to make setting of test coverage data simpler for a human, I believe it may also be useful as a simple coverage-interchange format. If you find it useful, please let the community know via the <a href="https://github.com/fvutils/pyucis/discussions">Discussion forum</a> on the PyUCIS GitHub page.</div><div><br /></div><div>You can find more details on the <a href="https://pyucis.readthedocs.io/en/latest/reference/yaml_coverage.html">YAML Coverage Data Format reference documentation</a> page.&nbsp;</div><p><b>Merging Coverage Data</b></p><p>One consistently-requested feature for PyUCIS is the ability to merge multiple databases into a single unified coverage database. PyUCIS now supports basic merge functionality. Currently, PyUCIS performs a union merge where all unique coverage features found in all the input databases are propagated to the output database. I anticipate that more merge algorithms will need to be added over time, but hopefully this is a good start.</p><p><br /></p><p>Let’s take a look at a very simple case. Let’s say we have two coverage-data sets shown below:</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg6vvu8qYu9EklhJd0IkaC4_Px0ZcCjPQZbFy1-TtgNviArsSrtHc-PN7rSRYJE2XpUo4uTHQ7YaBTY3D8_VcOjL3EuOsvVRKlwTvAQA6H9LRVLyL0bAtZdBSi3Wv51MQR4PzCk2WQj-EckBSwdhhcVylHWirWl36g7amzFFmC9jah_wc1YLYMtZmL9uA/s504/merge_input_data.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="245" data-original-width="504" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg6vvu8qYu9EklhJd0IkaC4_Px0ZcCjPQZbFy1-TtgNviArsSrtHc-PN7rSRYJE2XpUo4uTHQ7YaBTY3D8_VcOjL3EuOsvVRKlwTvAQA6H9LRVLyL0bAtZdBSi3Wv51MQR4PzCk2WQj-EckBSwdhhcVylHWirWl36g7amzFFmC9jah_wc1YLYMtZmL9uA/s16000/merge_input_data.png" /></a></div><br /><p>The structure of these two coverage databases is the same (same covergroup type, instance, and coverpoint). Each coverage database has 50% coverage. Let’s merge these two databases and report the coverage.</p><p><span style="font-family: courier;">% ucis merge -if yaml -o merge.xml coverage_1.ycdb coverage_2.ycdb</span></p><p>We specify the two input databases, as well as their format (yaml). We specify the output database as merge.xml.</p><p>The resulting coverage report on the merged database will report 100% coverage, as expected:</p><p><span style="font-family: courier;">% ucis report merge.xml</span></p><p><span style="font-family: courier;">TYPE i1 : 100.000000%</span></p><p><span style="font-family: courier;">&nbsp; &nbsp; CVP cp1 : 100.000000%</span></p><p><b>Reporting Coverage Data</b></p><p>Reporting is a key activity when working with coverage data. We’ve looked at the ability to browse coverage data graphically using the PyUCIS-Viewer, but getting a textual report is every bit as important. In addition to presenting information concisely, textual reports can be processed programmatically to extract key pieces of data.&nbsp;</p><p>We can list the currently-available report plugins using the ucis command:</p><p><span style="font-family: courier;">% ucis list-rpt-formats</span></p><p><span style="font-family: courier;">json - Produces a machine-readable JSON coverage report</span></p><p><span style="font-family: courier;">txt&nbsp; - Produces a human-readable textual coverage report</span></p><p><br /></p><p>The default report is textual. Let’s create a textual report on the YAML coverage-data above:</p><p><span style="font-family: courier;">% ucis report -if yaml coverage.ycdb&nbsp;</span></p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifRBJaI9kmG0at2fuySwAQRHqOrow3uiP17zEjyQmj0dU-N0z3ZJNIuxebRKzImTChJgFqOt1GvsbhS967pQ1gR42WDsn5QPTlcLx3WsuTlhF8oTs6bpQZAUlFdUrda7VrP9CJJ2hBxGzeReERNEC9fnSc_b2P0A79M4AQx-kIl2MgdAEEywcZmgVFBw/s244/txt_coverage_report.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="85" data-original-width="244" height="85" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifRBJaI9kmG0at2fuySwAQRHqOrow3uiP17zEjyQmj0dU-N0z3ZJNIuxebRKzImTChJgFqOt1GvsbhS967pQ1gR42WDsn5QPTlcLx3WsuTlhF8oTs6bpQZAUlFdUrda7VrP9CJJ2hBxGzeReERNEC9fnSc_b2P0A79M4AQx-kIl2MgdAEEywcZmgVFBw/s1600/txt_coverage_report.png" width="244" /></a></div><br /><p>Note that we need to specify the format of the input data (yaml). The result is a simple human-readable report of the coverage data in the database.</p><p>What if we wanted to post-process the data using a script? We certainly could extract what we need by parsing the output above, but working with data in a machine-readable format is often much simpler. Let’s report our data in JSON format:</p><p><span style="font-family: courier;">% ucis report -if yaml -of json coverage.ycdb</span>&nbsp;</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi1LqtUHblxEwKju_-m9tESy3trKWjJi_-y-NkYK73HhoU-Etm9xdqIwqzkS7E_ASMnNmu86N6dYm2GfSb8-WeHZdspBEWi3oM4CLmnnLr_Fjok8prBX5rg0r3ip4QQ7Z8H0meu74yU3PbLxcuPHw41KAx42h2pHHpeClfVe4O6TJyMrEgWX4eiDrNxtg/s862/json_coverage_report.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="862" data-original-width="409" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi1LqtUHblxEwKju_-m9tESy3trKWjJi_-y-NkYK73HhoU-Etm9xdqIwqzkS7E_ASMnNmu86N6dYm2GfSb8-WeHZdspBEWi3oM4CLmnnLr_Fjok8prBX5rg0r3ip4QQ7Z8H0meu74yU3PbLxcuPHw41KAx42h2pHHpeClfVe4O6TJyMrEgWX4eiDrNxtg/s16000/json_coverage_report.png" /></a></div><div>Obviously, the data is less compact and more verbose. But, reading this into a Python script for further post-processing is incredibly simple! If you’re interested in the JSON report format, have a look at the schema documentation &lt;https://pyucis.readthedocs.io/en/latest/reference/coverage_report_json.html&gt;.</div><div><br /></div><div>So, for now, PyUCIS supports two textual report formats, and would benefit from more report formats. For example, a plain HTML report and a fancy interactive web-based report. If someone in the community has the skills and is interested, the project would definitely be interested!</div><p><b>Next Steps</b></p><p>PyUCIS continues to evolve, adding a more more hopefully-useful features at a time. Stay tuned for a future post on the plug-in interface, and the addition of more coverage-database and report formats.&nbsp;</p><p><br /></p><div style="text-align: center;">Copyright 2022 Matthew Ballance</div><div><p style="font-variant-east-asian: normal; font-variant-numeric: normal; line-height: 16px; margin-bottom: 0in;"><span style="color: #666666;"><span face="Trebuchet MS, Trebuchet, Verdana, sans-serif"><span style="font-size: 9pt;"><i><span style="background: rgb(255, 255, 255);">The views and opinions expressed above are solely those of the author and do not represent those of my employer or any other party.</span></i></span></span></span></p></div>]]></content><author><name>Matthew Ballance</name></author><category term="functional coverage" /><category term="Python" /><category term="Altera" /><category term="UCIS" /><category term="Accellera" /><summary type="html"><![CDATA[&nbsp;In a prior post, we looked at how to inspect coverage as a text report and export coverage data using the PyVSC API, and view coverage graphically using the PyUCIS-Viewer. Recent enhancements have enabled the PyUCIS library to provide even more ways to manipulate coverage data. Over the next couple of posts, we’ll look at those enhancements.&nbsp;New ‘ucis’ CommandPyUCIS is a library for working with the Accellera UCIS data model. It started as a library for other applications and libraries, such as PyVSC and the PyUCIS Viewer, to use to read and write data using the UCIS data model. Recent enhancements have added standalone functionality which can meaningfully be accessed from the command line.&nbsp;You can find documentation for the ucis command and sub-commands in the PyUCIS documentation. Fundamentally, there are four key operations:Convert coverage data from one format to anotherMerge coverage data from multiple databases into a single databaseProduce coverage reports in various formatsObtain information about available coverage data and report formatsThese commands are just a starting point. They will be enhanced over time, and more commands may be added as well. If you have suggestions for new commands and/or new capabilities for existing commands, feel free to add an enhancement request on the PyUCIS GitHub page.Plug-in FrameworkPyUCIS has added a plug-in framework with support for database formats and report formats. The goal is to make commands operating on coverage data extensible extensible from the beginning, as well as to enable the set of supported coverage-data formats and report formats to be easily extended without changing PyUCIS.&nbsp; I’ll devote a future post to the plug-in framework. For now, the ucis command supports listing the available coverage-data and report plug-ins. For example:% ucis list-db-formatslibucis - Reads coverage data via an implementation of the UCIS C APIxml&nbsp; &nbsp; &nbsp;- Supports reading and writing UCIS XML interchangeyaml&nbsp; &nbsp; - Reads coverage data from a YAML fileNew Input FormatOne often-requested PyUCIS feature is the ability to merge coverage data from several input coverage databases into a single resulting coverage database. One of the first challenges I faced in implementing this functionality was how to write tests. The UCIS API is written with applications in mind. I’ve found it to be a pretty-verbose API when it comes to writing tests. Consequently, tests written directly in terms of the API aren’t particularly easy to follow from a code perspective.I decided to define a YAML format to make it simpler to capture coverage data in an easy-to -read way. Initially, this was just for testing. However, it may also be a useful interchange format that is less verbose and complex (also, quite possibly, more simplistic) that the XML interchange format defined by the UCIS standard.A simple coverage specification is shown above. This coverage data describes a covergroup type (my_cvg) with a single instance (i1). A single coverpoint (cp1) has two bins (b1, b2) of which one has a single hit and one has no hits. While this coverage specification was created to make setting of test coverage data simpler for a human, I believe it may also be useful as a simple coverage-interchange format. If you find it useful, please let the community know via the Discussion forum on the PyUCIS GitHub page.You can find more details on the YAML Coverage Data Format reference documentation page.&nbsp;Merging Coverage DataOne consistently-requested feature for PyUCIS is the ability to merge multiple databases into a single unified coverage database. PyUCIS now supports basic merge functionality. Currently, PyUCIS performs a union merge where all unique coverage features found in all the input databases are propagated to the output database. I anticipate that more merge algorithms will need to be added over time, but hopefully this is a good start.Let’s take a look at a very simple case. Let’s say we have two coverage-data sets shown below:The structure of these two coverage databases is the same (same covergroup type, instance, and coverpoint). Each coverage database has 50% coverage. Let’s merge these two databases and report the coverage.% ucis merge -if yaml -o merge.xml coverage_1.ycdb coverage_2.ycdbWe specify the two input databases, as well as their format (yaml). We specify the output database as merge.xml.The resulting coverage report on the merged database will report 100% coverage, as expected:% ucis report merge.xmlTYPE i1 : 100.000000%&nbsp; &nbsp; CVP cp1 : 100.000000%Reporting Coverage DataReporting is a key activity when working with coverage data. We’ve looked at the ability to browse coverage data graphically using the PyUCIS-Viewer, but getting a textual report is every bit as important. In addition to presenting information concisely, textual reports can be processed programmatically to extract key pieces of data.&nbsp;We can list the currently-available report plugins using the ucis command:% ucis list-rpt-formatsjson - Produces a machine-readable JSON coverage reporttxt&nbsp; - Produces a human-readable textual coverage reportThe default report is textual. Let’s create a textual report on the YAML coverage-data above:% ucis report -if yaml coverage.ycdb&nbsp;Note that we need to specify the format of the input data (yaml). The result is a simple human-readable report of the coverage data in the database.What if we wanted to post-process the data using a script? We certainly could extract what we need by parsing the output above, but working with data in a machine-readable format is often much simpler. Let’s report our data in JSON format:% ucis report -if yaml -of json coverage.ycdb&nbsp;Obviously, the data is less compact and more verbose. But, reading this into a Python script for further post-processing is incredibly simple! If you’re interested in the JSON report format, have a look at the schema documentation &lt;https://pyucis.readthedocs.io/en/latest/reference/coverage_report_json.html&gt;.So, for now, PyUCIS supports two textual report formats, and would benefit from more report formats. For example, a plain HTML report and a fancy interactive web-based report. If someone in the community has the skills and is interested, the project would definitely be interested!Next StepsPyUCIS continues to evolve, adding a more more hopefully-useful features at a time. Stay tuned for a future post on the plug-in interface, and the addition of more coverage-database and report formats.&nbsp;Copyright 2022 Matthew BallanceThe views and opinions expressed above are solely those of the author and do not represent those of my employer or any other party.]]></summary></entry><entry><title type="html">Tools and Techniques to Improve YAML-File Usability</title><link href="https://bitsbytesgates.com/2022/06/27/tools-and-techniques-to-improve-yaml.html" rel="alternate" type="text/html" title="Tools and Techniques to Improve YAML-File Usability" /><published>2022-06-27T00:31:00+00:00</published><updated>2022-06-27T00:31:00+00:00</updated><id>https://bitsbytesgates.com/2022/06/27/tools-and-techniques-to-improve-yaml</id><content type="html" xml:base="https://bitsbytesgates.com/2022/06/27/tools-and-techniques-to-improve-yaml.html"><![CDATA[<p style="text-align: center;">&nbsp;</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeUm4wAbSaE2GiFPDoa1M_6U6OqGwT5jusXQtor0CaVBSy_OYi1JCUko102SmcL0MBhnNOpBpQznLycMXV4T3OyDLnYaicBSEAyTMzPIyjswUegXQPOutmCFliEKg3Njs3gollZawm6YiJq1Q2fb2APy3gJlnwSgON-q__3hMqnytgYdhA_1YWKHuGZg/s540/splash.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="300" data-original-width="540" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeUm4wAbSaE2GiFPDoa1M_6U6OqGwT5jusXQtor0CaVBSy_OYi1JCUko102SmcL0MBhnNOpBpQznLycMXV4T3OyDLnYaicBSEAyTMzPIyjswUegXQPOutmCFliEKg3Njs3gollZawm6YiJq1Q2fb2APy3gJlnwSgON-q__3hMqnytgYdhA_1YWKHuGZg/s16000/splash.png" /></a></div><br /><p></p><p>This blog post is a bit of a departure from many that I’ve created for this blog. Most of my blog posts are about things I’ve created. This post is about a collection of tools that I use in developing the things I create.&nbsp;&nbsp;</p><p>I’ve recently come back to working on some new features in PyUCIS, the Python library for accessing functional coverage data. PyUCIS provides an implementation of the Accellera UCIS, and several back-end implementations. Good tests are critical when developing new functionality and, in the case of PyUCIS, tests rely on having coverage data to manipulate. As it so happens, while the UCIS API is good for providing tools access to coverage data, it’s not a great interface for humans (and, specifically, for test writers). What test writers need is a very concise and easy-to-read mechanism to capture the coverage data on which the library should operate. How should we capture this data? A couple decades ago, I might have toyed with developing a small language grammar to capture exactly the data I needed. Today, using a mark-up language like YAML or JSON to capture such data is my go-to approach.</p><p><b>YAML - A Data Format for Everything and Nothing</b></p><p>There are many reasons for the popularity of YAML for capturing application-configuration information, such as what we need to capture coverage data. YAML’s structure of a nested series of mappings and lists lends itself to easily capturing all manner of data. Furthermore, support for reading and writing YAML is available the vast majority of programming languages.&nbsp;</p><p>However, the ease with which we can define new data formats, and create simple processors to accept data captured in these formats can be deceptive. It’s tempting to think that, because YAML defines a standard set of structures for capturing data, users will find it easy and intuitive to capture data in our specific format. It’s tempting to think that our format might be so simple that only a little documentation with a few examples may be more than sufficient. The truth, however, is that making our application-specific data format usable requires us to do many of the same things that we would have to do if we defined a custom language. Our YAML-based format must be fully-documented, our data processors must be robust in accepting valid content, rejecting invalid content, and not silently ignore unrecognized input. I’ve had the painful experience of coming back to a project (yep, one that I created) after a few months away and having to dig into the YAML-processing code to remember the data format.&nbsp;</p><p>The apparent ease with which we can access data from our application code is also a bit deceptive. Most YAML-reading libraries provide access to the data through a hierarchy of maps and list that mirrors the structure of the data. Depending on how we might want to subsequently process the data, we might first copy it to a set of custom data object, or we might access it by directly querying the maps and lists. In both cases,&nbsp;&nbsp;</p><p>The really thing about YAML, though, is that many tools exist precisely to help make a custom YAML-based format easy to use and reliable to implement. For the most part, I will focus on tools available in the Python ecosystem. However, many of these tools are equally-useful in when implementing applications in other languages. YAML-processing libraries exist in other language ecosystems as well.</p><p><b>PyUCIS Coverage Example</b></p><p>Let’s look at the following tools in the context of the YAML data format that PyUCIS uses to capture coverage data for testing. Here’s a small example:</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgfhgdr7Od7ZdsjwfNr912DRS5sh6z49-lKhnUCNzdbptH-ovF_YCyBbzDmzNrlQPGib_OvrMjpVwLqhe61HSwt39QSQ_p_TQXxkTcGpELesLCoUAjjXb2p-KA3Wwt3R2DBcIdDggHTN7Vjfd_O4rl_NJq2ES_TMpPT3lIiXy-QoDg4-AEbcUqwG3POzw/s393/coverage_yaml.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="393" data-original-width="245" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgfhgdr7Od7ZdsjwfNr912DRS5sh6z49-lKhnUCNzdbptH-ovF_YCyBbzDmzNrlQPGib_OvrMjpVwLqhe61HSwt39QSQ_p_TQXxkTcGpELesLCoUAjjXb2p-KA3Wwt3R2DBcIdDggHTN7Vjfd_O4rl_NJq2ES_TMpPT3lIiXy-QoDg4-AEbcUqwG3POzw/s16000/coverage_yaml.png" /></a></div><div class="separator" style="clear: both; text-align: center;"><br /></div><div class="separator" style="clear: both; text-align: left;">The root of data in the document is named ‘coverage’. Currently, ‘coverage’ consists of a series of covergroup types under the ‘covergroups’ section. Each covergroup type has a name and a list of instances. A covergroup instance holds coverpoints, which have bins in which hit counts are stored. The format is intended to make it very simple to capture coverage data for use in testing coverage reporting and merging tools. It’s also not a bad format to bring in coverage data from other tools.</div><div class="separator" style="clear: both; text-align: left;"><br /></div><div class="separator" style="clear: both; text-align: left;"><b>PyYAML</b></div><div class="separator" style="clear: both; text-align: left;"><div class="separator" style="clear: both;">It’s incredibly simple to read data from a YAML-formatted file. I’ve tended to use the PyYAML Python library, but there are many other choices. With PyYAML, reading in file like the example above is incredibly simple:</div><div class="separator" style="clear: both;"><br /></div><div class="separator" style="clear: both;"><span style="font-family: courier;">import yaml</span></div><div class="separator" style="clear: both;"><span style="font-family: courier;"><br /></span></div><div class="separator" style="clear: both;"><span style="font-family: courier;">with open(“coverage.yaml”, “r”) as fp:</span></div><div class="separator" style="clear: both;"><span style="font-family: courier;"><span style="white-space: pre;">	</span>yaml_data = yaml.load(fp, Loader=yaml.FullLoader)</span></div><div class="separator" style="clear: both;"><br /></div><div class="separator" style="clear: both;">The result is a hierarchy of Python dictionaries and lists containing the data from the file, which we can walk by indexing. For example:</div><div class="separator" style="clear: both;"><br /></div><div class="separator" style="clear: both;"><span style="font-family: courier;">for cg in yaml_data[“coverage”][“covergroups”]:</span></div><div class="separator" style="clear: both;"><span style="font-family: courier;">&nbsp; print(“Covergroup type: %s” % cg[“name”])</span></div><div style="font-weight: bold;"><br /></div></div><b>JSON Schema</b><div><b><br /></b><div><div>One thing we will always want to ensure is that a coverage file conforms to the required syntax. One way to do this is to hand-code a validator that walks through the data structure from the parser and confirms that required elements are present and unexpected elements are not. Another is to create a schema for the document and use a validation library.&nbsp;</div><div>We will create a schema for the coverage file format. Creating a schema is the most efficient way to enable validation of our file format. In addition, once we have a schema, there are many other ways that we can use it.</div><div>Despite the fact that we are using YAML for our data, we will capture the schema using json-schema.</div><div style="font-weight: bold;"><br /></div><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6PcfTk_RTY780zR_cGW4akORUYkp9K9EeglDvrt627RUf1Zbb065vcMHve20Nf3AztmLwPWto_x1AJbk33EX2oGC4L-zoxyrxjQ3XSmRNG018INzG66fly9nF1yRuyjXV68g1KxA7QkBoZDUs0t6vPQa615j0-n-b0XgeKN0Bo_UkTSpYgAt5Lw1I8w/s759/schema_ex.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="650" data-original-width="759" height="549" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6PcfTk_RTY780zR_cGW4akORUYkp9K9EeglDvrt627RUf1Zbb065vcMHve20Nf3AztmLwPWto_x1AJbk33EX2oGC4L-zoxyrxjQ3XSmRNG018INzG66fly9nF1yRuyjXV68g1KxA7QkBoZDUs0t6vPQa615j0-n-b0XgeKN0Bo_UkTSpYgAt5Lw1I8w/w640-h549/schema_ex.png" width="640" /></a></div><br /><p>The example above is the first part of the schema for our coverage data. It’s a bit verbose, but notice a few things:</p><p></p><ul style="text-align: left;"><li>The root of our document is an object (a dictionary with keys and values) with a single root element coverage</li><li>A coverage section is an array of covergroupType. Note that the schema refers to this separate declaration, which allows it to be referenced and reused in multiple locations.</li><li>covergroupType&nbsp; specifies that it is an object that has three possible sub-entries (name, weight, instances)</li><li>Of these possible sub-entries, only ‘name’ is required</li></ul><p></p><p>This merely scratches the surface of what is possible to describe with json-schema. There’s a bit of a learning curve, but my experience has been that it’s pretty straightforward once you learn a few fundamentals.</p><p>Once we have a schema, we can validate the data-structure returned from the YAML parser against the schema using the jsonschema Python library.</p><p><span style="font-family: courier;">import yaml</span></p><p><span style="font-family: courier;">import json</span></p><p><span style="font-family: courier;">import jsonschema</span></p><p><span style="font-family: courier;"><br /></span></p><p><span style="font-family: courier;">with open(“coverage.yaml”, “r”) as fp:</span></p><p><span style="font-family: courier;"><span style="white-space: pre;">	</span>yaml_data = yaml.load(fp, Loader=yaml.FullLoader)</span></p><p><span style="font-family: courier;">with open(“coverage_schema.json”, “r”) as fp:</span></p><p><span style="font-family: courier;">&nbsp; &nbsp; &nbsp;schema = json.load(fp)</span></p><p><span style="font-family: courier;">jsonschema.validate(instance=yaml_data, schema=schema)</span></p><p><br /></p><p>Validating a document prior to attempting to process the data structure from the YAML parser allows us to simplify our processing code because we can assuming that the structure of the data is correct.</p><p><b>Python-JsonSchema-Objects</b></p><p>The simplest way to obtain data is to operate directly on the data structure returned by the parser.&nbsp;</p><p>While&nbsp; this is simple and straightforward, there is at least one significant pitfall: it’s almost never a good idea to use string literals. Consider what happens if we change the name of one of our optional keywords just a bit.&nbsp;</p><p><span style="font-family: courier;">weight=1</span></p><p><span style="font-family: courier;">if “weight” in cg.keys():</span></p><p><span style="font-family: courier;">&nbsp; weight = cg[“weight”]</span></p><p>If we neglect to update all the locations in our code that use this string literal, some of our data will simply be silently ignored. Clearly, there are some incremental steps we can take – for example, defining a constant for each string literal, making it easier to update.&nbsp;</p><p>Another approach is to work with classes that are generated from our schema. This approach makes it much more likely that we’ll find data misuse issues earlier, and has the added benefit of giving us actual classes to work with. I recently discovered the python-jsonschema-objects project, and used it on PyUCIS for the first time. Thus far, I’m extremely impressed and plan to use it more broadly.</p><p>The short version of how it works is as follows. python-jsonschema-objects works off of a JSON-schema document. Each section of the schema (eg covergroupType) should be given a title from which the class name will be derived. Call python-schema-objects to build a Python namespace containing class declarations. Your code can then create classes and populate them – either directly or from parsed data.</p><p>It looks like this:</p><p><span style="font-family: courier;">import python_jsonscehma_objects as pjs</span></p><p><span style="font-family: courier;"><br /></span></p><p><span style="font-family: courier;">builder = pjs.ObjectBuilder(schema)</span></p><p><span style="font-family: courier;">ns = builder.build_classes()</span></p><p><span style="font-family: courier;">cov = ns.CoverageData().from_json(json.dumps(yaml_data))</span></p><p><span style="font-family: courier;"><br /></span></p><p><span style="font-family: courier;">if cov.covergroups is not None:</span></p><p><span style="font-family: courier;">&nbsp; for cg in cov.covergroups:</span></p><p><span style="font-family: courier;">&nbsp; &nbsp; print(“cg: %s” % cg.name)</span></p><p><br /></p><p></p><p>The ‘ns’ object above contains the classes derived from the definitions in the schema. We can create an instance of a CoverageData class that contains our schema-compliant data just by loading the JSON representation of that YAML data. From there on, we can directly access our data as class fields.</p><p><b>VSCode YAML Editor</b></p><p>Thus far, we’ve primarily looked at tools that help the developer. The final two tools are focused on improving the user experience. Both leverage our document schema.</p><p>Visual Studio Code (VSCode) is a free integrated development environment (IDE) produced by Microsoft. In open source terms, it’s free as in beer. My understanding is that there are compatible truly open source versions as well. As with many IDEs, there is an extensive ecosystem of plug-ins available&nbsp; to assist in developing different types of code. One of those plug-ins supports YAML development.</p><p>So, what does having a schema allow an intelligent editor to do for us? Well, for one thing, it can check the validity of a YAML file as we type it and allow us to fix errors as we go.&nbsp;</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhmCCgG5L9mMcINvPYrvpocZhzGm-RKNB9pw1alxT6ady2z5MXX3dKUiJ5eJoIizFGYo1LidRk8zvopvVjuA0c0ieimBq4KAmwUHlwcEIV6ngLGPmo9zYFEi-Fn1wpuDeGo708acq1JLH0hjqHMqv0OZV7EK2-Jj5B8hXyJeTh9GravZTTLZw0Q6JFRrQ/s602/vscode_autocomplete.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="232" data-original-width="602" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhmCCgG5L9mMcINvPYrvpocZhzGm-RKNB9pw1alxT6ady2z5MXX3dKUiJ5eJoIizFGYo1LidRk8zvopvVjuA0c0ieimBq4KAmwUHlwcEIV6ngLGPmo9zYFEi-Fn1wpuDeGo708acq1JLH0hjqHMqv0OZV7EK2-Jj5B8hXyJeTh9GravZTTLZw0Q6JFRrQ/s16000/vscode_autocomplete.png" /></a></div><br /><p>It can suggest what content is valid based on where we are in the document. For example, the schema states that we can have coverpoints and crosses elements inside an instances section. The editor knows this, and prompts us with what it knows is valid.</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxqBocp04oJKSm7KgrkmvArNER7hJZKhT-t0ptwZhRlXUEQY7YsdYrqB7iIYvsXaLVTVNmf-5TsgI-QkhG8WCDTwHfqueqlEw3mto07zL8mnHqQWBlNsLh9eDEhkT7Jfot4nhjP0j3dZCLI7B2K5nlAbQQenJY-y8RBWHZ2xwzMLEj9jglgOu1-bEu3A/s407/vscode_hover.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="197" data-original-width="407" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxqBocp04oJKSm7KgrkmvArNER7hJZKhT-t0ptwZhRlXUEQY7YsdYrqB7iIYvsXaLVTVNmf-5TsgI-QkhG8WCDTwHfqueqlEw3mto07zL8mnHqQWBlNsLh9eDEhkT7Jfot4nhjP0j3dZCLI7B2K5nlAbQQenJY-y8RBWHZ2xwzMLEj9jglgOu1-bEu3A/s16000/vscode_hover.png" /></a></div><div class="separator" style="clear: both; text-align: center;"><br /></div><div class="separator" style="clear: both; text-align: left;"><br /></div><div class="separator" style="clear: both; text-align: left;">It can also shows us information about the document section we’re hovering over. Features like these can significantly improve ease of use, making it easier for your users to get started.&nbsp;</div><div><br /></div><b>Sphinx Json Schema</b></div><div><div>Over time, I’ve really come to love Sphinx-Doc for documenting projects. I really like the way it enables combining human-created content with content extracted from the implementation code. I think it finds a great middle ground between tools that fully-generate documentation from code comments and documentation that is fully human created.</div><div><br /></div><div>Not surprisingly, Sphinx has an extension that supports extracting data from a JSON schema. The extracted data provides a great synopsis of the data format. It’s very likely that you’ll want to add in a bit of extra description on top of what makes sense to put directly in the schema documentation.</div><div><br /></div><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjyLqGYA4KAKB2iLodt0A-lKIYoYwG9i8pSM26Lcw85XZ5XUlctJtYYZPTec55urV5x_QBb20etVf2M60i0MsWyf7Sip7l_ZDU6hzr4zgSwT7o8il8p5f-FZyGN89VcWuCXnJlDWRK-a6b6RMXli-vlTnF8UW3H_aPcBxUVSGhOb-8IGmQaMlx83Oq4A/s756/sphinx-doc.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="526" data-original-width="756" height="446" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjyLqGYA4KAKB2iLodt0A-lKIYoYwG9i8pSM26Lcw85XZ5XUlctJtYYZPTec55urV5x_QBb20etVf2M60i0MsWyf7Sip7l_ZDU6hzr4zgSwT7o8il8p5f-FZyGN89VcWuCXnJlDWRK-a6b6RMXli-vlTnF8UW3H_aPcBxUVSGhOb-8IGmQaMlx83Oq4A/w640-h446/sphinx-doc.png" width="640" /></a></div><br /><div>The heading and table above are the result of using sphinx-jsonschema to document the covergroupType entity in our coverage schema. All the sub-elements are documented, and complex sub-elements have links to the relevant documentation. The text below the table is description that was manually added to the document. As with most Sphinx plug-ins, the jsonschema plug-in saves the developer from the laborious work of documenting the structure of the document.</div><div><br /></div><div><b>Conclusion</b></div><div>YAML is an excellent textual format structure for capturing structured data in a human readable way. Making use of a few readily-available free and open-source tools can make domain-specific YAML-based file formats much easier and reliable to implement, and can dramatically increase their usability. Next time you start sketching out a YAML-file format to use in your application, I’d encourage you to also reach for some of these tools. Your users will thank you – even if the sole user ends up being you!</div><div><br /></div><div><i><b>References</b></i></div><div><div><b style="font-style: italic;">&nbsp; &nbsp; • </b>PyUCIS GitHub - https://github.com/fvutils/pyucis</div><div>&nbsp; &nbsp; • PyUCIS Docs - https://fvutils.github.io/pyucis/</div><div>&nbsp; &nbsp; • json-schema - https://json-schema.org/</div><div>&nbsp; &nbsp; • jsonschema Python library - https://pypi.org/project/jsonschema/</div><div>&nbsp; &nbsp; • RedHat YAML editor for VSCode - https://github.com/redhat-developer/vscode-yaml</div><div>&nbsp; &nbsp; • sphinx-jsonschema - https://sphinx-jsonschema.readthedocs.io/en/latest/</div><div><br /></div><div><br /></div><div><br /></div><div><div style="text-align: center;">Copyright 2022 Matthew Ballance</div><div><p style="font-variant-east-asian: normal; font-variant-numeric: normal; line-height: 16px; margin-bottom: 0in;"><span style="color: #666666;"><span face="Trebuchet MS, Trebuchet, Verdana, sans-serif"><span style="font-size: 9pt;"><i><span style="background: rgb(255, 255, 255);">The views and opinions expressed above are solely those of the author and do not represent those of my employer or any other party.</span></i></span></span></span></p></div></div><div><br /></div></div><p align="left" style="background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; line-height: 1px; margin-bottom: 0in;"><br /></p><p align="left" style="background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; line-height: 1px; margin-bottom: 0in;"><br /></p><p><br /></p></div></div>]]></content><author><name>Matthew Ballance</name></author><category term="Python" /><category term="YAML" /><category term="JSON" /><category term="Schema" /><summary type="html"><![CDATA[&nbsp;This blog post is a bit of a departure from many that I’ve created for this blog. Most of my blog posts are about things I’ve created. This post is about a collection of tools that I use in developing the things I create.&nbsp;&nbsp;I’ve recently come back to working on some new features in PyUCIS, the Python library for accessing functional coverage data. PyUCIS provides an implementation of the Accellera UCIS, and several back-end implementations. Good tests are critical when developing new functionality and, in the case of PyUCIS, tests rely on having coverage data to manipulate. As it so happens, while the UCIS API is good for providing tools access to coverage data, it’s not a great interface for humans (and, specifically, for test writers). What test writers need is a very concise and easy-to-read mechanism to capture the coverage data on which the library should operate. How should we capture this data? A couple decades ago, I might have toyed with developing a small language grammar to capture exactly the data I needed. Today, using a mark-up language like YAML or JSON to capture such data is my go-to approach.YAML - A Data Format for Everything and NothingThere are many reasons for the popularity of YAML for capturing application-configuration information, such as what we need to capture coverage data. YAML’s structure of a nested series of mappings and lists lends itself to easily capturing all manner of data. Furthermore, support for reading and writing YAML is available the vast majority of programming languages.&nbsp;However, the ease with which we can define new data formats, and create simple processors to accept data captured in these formats can be deceptive. It’s tempting to think that, because YAML defines a standard set of structures for capturing data, users will find it easy and intuitive to capture data in our specific format. It’s tempting to think that our format might be so simple that only a little documentation with a few examples may be more than sufficient. The truth, however, is that making our application-specific data format usable requires us to do many of the same things that we would have to do if we defined a custom language. Our YAML-based format must be fully-documented, our data processors must be robust in accepting valid content, rejecting invalid content, and not silently ignore unrecognized input. I’ve had the painful experience of coming back to a project (yep, one that I created) after a few months away and having to dig into the YAML-processing code to remember the data format.&nbsp;The apparent ease with which we can access data from our application code is also a bit deceptive. Most YAML-reading libraries provide access to the data through a hierarchy of maps and list that mirrors the structure of the data. Depending on how we might want to subsequently process the data, we might first copy it to a set of custom data object, or we might access it by directly querying the maps and lists. In both cases,&nbsp;&nbsp;The really thing about YAML, though, is that many tools exist precisely to help make a custom YAML-based format easy to use and reliable to implement. For the most part, I will focus on tools available in the Python ecosystem. However, many of these tools are equally-useful in when implementing applications in other languages. YAML-processing libraries exist in other language ecosystems as well.PyUCIS Coverage ExampleLet’s look at the following tools in the context of the YAML data format that PyUCIS uses to capture coverage data for testing. Here’s a small example:The root of data in the document is named ‘coverage’. Currently, ‘coverage’ consists of a series of covergroup types under the ‘covergroups’ section. Each covergroup type has a name and a list of instances. A covergroup instance holds coverpoints, which have bins in which hit counts are stored. The format is intended to make it very simple to capture coverage data for use in testing coverage reporting and merging tools. It’s also not a bad format to bring in coverage data from other tools.PyYAMLIt’s incredibly simple to read data from a YAML-formatted file. I’ve tended to use the PyYAML Python library, but there are many other choices. With PyYAML, reading in file like the example above is incredibly simple:import yamlwith open(“coverage.yaml”, “r”) as fp: yaml_data = yaml.load(fp, Loader=yaml.FullLoader)The result is a hierarchy of Python dictionaries and lists containing the data from the file, which we can walk by indexing. For example:for cg in yaml_data[“coverage”][“covergroups”]:&nbsp; print(“Covergroup type: %s” % cg[“name”])JSON SchemaOne thing we will always want to ensure is that a coverage file conforms to the required syntax. One way to do this is to hand-code a validator that walks through the data structure from the parser and confirms that required elements are present and unexpected elements are not. Another is to create a schema for the document and use a validation library.&nbsp;We will create a schema for the coverage file format. Creating a schema is the most efficient way to enable validation of our file format. In addition, once we have a schema, there are many other ways that we can use it.Despite the fact that we are using YAML for our data, we will capture the schema using json-schema.The example above is the first part of the schema for our coverage data. It’s a bit verbose, but notice a few things:The root of our document is an object (a dictionary with keys and values) with a single root element coverageA coverage section is an array of covergroupType. Note that the schema refers to this separate declaration, which allows it to be referenced and reused in multiple locations.covergroupType&nbsp; specifies that it is an object that has three possible sub-entries (name, weight, instances)Of these possible sub-entries, only ‘name’ is requiredThis merely scratches the surface of what is possible to describe with json-schema. There’s a bit of a learning curve, but my experience has been that it’s pretty straightforward once you learn a few fundamentals.Once we have a schema, we can validate the data-structure returned from the YAML parser against the schema using the jsonschema Python library.import yamlimport jsonimport jsonschemawith open(“coverage.yaml”, “r”) as fp: yaml_data = yaml.load(fp, Loader=yaml.FullLoader)with open(“coverage_schema.json”, “r”) as fp:&nbsp; &nbsp; &nbsp;schema = json.load(fp)jsonschema.validate(instance=yaml_data, schema=schema)Validating a document prior to attempting to process the data structure from the YAML parser allows us to simplify our processing code because we can assuming that the structure of the data is correct.Python-JsonSchema-ObjectsThe simplest way to obtain data is to operate directly on the data structure returned by the parser.&nbsp;While&nbsp; this is simple and straightforward, there is at least one significant pitfall: it’s almost never a good idea to use string literals. Consider what happens if we change the name of one of our optional keywords just a bit.&nbsp;weight=1if “weight” in cg.keys():&nbsp; weight = cg[“weight”]If we neglect to update all the locations in our code that use this string literal, some of our data will simply be silently ignored. Clearly, there are some incremental steps we can take – for example, defining a constant for each string literal, making it easier to update.&nbsp;Another approach is to work with classes that are generated from our schema. This approach makes it much more likely that we’ll find data misuse issues earlier, and has the added benefit of giving us actual classes to work with. I recently discovered the python-jsonschema-objects project, and used it on PyUCIS for the first time. Thus far, I’m extremely impressed and plan to use it more broadly.The short version of how it works is as follows. python-jsonschema-objects works off of a JSON-schema document. Each section of the schema (eg covergroupType) should be given a title from which the class name will be derived. Call python-schema-objects to build a Python namespace containing class declarations. Your code can then create classes and populate them – either directly or from parsed data.It looks like this:import python_jsonscehma_objects as pjsbuilder = pjs.ObjectBuilder(schema)ns = builder.build_classes()cov = ns.CoverageData().from_json(json.dumps(yaml_data))if cov.covergroups is not None:&nbsp; for cg in cov.covergroups:&nbsp; &nbsp; print(“cg: %s” % cg.name)The ‘ns’ object above contains the classes derived from the definitions in the schema. We can create an instance of a CoverageData class that contains our schema-compliant data just by loading the JSON representation of that YAML data. From there on, we can directly access our data as class fields.VSCode YAML EditorThus far, we’ve primarily looked at tools that help the developer. The final two tools are focused on improving the user experience. Both leverage our document schema.Visual Studio Code (VSCode) is a free integrated development environment (IDE) produced by Microsoft. In open source terms, it’s free as in beer. My understanding is that there are compatible truly open source versions as well. As with many IDEs, there is an extensive ecosystem of plug-ins available&nbsp; to assist in developing different types of code. One of those plug-ins supports YAML development.So, what does having a schema allow an intelligent editor to do for us? Well, for one thing, it can check the validity of a YAML file as we type it and allow us to fix errors as we go.&nbsp;It can suggest what content is valid based on where we are in the document. For example, the schema states that we can have coverpoints and crosses elements inside an instances section. The editor knows this, and prompts us with what it knows is valid.It can also shows us information about the document section we’re hovering over. Features like these can significantly improve ease of use, making it easier for your users to get started.&nbsp;Sphinx Json SchemaOver time, I’ve really come to love Sphinx-Doc for documenting projects. I really like the way it enables combining human-created content with content extracted from the implementation code. I think it finds a great middle ground between tools that fully-generate documentation from code comments and documentation that is fully human created.Not surprisingly, Sphinx has an extension that supports extracting data from a JSON schema. The extracted data provides a great synopsis of the data format. It’s very likely that you’ll want to add in a bit of extra description on top of what makes sense to put directly in the schema documentation.The heading and table above are the result of using sphinx-jsonschema to document the covergroupType entity in our coverage schema. All the sub-elements are documented, and complex sub-elements have links to the relevant documentation. The text below the table is description that was manually added to the document. As with most Sphinx plug-ins, the jsonschema plug-in saves the developer from the laborious work of documenting the structure of the document.ConclusionYAML is an excellent textual format structure for capturing structured data in a human readable way. Making use of a few readily-available free and open-source tools can make domain-specific YAML-based file formats much easier and reliable to implement, and can dramatically increase their usability. Next time you start sketching out a YAML-file format to use in your application, I’d encourage you to also reach for some of these tools. Your users will thank you – even if the sole user ends up being you!References&nbsp; &nbsp; • PyUCIS GitHub - https://github.com/fvutils/pyucis&nbsp; &nbsp; • PyUCIS Docs - https://fvutils.github.io/pyucis/&nbsp; &nbsp; • json-schema - https://json-schema.org/&nbsp; &nbsp; • jsonschema Python library - https://pypi.org/project/jsonschema/&nbsp; &nbsp; • RedHat YAML editor for VSCode - https://github.com/redhat-developer/vscode-yaml&nbsp; &nbsp; • sphinx-jsonschema - https://sphinx-jsonschema.readthedocs.io/en/latest/Copyright 2022 Matthew BallanceThe views and opinions expressed above are solely those of the author and do not represent those of my employer or any other party.]]></summary></entry><entry><title type="html">PyVSC: Working with Coverage Data</title><link href="https://bitsbytesgates.com/2022/06/12/pyvsc-working-with-coverage-data.html" rel="alternate" type="text/html" title="PyVSC: Working with Coverage Data" /><published>2022-06-12T23:01:00+00:00</published><updated>2022-06-12T23:01:00+00:00</updated><id>https://bitsbytesgates.com/2022/06/12/pyvsc-working-with-coverage-data</id><content type="html" xml:base="https://bitsbytesgates.com/2022/06/12/pyvsc-working-with-coverage-data.html"><![CDATA[<p>&nbsp;</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjq5wnj9yN65OUU7dsp8gm5PRovjwCZlQubAsXkkaEis47rG46JBgh0gaNcbGqXjUX0HbjZQd1nRKrIChY0X6x_pVn_vbvGHNWxKyFDRiEATtKA-HWvSGjqViM03kqSmXfOsXtEI1NrobfqM6Q1E-rAkNKXk4RqeJfAIR3Wi_tLDYutBFSiDxMU2VjOFg/s540/pyvsc_coverage.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="300" data-original-width="540" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjq5wnj9yN65OUU7dsp8gm5PRovjwCZlQubAsXkkaEis47rG46JBgh0gaNcbGqXjUX0HbjZQd1nRKrIChY0X6x_pVn_vbvGHNWxKyFDRiEATtKA-HWvSGjqViM03kqSmXfOsXtEI1NrobfqM6Q1E-rAkNKXk4RqeJfAIR3Wi_tLDYutBFSiDxMU2VjOFg/s16000/pyvsc_coverage.png" /></a></div><br /><p></p><p>I’ve been investing some time in documentation updates this weekend, after a couple of <a href="https://github.com/fvutils/pyvsc">PyVSC</a> users pointed out some under-described aspects of the PyVSC coverage flow. Given that these areas were under-documented in the past, it seemed a good opportunity to highlight what can be done with functional coverage data once it is sampled by a PyVSC covergroup.</p><p>So, we’ve described some functional coverage goals using a PyVSC covergroup and coverpoints, created a covergroup instance, and sampled some coverage data – perhaps it was randomly-generated stimulus or data sampled from a monitor. What now?</p><p><b>Runtime Coverage API</b></p><p>One simple thing we can do is to query coverage achieved using the coverage APIs implemented by PyVSC covergroup classes. The `get_coverage` method returns the coverage achieved by all instances of a covergroup type. The `get_inst_coverage` method returns the coverage achieved by the specified covergroup instance.</p><p>Let’s look at an example:</p><div class="separator" style="clear: both; text-align: left;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgp4sxGCmgvfSB9S5pp78tiKP9ANcuyBKm63jyX6Ba4FzkbWym3MKXtZm7XHklK37Q85x1d-B2WjNvVk4emQP6dQqtWWRBFb1QXRiB2osalTHrhv1_QJqVGejSBiGVBm4-z0QpU_wXceqAAilxYOfPOTI9aFJac3NmpNevdRnePWsu78MXv-8kluu0ppA/s696/CoverageMethodsExample.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="446" data-original-width="696" height="410" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgp4sxGCmgvfSB9S5pp78tiKP9ANcuyBKm63jyX6Ba4FzkbWym3MKXtZm7XHklK37Q85x1d-B2WjNvVk4emQP6dQqtWWRBFb1QXRiB2osalTHrhv1_QJqVGejSBiGVBm4-z0QpU_wXceqAAilxYOfPOTI9aFJac3NmpNevdRnePWsu78MXv-8kluu0ppA/w640-h410/CoverageMethodsExample.png" width="640" /></a></div><br /><p>In the example above, we define a covergroup with a coverpoint that contains four bins (1, 2, 4, 8). We create two instances of this covergroup and sample them with two different values. After each call to sample, we display the coverage achieved by all instances of the covergroup (type coverage) and the coverage achieved by each instance.</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2qk4RnWoLhRiXIiobU0GG5RUXFb-vzkDOOWbZxIQJEE9LIgaPJHgdw39_4zxORd4TArXagA9F8oeCRqPX_huZZNdsISo9EBDq4p2wpwTKlm4_AvW7DLRbrY871A7AWpp7WBCClI1FiPQhtevTqBZKKcNEvvCXTHgGa_Zyk-dINdavQtYNqhb7E8Zvmw/s696/CoverageMethodsExample_output.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="60" data-original-width="696" height="55" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2qk4RnWoLhRiXIiobU0GG5RUXFb-vzkDOOWbZxIQJEE9LIgaPJHgdw39_4zxORd4TArXagA9F8oeCRqPX_huZZNdsISo9EBDq4p2wpwTKlm4_AvW7DLRbrY871A7AWpp7WBCClI1FiPQhtevTqBZKKcNEvvCXTHgGa_Zyk-dINdavQtYNqhb7E8Zvmw/w640-h55/CoverageMethodsExample_output.png" width="640" /></a></div><br /><p>The output from this example is shown above. After sampling the first covergroup, the coverage achieved for that, and all, instances is 25% since one of four bins was hit. After sampling the second covergroup, the coverage achieved for that covergroup instance is also 25%. Because two different bins are hit between the two covergroup instances, two of four bins are hit (50%) for type coverage.</p><p><br /></p><p><b>Runtime Coverage Reports</b></p><p>Another way to look at collected coverage is via a coverage report. PyVSC provides two methods that are nearly identical for obtaining a textual coverage report:</p><p></p><ul style="text-align: left;"><li>get_coverage_report – Returns the report as a string</li><li>report_coverage – Writes the report to a string (stdout by default)</li></ul><p></p><p>Both of these methods accept a keyword parameter named ‘details’ which controls whether bin hits are reported or just the top-level coverage achieved. Let’s look at a derivative of the first example to better understand the textual coverage report options.</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRCPcddPWGxNhpIXiH6rLaOqn_mExL05U8D8FGs2S_j4b6sNzqrzZNGd7oc7DeAAznwyc625pf5FePHsBmfrNXm2iHnbIeCxwU_ivnGg6XRTLA_dtbUyVPRhPK3k1kVmvedG07N5xB_1Won8A0oDH3ECbnODzM4NrETSJ5Mmt2yl73YCOVpxgLQR-mmg/s695/CoverageReportExample.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="396" data-original-width="695" height="364" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRCPcddPWGxNhpIXiH6rLaOqn_mExL05U8D8FGs2S_j4b6sNzqrzZNGd7oc7DeAAznwyc625pf5FePHsBmfrNXm2iHnbIeCxwU_ivnGg6XRTLA_dtbUyVPRhPK3k1kVmvedG07N5xB_1Won8A0oDH3ECbnODzM4NrETSJ5Mmt2yl73YCOVpxgLQR-mmg/w640-h364/CoverageReportExample.png" width="640" /></a></div><br /><p>This example is nearly identical to the first one, but with calls to ‘report_coverage’ instead of calls to the covergroup get_coverage methods.</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdCGkZXcD3sQrlwTpP0rZV5xdOBlI95xSCqcq0mR9pAhpIHb32K82BLuZvwxq7xaFCo7xryzLMcwcZhZSOpHkIYgWdqjisDqq-E7089ddMUZ1vS85LNACJh7JFmz9fXMME4bXGS9gXXWc3CxuJy6VQHCPkI6QFxHJH6DEk2-QbrvJszihsWpkEgtqgqA/s694/CoverageReportExample_output.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="509" data-original-width="694" height="470" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdCGkZXcD3sQrlwTpP0rZV5xdOBlI95xSCqcq0mR9pAhpIHb32K82BLuZvwxq7xaFCo7xryzLMcwcZhZSOpHkIYgWdqjisDqq-E7089ddMUZ1vS85LNACJh7JFmz9fXMME4bXGS9gXXWc3CxuJy6VQHCPkI6QFxHJH6DEk2-QbrvJszihsWpkEgtqgqA/w640-h470/CoverageReportExample_output.png" width="640" /></a></div><br /><p>The output from running this example is shown above. When reporting ‘details’ is enabled, the content of each coverage bin is reported. When reporting ‘details’ is disabled, only the top-level coverage achieved is reported. Displaying a coverage report with details is often helpful for confirming the correctness of a coverage model during development.</p><p><br /></p><p><b>Saving Coverage Data</b></p><p>The <a href="https://github.com/fvutils/pyucis">PyUCIS library</a> implements a Python interface to coverage data via the <a href="https://www.accellera.org/downloads/standards/ucis">Accellera UCIS</a> data model. It implements an object-oriented interface to coverage data, in addition to the Python equivalent of the UCIS C API. PyVSC uses the PyUCIS library to save coverage data, and can do so in a couple of interesting ways. Coverage data is written via the vsc.write_coverage_db method.</p><p>PyVSC can save coverage data to the XML interchange format defined by the UCIS standard. This is the default operation model for write_coverage_db. The example below shows saving it to a file named&nbsp; ‘cov.xml’.&nbsp;</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiv9Ox2BndFzQFeA46VCxqyroz3t-tNew5qvX0tu6xOiqxs83CdOn2jTq6mB_XxE3wsLjewGA1A4TdhpMRBk9SK1FE2uJl5UX-2zKQ9KElZunwOqpCiIKtJR5WgUGBjdn9kmzVap71ITuLTxE1zNrRRAKtR9KETlHXt0lzhEgQ1TDjdK34802Hu0_y_5Q/s698/CoverageSave_xml.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="332" data-original-width="698" height="304" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiv9Ox2BndFzQFeA46VCxqyroz3t-tNew5qvX0tu6xOiqxs83CdOn2jTq6mB_XxE3wsLjewGA1A4TdhpMRBk9SK1FE2uJl5UX-2zKQ9KElZunwOqpCiIKtJR5WgUGBjdn9kmzVap71ITuLTxE1zNrRRAKtR9KETlHXt0lzhEgQ1TDjdK34802Hu0_y_5Q/w640-h304/CoverageSave_xml.png" width="640" /></a></div><br /><p>PyVSC can also save coverage data to a custom database format, provided the tool that implements that database implements the UCIS C API. The example below saves coverage data to a custom database using the UCIS C API implemented in the shared library named ‘libucis.so’.</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1XaHSaOJZFYZ7xXLeQykKEXpAIkqXy69KBbpgxUAbjfjq7LL3hKaYm8UfihtSpZdmWiDVx9MtxhSOBQ28JnD5oQSaMtwXHtsW2DYgK30APCv78PAAW6WW_FtsIDUeiay1Tgd-sCUfRfSHCY4FDVQ7VfBWdL3OkYceMQZDHUgrhNj00A70KeKlaYW_WQ/s699/CoverageSave_ucis.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="330" data-original-width="699" height="302" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1XaHSaOJZFYZ7xXLeQykKEXpAIkqXy69KBbpgxUAbjfjq7LL3hKaYm8UfihtSpZdmWiDVx9MtxhSOBQ28JnD5oQSaMtwXHtsW2DYgK30APCv78PAAW6WW_FtsIDUeiay1Tgd-sCUfRfSHCY4FDVQ7VfBWdL3OkYceMQZDHUgrhNj00A70KeKlaYW_WQ/w640-h302/CoverageSave_ucis.png" width="640" /></a></div><br /><p>Both of these paths to saving coverage may provide ways to bring coverage data collected by PyVSC into coverage-analysis flows implemented by commercial EDA tools. Check your tool’s documentation and/or check with your application engineer to understand which options may be available. Feel free to report what works for you on the <a href="https://github.com/fvutils/pyvsc/discussions">PyVSC discussion forum</a> so that others can benefit as well.</p><p><b>Viewing Coverage Data</b></p><p>Obviously, you can use commercial EDA tools to view coverage data from PyVSC if your tool provides a path to bring UCIS XML in, or if it implements the UCIS C API. <a href="https://github.com/fvutils/pyucis-viewer">PyUCIS Viewer</a> provides a very simple open-source graphical application for viewing coverage in UCIS XML format.&nbsp;</p><p>To use PyUCIS Viewer, save coverage data in UCIS XML interchange format, then run PyUICIS Viewer on that XML file:</p><p><span style="font-family: courier;">% pyucis-viewer cov.xml</span></p><p>A simple tree-based graphical viewer will open to show type and instance coverage.&nbsp;</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyl0Aq9crAVbgEDjUxRerI8F_fGYYudJiKtinT634Xb9HxvehdeaPSIhEPcm2AJhPakC36tHA5EmWFZnkCVKSh7PX0geeo34oaTlByWx1o13cfj8nj29Dj17Omvz-BmVWxQrpM-h26voDzzMpfec7idota5KvUW96RgrVbFMoLqvVVyf--L5g-mH7lNA/s893/RISCV-DV_Coverage.PNG" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="721" data-original-width="893" height="516" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyl0Aq9crAVbgEDjUxRerI8F_fGYYudJiKtinT634Xb9HxvehdeaPSIhEPcm2AJhPakC36tHA5EmWFZnkCVKSh7PX0geeo34oaTlByWx1o13cfj8nj29Dj17Omvz-BmVWxQrpM-h26voDzzMpfec7idota5KvUW96RgrVbFMoLqvVVyf--L5g-mH7lNA/w640-h516/RISCV-DV_Coverage.PNG" width="640" /></a></div><br /><p><b>Conclusion</b></p><p>There are several options for viewing and manipulating coverage once it has been collected via a covergroup modeled with PyVSC. In a future post, we’ll look at some additional manipulation and reporting options being implemented within <a href="https://github.com/fvutils/pyucis">PyUCIS</a>.&nbsp;</p><p>Until then, check out the latest additions to the <a href="https://fvutils.github.io/pyvsc/">PyVSC documentation</a> and raise questions and issues on the<a href="https://github.com/fvutils/pyvsc"> PyVSC GitHub</a> page.</p><p><br /></p><div style="text-align: center;">Copyright 2022 Matthew Ballance</div><div><p style="font-variant-east-asian: normal; font-variant-numeric: normal; line-height: 16px; margin-bottom: 0in;"><span style="color: #666666;"><span face="Trebuchet MS, Trebuchet, Verdana, sans-serif"><span style="font-size: 9pt;"><i><span style="background: rgb(255, 255, 255);">The views and opinions expressed above are solely those of the author and do not represent those of my employer or any other party.</span></i></span></span></span></p></div><div><br /></div>]]></content><author><name>Matthew Ballance</name></author><category term="functional coverage" /><category term="Python" /><category term="Functional Verification" /><summary type="html"><![CDATA[&nbsp;I’ve been investing some time in documentation updates this weekend, after a couple of PyVSC users pointed out some under-described aspects of the PyVSC coverage flow. Given that these areas were under-documented in the past, it seemed a good opportunity to highlight what can be done with functional coverage data once it is sampled by a PyVSC covergroup.So, we’ve described some functional coverage goals using a PyVSC covergroup and coverpoints, created a covergroup instance, and sampled some coverage data – perhaps it was randomly-generated stimulus or data sampled from a monitor. What now?Runtime Coverage APIOne simple thing we can do is to query coverage achieved using the coverage APIs implemented by PyVSC covergroup classes. The `get_coverage` method returns the coverage achieved by all instances of a covergroup type. The `get_inst_coverage` method returns the coverage achieved by the specified covergroup instance.Let’s look at an example:In the example above, we define a covergroup with a coverpoint that contains four bins (1, 2, 4, 8). We create two instances of this covergroup and sample them with two different values. After each call to sample, we display the coverage achieved by all instances of the covergroup (type coverage) and the coverage achieved by each instance.The output from this example is shown above. After sampling the first covergroup, the coverage achieved for that, and all, instances is 25% since one of four bins was hit. After sampling the second covergroup, the coverage achieved for that covergroup instance is also 25%. Because two different bins are hit between the two covergroup instances, two of four bins are hit (50%) for type coverage.Runtime Coverage ReportsAnother way to look at collected coverage is via a coverage report. PyVSC provides two methods that are nearly identical for obtaining a textual coverage report:get_coverage_report – Returns the report as a stringreport_coverage – Writes the report to a string (stdout by default)Both of these methods accept a keyword parameter named ‘details’ which controls whether bin hits are reported or just the top-level coverage achieved. Let’s look at a derivative of the first example to better understand the textual coverage report options.This example is nearly identical to the first one, but with calls to ‘report_coverage’ instead of calls to the covergroup get_coverage methods.The output from running this example is shown above. When reporting ‘details’ is enabled, the content of each coverage bin is reported. When reporting ‘details’ is disabled, only the top-level coverage achieved is reported. Displaying a coverage report with details is often helpful for confirming the correctness of a coverage model during development.Saving Coverage DataThe PyUCIS library implements a Python interface to coverage data via the Accellera UCIS data model. It implements an object-oriented interface to coverage data, in addition to the Python equivalent of the UCIS C API. PyVSC uses the PyUCIS library to save coverage data, and can do so in a couple of interesting ways. Coverage data is written via the vsc.write_coverage_db method.PyVSC can save coverage data to the XML interchange format defined by the UCIS standard. This is the default operation model for write_coverage_db. The example below shows saving it to a file named&nbsp; ‘cov.xml’.&nbsp;PyVSC can also save coverage data to a custom database format, provided the tool that implements that database implements the UCIS C API. The example below saves coverage data to a custom database using the UCIS C API implemented in the shared library named ‘libucis.so’.Both of these paths to saving coverage may provide ways to bring coverage data collected by PyVSC into coverage-analysis flows implemented by commercial EDA tools. Check your tool’s documentation and/or check with your application engineer to understand which options may be available. Feel free to report what works for you on the PyVSC discussion forum so that others can benefit as well.Viewing Coverage DataObviously, you can use commercial EDA tools to view coverage data from PyVSC if your tool provides a path to bring UCIS XML in, or if it implements the UCIS C API. PyUCIS Viewer provides a very simple open-source graphical application for viewing coverage in UCIS XML format.&nbsp;To use PyUCIS Viewer, save coverage data in UCIS XML interchange format, then run PyUICIS Viewer on that XML file:% pyucis-viewer cov.xmlA simple tree-based graphical viewer will open to show type and instance coverage.&nbsp;ConclusionThere are several options for viewing and manipulating coverage once it has been collected via a covergroup modeled with PyVSC. In a future post, we’ll look at some additional manipulation and reporting options being implemented within PyUCIS.&nbsp;Until then, check out the latest additions to the PyVSC documentation and raise questions and issues on the PyVSC GitHub page.Copyright 2022 Matthew BallanceThe views and opinions expressed above are solely those of the author and do not represent those of my employer or any other party.]]></summary></entry><entry><title type="html">TbLink-RPC: Simplifying the Multi-Language Testbench</title><link href="https://bitsbytesgates.com/2022/03/27/tblink-rpc-simplifying-multi-language.html" rel="alternate" type="text/html" title="TbLink-RPC: Simplifying the Multi-Language Testbench" /><published>2022-03-27T20:40:00+00:00</published><updated>2022-03-27T20:40:00+00:00</updated><id>https://bitsbytesgates.com/2022/03/27/tblink-rpc-simplifying-multi-language</id><content type="html" xml:base="https://bitsbytesgates.com/2022/03/27/tblink-rpc-simplifying-multi-language.html"><![CDATA[<div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRHKdkXm_YqfEF4nahvthmhcSyVkvuROeDoDcR-M25X1Gi4KFoG15drTEl7t3H1HOyImflxhYPzMa1pYxQ0wNDNpekQqWBkJA3JjecSNMlZkG8wTEczhFu-1mfHq0PQggd8LWdgvuRiPGEqjRhulETC-0LC-JRH3L7p0ZUDQBepfWBgn6wtw5cznPjhg/s540/MultiLanguageTestbench_splash.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="300" data-original-width="540" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRHKdkXm_YqfEF4nahvthmhcSyVkvuROeDoDcR-M25X1Gi4KFoG15drTEl7t3H1HOyImflxhYPzMa1pYxQ0wNDNpekQqWBkJA3JjecSNMlZkG8wTEczhFu-1mfHq0PQggd8LWdgvuRiPGEqjRhulETC-0LC-JRH3L7p0ZUDQBepfWBgn6wtw5cznPjhg/s16000/MultiLanguageTestbench_splash.png" /></a></div><br /><p></p><p>SystemVerilog/UVM is, by far, the most widely-used language and methodology for block and subsystem-level verification environments today. The simplicity of that statement overlooks the fact that it’s often very common to have other bits of non-SystemVerilog code connected. Maybe it’s some C/C++ code that implements a reference algorithm used by the scoreboard. Maybe it’s an instruction-set simulation model used to implement software behavior. Maybe it’s infrastructure that allows early firmware code to drive behavior in the simulation. Either way, integrating non-SystemVerilog code is a non-trival development and maintenance task, despite the fact that SystemVerilog defines a standard API (the Direct Procedure Interface – DPI).</p><p>This is the first in a series of posts describing a project that I’ve been working with a goal of simplifying this situation. The TbLink-RPC (roughly Testbench Link Remote Procedure Call) provides infrastructure that dramatically reduces the code a testbench developer needs to create in order to integrate code with a simulation environment.&nbsp;</p><p>Two primary experience brought me to working on the TbLink-RPC project. The first was a somewhat long history of feeling like I had to reinvent the wheel every time I needed to integrate non-SystemVerilog code into a testbench environment. The second was my interest in ‘alternative’ testbench languages and my experience using Python as a verification language.</p><p>Back in the 2017/2018 timeframe, I started to get re-involved with open-source hardware design targeting, primarily, FPGAs. When you’re designing gateware (frankly, any software-like thing), it’s imperative to have a good test environment. I did a bit of exploration, trying out bespoke C++ testbench environments and a few other things, before landing on Python and cocotb as my test framework. A lot of this was motivated by open-source tool capabilities and community. I was committed to using open-source tools as much as possible for my open-source gateware, and open-source simulators (eg Icarus and Verilator) didn’t support sufficient SystemVerilog features to be able to use SV-UVM. After looking around a bit, cocotb seemed to have the largest community around it making it the obvious choice.</p><p>I found a lot to like about Python and cocotb for developing testbench environments. Python has a large collection of libraries, and the ability to easily incorporate these in a testbench boosted my productivity. I find the Python language easy to write and use – especially for smaller projects. Having a pure-Python testbench worked for me as a hobbyist. In many ways, that is because I create all my testbench environments from the ground up and don’t use commercial Verification IP (VIP/UVCs).&nbsp;</p><p>This same approach doesn’t work in most commercial environments. Testbench environments must reuse existing VIPs/UVCs (either commercial or developed in-house), and it’s common for a testbench architecture to remain mostly-unchanged across multiple design cycles. Doing a wholesale conversion to Python (or any other ‘alternative’ language) doesn’t make sense. Furthermore, bringing in small amounts of a different language has a high development and maintenance cost.</p><p>What I concluded I really wanted was a framework that would simplify the process of integrating some amount of testbench code written in any language (more precisely, any non-SystemVerilog language since the simulator already supports SystemVerilog) into a simulation environment.</p><p><b>Concept</b></p><p>I’ll get into more detail about the TbLink-RPC architecture in a future post. Fundamentally, though, the idea is to form a point-to-point connection between two environments through which pairs of objects can communicate via method calls.&nbsp;</p><p></p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnNyy_ASmMS639E9gPwbBIMGmmP5wCblEc5CTH-LsMynJ2QRwyJ-CyQvan5OdxKEb8IrAJviaEOw8FVQfExXzvwi-3TKbAWff45bEPh1J_Uut8-kz7n7JVNwKRXp_vRQMhI6oKzWBRbUVRu3ISqhRo31ikIDUG7TDtZB9QlEa8YNDa0qYR1GQEZpBozg/s932/concept_diagram.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="384" data-original-width="932" height="165" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnNyy_ASmMS639E9gPwbBIMGmmP5wCblEc5CTH-LsMynJ2QRwyJ-CyQvan5OdxKEb8IrAJviaEOw8FVQfExXzvwi-3TKbAWff45bEPh1J_Uut8-kz7n7JVNwKRXp_vRQMhI6oKzWBRbUVRu3ISqhRo31ikIDUG7TDtZB9QlEa8YNDa0qYR1GQEZpBozg/w400-h165/concept_diagram.png" width="400" /></a></div><br /><p></p><p>There are a few things that make TbLink-RPC different from other remote-procedure-call solutions. The first is that TbLink-RPC is simulation centric. Or, rather, it is designed to work with environments that maintain a local “simulated” notion of time. To that end, TbLink-RPC supports both blocking (time-consuming) and non-blocking functions, and defines a protocol to ensure that time advances at the intended time.</p><p>TbLink-RPC is designed to support both single OS process and multi OS-process integration. Single-process integration (where both environments run in the same OS process) provide higher performance.&nbsp; &nbsp; But, single-process integration isn’t always feasible. For example, one environment might be an instruction-set simulator (eg QEMU) that must run in its own process in order to manage memory in its own highly-specialized way.&nbsp;</p><p>TbLink-RPC emphasizes automation and modularity. Code-generation automation is used to create the boilerplate code, minimizing user effort to integrate new APIs. The entire system is designed such that integrations created independently can be easily combined. Even better, this is done in such a way that SystemVerilog users don’t need to deal with generated DPI integration code.</p><p><br /></p><p><b>Example</b></p><p>An example is often the simplest way to get across a concept, and I have a very simple one here. For now, I’ll just show the key elements of a typical use case: connecting a Python reference model to a UVM testbench environment.&nbsp;</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjR0Ce9JX3-2lmnyV1vKc13y4QZLFNOWk4pLts4jkI95i_MbQDBCXWqbKAf42gUX5wJWbbInFlBPJlRw-tg69wLKkAzULM-l8aDiHQlgOG36Vs-WMNQpsSy-uhSzDXNk5NcbyEafwMED33RrrAIwjPrTiaLWkjELNmHSEeFl4SRQAFoY3fb-lvB-ylevw/s408/UVM_tb_diagram.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="336" data-original-width="408" height="264" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjR0Ce9JX3-2lmnyV1vKc13y4QZLFNOWk4pLts4jkI95i_MbQDBCXWqbKAf42gUX5wJWbbInFlBPJlRw-tg69wLKkAzULM-l8aDiHQlgOG36Vs-WMNQpsSy-uhSzDXNk5NcbyEafwMED33RrrAIwjPrTiaLWkjELNmHSEeFl4SRQAFoY3fb-lvB-ylevw/s320/UVM_tb_diagram.png" width="320" /></a></div><br /><p>Now, in this case our DUT isn’t that exciting. It’s just an adder, so our reference model is correspondingly trivial.&nbsp;</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6uJXRt5gO1iGAxSMHaHA5XGbuWTVq9QI_plx5v0ErXbeQKM4cippRZZzvM1tu_tffUnafRuJ3BTm9b8ZSpG2kc8mdYA9_cKVCh5Fgi2gxPZQnGMSNOSKmferC2OpWpfnZTwLv92Uvfk3VZ6MHEw0J8Vpkfdeo0wRi2dJj0PxT5AxzvkoUgZkOutbPbg/s646/python_class.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="179" data-original-width="646" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6uJXRt5gO1iGAxSMHaHA5XGbuWTVq9QI_plx5v0ErXbeQKM4cippRZZzvM1tu_tffUnafRuJ3BTm9b8ZSpG2kc8mdYA9_cKVCh5Fgi2gxPZQnGMSNOSKmferC2OpWpfnZTwLv92Uvfk3VZ6MHEw0J8Vpkfdeo0wRi2dJj0PxT5AxzvkoUgZkOutbPbg/s16000/python_class.png" /></a></div><br /><p>Our reference-model class contains a method named ‘add’ that returns the sum of two parameters passed to it. Note that we apply a decorator (tblink_rpc.iftype) to the class. This registers the class with the TbLink-RPC infrastructure. Note, also, that we apply a decorator to the ‘add’ method and specify typing annotations for the parameters and return type. Together, these register the method with TbLink-RPC and specify the parameter types to be used.</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBMcPcMsk5zKpVnNvI3n7ucOU0Er2mEZXdvzsYTnzYbnpH1wWd158FalAlz2jPER6yxqb0GZ38xnlszVf8F7hDGasqmIdh5N4Uc-fRLdGsuvVKKYTkpMtfcPjDT3Bnrv71auuftoD_THlwWekKrETqoyQ5TgHpApOObjeUZTEvVuHFqmoGB8XwWXeS7Q/s529/sv_object_class.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="275" data-original-width="529" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBMcPcMsk5zKpVnNvI3n7ucOU0Er2mEZXdvzsYTnzYbnpH1wWd158FalAlz2jPER6yxqb0GZ38xnlszVf8F7hDGasqmIdh5N4Uc-fRLdGsuvVKKYTkpMtfcPjDT3Bnrv71auuftoD_THlwWekKrETqoyQ5TgHpApOObjeUZTEvVuHFqmoGB8XwWXeS7Q/s16000/sv_object_class.png" /></a></div><br /><p>In order to call our Python class from SystemVerilog, we will need a SV class to call. That class is shown above, and combines a class from the TbLink-RPC library (TbLinkLaunchUvmObj) with some generated implementation classes created from the API definition that in Python. If our class contained methods implemented in SystemVerilog that could be called from Python, then this class would contain the implementation. Since that’s not the case, there isn’t anything to implement here.</p><p>From a UVM testbench perspective, using our Python class involves two steps:</p><p></p><ul style="text-align: left;"><li>Launch an instance of the class in Python connected with the SV class</li><li>Call the API</li></ul><p></p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXGIxM1M5fLJWbtYtnT6xs3t-cJm7qeHzmum5xqxPlYB9Pb2V-_vLjIuTgYgSSgYGwZeDflywQKqhzK9TH7x0-bLzJZg42Dkfw1CR0hyVE7y7gkIxngndkQAW5AamvgM943snUyd8ELt-umJVGpONImT5xDQXbPl6xVTb5LOWZXS5sT0qChz596pKCJw/s599/launch_remote_obj.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="216" data-original-width="599" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXGIxM1M5fLJWbtYtnT6xs3t-cJm7qeHzmum5xqxPlYB9Pb2V-_vLjIuTgYgSSgYGwZeDflywQKqhzK9TH7x0-bLzJZg42Dkfw1CR0hyVE7y7gkIxngndkQAW5AamvgM943snUyd8ELt-umJVGpONImT5xDQXbPl6xVTb5LOWZXS5sT0qChz596pKCJw/s16000/launch_remote_obj.png" /></a></div>The first step, launching, is shown above. The TbLink-RPC library class (TbLinkLaunchUvmObj) that our class inherits from implements the details of starting up and communicating with an embedded or remote environment. We just have to specify the details of how to do this via the configuration object. In this case, and in most cases, we will use a factory method to fill in common details. Because we are starting a Python environment, we must specify the Python module (uvm_python_obj) that contains the Python class we wish to call.<div><br /></div><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhBp3tOO2_5c-WzCUf_VbKSgN1UPwUw4zVeKRrv-jlMnB3cqaOCVkPR5JEpsWBkrOU9i19VZS4Tv9KYwIFgqCjoRiY8KVyiovD90wEY69gOc9fK-xaxtjFZKrMBBu2t7nOYRrAwlq8QQCXYvEsyYJ7Z4HrRgZiBPahTZyxGadnhGOgKq5q1BIc0ZLmpzQ/s653/call_remote_obj.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="201" data-original-width="653" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhBp3tOO2_5c-WzCUf_VbKSgN1UPwUw4zVeKRrv-jlMnB3cqaOCVkPR5JEpsWBkrOU9i19VZS4Tv9KYwIFgqCjoRiY8KVyiovD90wEY69gOc9fK-xaxtjFZKrMBBu2t7nOYRrAwlq8QQCXYvEsyYJ7Z4HrRgZiBPahTZyxGadnhGOgKq5q1BIc0ZLmpzQ/s16000/call_remote_obj.png" /></a></div><br /><div>Finally, we need to call our reference model. Our scoreboard contains a method that accepts operand data from the agent driving the ALU, and a result from the agent monitoring the ALU output. We call the reference model by making a SV class-method call to obtain the expected result from the Python reference model.</div><div><br /></div><div><b>Next Steps</b></div><div><p>TbLink-RPC is designed to simplify integrating code into simulation environment. In the context of a SystemVerilog environment, this makes it much easier and simpler to bring in external models written in non-SystemVerilog. Over the course of the next few posts, I’ll go into a bit more detail on TbLink-RPC architecture and the nuts and bolts of the integration process.</p><p><br /></p><p><b>References</b></p><p></p><ul style="text-align: left;"><li>TbLink-RPC (under construction) -&nbsp;<a href="https://tblink-rpc.github.io/">https://tblink-rpc.github.io/</a></li><li>cocotb -&nbsp;<a href="https://docs.cocotb.org/en/stable/">https://docs.cocotb.org/en/stable/</a></li></ul><div><br /></div><div style="text-align: center;">Copyright 2022 Matthew Ballance</div><div style="text-align: left;"><p style="font-variant-east-asian: normal; font-variant-numeric: normal; line-height: 100%; margin-bottom: 0in;"><span style="color: #666666;"><span style="font-family: Trebuchet MS, Trebuchet, Verdana, sans-serif;"><span style="font-size: 9pt;"><i><span style="background: #ffffff;">The views and opinions expressed above are solely those of the author and do not represent those of my employer or any other party.</span></i></span></span></span></p></div><div><br /></div><p></p></div>]]></content><author><name>Matthew Ballance</name></author><summary type="html"><![CDATA[SystemVerilog/UVM is, by far, the most widely-used language and methodology for block and subsystem-level verification environments today. The simplicity of that statement overlooks the fact that it’s often very common to have other bits of non-SystemVerilog code connected. Maybe it’s some C/C++ code that implements a reference algorithm used by the scoreboard. Maybe it’s an instruction-set simulation model used to implement software behavior. Maybe it’s infrastructure that allows early firmware code to drive behavior in the simulation. Either way, integrating non-SystemVerilog code is a non-trival development and maintenance task, despite the fact that SystemVerilog defines a standard API (the Direct Procedure Interface – DPI).This is the first in a series of posts describing a project that I’ve been working with a goal of simplifying this situation. The TbLink-RPC (roughly Testbench Link Remote Procedure Call) provides infrastructure that dramatically reduces the code a testbench developer needs to create in order to integrate code with a simulation environment.&nbsp;Two primary experience brought me to working on the TbLink-RPC project. The first was a somewhat long history of feeling like I had to reinvent the wheel every time I needed to integrate non-SystemVerilog code into a testbench environment. The second was my interest in ‘alternative’ testbench languages and my experience using Python as a verification language.Back in the 2017/2018 timeframe, I started to get re-involved with open-source hardware design targeting, primarily, FPGAs. When you’re designing gateware (frankly, any software-like thing), it’s imperative to have a good test environment. I did a bit of exploration, trying out bespoke C++ testbench environments and a few other things, before landing on Python and cocotb as my test framework. A lot of this was motivated by open-source tool capabilities and community. I was committed to using open-source tools as much as possible for my open-source gateware, and open-source simulators (eg Icarus and Verilator) didn’t support sufficient SystemVerilog features to be able to use SV-UVM. After looking around a bit, cocotb seemed to have the largest community around it making it the obvious choice.I found a lot to like about Python and cocotb for developing testbench environments. Python has a large collection of libraries, and the ability to easily incorporate these in a testbench boosted my productivity. I find the Python language easy to write and use – especially for smaller projects. Having a pure-Python testbench worked for me as a hobbyist. In many ways, that is because I create all my testbench environments from the ground up and don’t use commercial Verification IP (VIP/UVCs).&nbsp;This same approach doesn’t work in most commercial environments. Testbench environments must reuse existing VIPs/UVCs (either commercial or developed in-house), and it’s common for a testbench architecture to remain mostly-unchanged across multiple design cycles. Doing a wholesale conversion to Python (or any other ‘alternative’ language) doesn’t make sense. Furthermore, bringing in small amounts of a different language has a high development and maintenance cost.What I concluded I really wanted was a framework that would simplify the process of integrating some amount of testbench code written in any language (more precisely, any non-SystemVerilog language since the simulator already supports SystemVerilog) into a simulation environment.ConceptI’ll get into more detail about the TbLink-RPC architecture in a future post. Fundamentally, though, the idea is to form a point-to-point connection between two environments through which pairs of objects can communicate via method calls.&nbsp;There are a few things that make TbLink-RPC different from other remote-procedure-call solutions. The first is that TbLink-RPC is simulation centric. Or, rather, it is designed to work with environments that maintain a local “simulated” notion of time. To that end, TbLink-RPC supports both blocking (time-consuming) and non-blocking functions, and defines a protocol to ensure that time advances at the intended time.TbLink-RPC is designed to support both single OS process and multi OS-process integration. Single-process integration (where both environments run in the same OS process) provide higher performance.&nbsp; &nbsp; But, single-process integration isn’t always feasible. For example, one environment might be an instruction-set simulator (eg QEMU) that must run in its own process in order to manage memory in its own highly-specialized way.&nbsp;TbLink-RPC emphasizes automation and modularity. Code-generation automation is used to create the boilerplate code, minimizing user effort to integrate new APIs. The entire system is designed such that integrations created independently can be easily combined. Even better, this is done in such a way that SystemVerilog users don’t need to deal with generated DPI integration code.ExampleAn example is often the simplest way to get across a concept, and I have a very simple one here. For now, I’ll just show the key elements of a typical use case: connecting a Python reference model to a UVM testbench environment.&nbsp;Now, in this case our DUT isn’t that exciting. It’s just an adder, so our reference model is correspondingly trivial.&nbsp;Our reference-model class contains a method named ‘add’ that returns the sum of two parameters passed to it. Note that we apply a decorator (tblink_rpc.iftype) to the class. This registers the class with the TbLink-RPC infrastructure. Note, also, that we apply a decorator to the ‘add’ method and specify typing annotations for the parameters and return type. Together, these register the method with TbLink-RPC and specify the parameter types to be used.In order to call our Python class from SystemVerilog, we will need a SV class to call. That class is shown above, and combines a class from the TbLink-RPC library (TbLinkLaunchUvmObj) with some generated implementation classes created from the API definition that in Python. If our class contained methods implemented in SystemVerilog that could be called from Python, then this class would contain the implementation. Since that’s not the case, there isn’t anything to implement here.From a UVM testbench perspective, using our Python class involves two steps:Launch an instance of the class in Python connected with the SV classCall the APIThe first step, launching, is shown above. The TbLink-RPC library class (TbLinkLaunchUvmObj) that our class inherits from implements the details of starting up and communicating with an embedded or remote environment. We just have to specify the details of how to do this via the configuration object. In this case, and in most cases, we will use a factory method to fill in common details. Because we are starting a Python environment, we must specify the Python module (uvm_python_obj) that contains the Python class we wish to call.Finally, we need to call our reference model. Our scoreboard contains a method that accepts operand data from the agent driving the ALU, and a result from the agent monitoring the ALU output. We call the reference model by making a SV class-method call to obtain the expected result from the Python reference model.Next StepsTbLink-RPC is designed to simplify integrating code into simulation environment. In the context of a SystemVerilog environment, this makes it much easier and simpler to bring in external models written in non-SystemVerilog. Over the course of the next few posts, I’ll go into a bit more detail on TbLink-RPC architecture and the nuts and bolts of the integration process.ReferencesTbLink-RPC (under construction) -&nbsp;https://tblink-rpc.github.io/cocotb -&nbsp;https://docs.cocotb.org/en/stable/Copyright 2022 Matthew BallanceThe views and opinions expressed above are solely those of the author and do not represent those of my employer or any other party.]]></summary></entry></feed>